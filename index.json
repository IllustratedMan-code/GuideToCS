[
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-1-2021/",
	"title": "10/1/2021",
	"tags": [],
	"description": "",
	"content": "Original is here.\nWe made it into October!! Spooky, spooky!\nCorrections Like in real newspapers (Links to an external site.), we are going to start including Corrections in each edition! We want to make sure that our reporters adhere to the highest standards:\nThe JVM will insert an implicit call to the to-be-instantiated class' default constructor (i.e., the one with no parameters) if the the to-be-constructed (sub)class does not do so explicitly. We\u0026rsquo;ll make this clear with an example:\nclass Parent { Parent() { System.out.println(\u0026#34;I am in the Parent constructor.\u0026#34;); } Parent(int parameter) { System.out.println(\u0026#34;This version of the constructor is not called.\u0026#34;); } } class Child extends Parent { Child() { /* * No explicit call to super -- one is automatically * injected to the parent constructor with no parameters. */ System.out.println(\u0026#34;I am in the Child constructor.\u0026#34;); } } public class DefaultConstructor { public static void main(String args[]) { Child c = new Child(); } } When this program is executed, it will print\nI am in the Parent constructor. I am in the Child constructor. The main function is instantiating an object of the type Child. We can visually inspect that there is no explicit call the super() from within the Child class' constructor. Therefore, the JVM will insert an implicit call to super() which actually invokes Parent().\nHowever, if we make the following change:\nclass Parent { Parent() { System.out.println(\u0026#34;I am in the Parent constructor.\u0026#34;); } Parent(int parameter) { System.out.println(\u0026#34;This version of the constructor is not called.\u0026#34;); } } class Child extends Parent { Child() { /* * No explicit call to super -- one is automatically * injected to the parent constructor with no parameters. */ super(1); System.out.println(\u0026#34;I am in the Child constructor.\u0026#34;); } } public class DefaultConstructor { public static void main(String args[]) { Child c = new Child(); } } Something different happens. We see that there is a call to Child\u0026rsquo;s superclass' constructor (the one that takes a single int-typed parameter). That means that the JVM will not insert an implicit call to super() and we will get the following output:\nThis version of the constructor is not called. I am in the Child constructor.\nThe C++ standard sanctions a main function without a return statement. The standard says: \u0026ldquo;if control reaches the end of main without encountering a return statement, the effect is that of executing return 0;.\u0026rdquo;\nA Different Way to OOP So far we have talked about OOP in the context of Java. Java, and languages like it, are called Class-based OOP languages. In a Class-based OOP, classes and objects exist in different worlds. Classes are used to define/declare\n the attributes and methods of an encapsulation, and the relationships between them.  From these classes, objects are instantiated that contain those attributes and methods and respect the defined/declared hierarchy. We can see this in the example given above: The classes Parent and Child define (no) attributes and (no) methods and define the relationship between them. In main(), a Child is instantiated and stored in the variable c. c is an object of type Child that contains all the data associated with a Child and a Parent and can perform all the actions of a Child and a Parent.\nNothing about Class-based OOP should be different than what you\u0026rsquo;ve learned in the past as you\u0026rsquo;ve worked with C++. There are several problems with Class-based OOP.\n The supported attributes and method of each class must be determined before the application is developed (once the code is compiled and the system is running, an object cannot add, remove or modify its own methods or attributes); The inheritance hierarchy between classes must be determined before the application is developed (once the code is compiled, changing the relationship between classes will require that the application be recompiled!).  In other words, Class-based OOP does not allow the structure of the Classes (nor their relationships) to easily evolve with the implementation of a system.\nThere is another way, though. It\u0026rsquo;s called Prototypal OOP. The most commonly known languages that use Prototypal OOP are JavaScript and Ruby! In Prototypal (which is a very hard word to spell!) OOP there is no distinction between Class and object \u0026ndash; everything is an object! In a Prototypal OOP there is a base object that has no methods or data attributes and every object is able to modify itself (its attributes and methods). To build a new object, the programmer simply copies from an existing object, the new object\u0026rsquo;s so-called prototype, and customizes the copied object appropriately.\nFor example, assume that there is an object called Car that has one attribute (the number of wheels) and one method (start). That object can serve as the prototype car. To \u0026ldquo;instantiate\u0026rdquo; a new Car, the programmer simply copies the existing prototypical car object Car and gives it a name, say, c. The programmer can change the value of c\u0026rsquo;s number of wheels and invoke its method, start. Let\u0026rsquo;s say that the same programmer wants to create something akin to a subclass of Car. The programmer would create a new, completely fresh object (one that has no methods or attributes), name it, say, Tesla, and link the new prototype Tesla object to the existing prototype car Car object through the prototype Tesla object\u0026rsquo;s prototype link (the sequence of links that connects prototype objects to one another is called a prototype chain). If a Tesla has attributes (range, etc) or methods (self_drive) that the prototype car does not, then the programmer would install those methods on the prototype Tesla Tesla. Finally, the programmer would \u0026ldquo;declare\u0026rdquo; that the Tesla object is a prototype Tesla.\n The blue arrows in the diagram above are prototype links. The orange lines indicate where a copy is made.\nHow does inheritance work in such a model? Well, it\u0026rsquo;s actually pretty straightforward: When a method is invoked or an attribute is read/assigned, the runtime will search the prototype chain for the first prototypical object that has such a method or attribute. Mic drop. In the diagram above, let\u0026rsquo;s follow how this would play out when the programmer calls start() on the Model 3 Instance. The Model 3 Instance does not contain a method named start. So, up we go! The Tesla Prototype Object does not contain that me either. All the way up! The Car Prototype Object, does, however, so that method is executed!\nWhat would it look like to override a function? Again, relatively straightforward. If a Tesla performs different behavior than a normal Car when it starts, the programmer creating the Tesla Prototype Object would just add a method to that object with the name start. Then, when the prototype chain is traversed by the runtime looking for the method, it will stop at the start method defined in the Tesla Prototype Object instead of continuing on to the start method in the Car Prototype Object. (The same is true of attributes!)\nThere is (at least) one really powerful feature of this model. Keep in mind that the prototype objects are real things that can be manipulated at runtime (unlike classes which do not really exist after compilation) and prototype objects are linked together to achieve a type of inheritance. With reference to the diagram above, say the programmer changes the definition of the start method on the Car Prototype Object. With only that change, any object whose prototype chain includes the Car Prototype Object will immediately have that new functionality (where it is not otherwise overridden, obviously) \u0026ndash; all without stopping the system!! How cool is that?\nHow scary is that? Can you imagine working on a system where certain methods you \u0026ldquo;inherit\u0026rdquo; change at runtime?\n OOP or Interfaces? Newer languages (e.g., Go, Rust, (new versions of) Java) are experimenting with new features that support one of the \u0026ldquo;killer apps\u0026rdquo; of OOP: The ability to define a function that takes a parameter of type A but that works just the same as long as it is called with an argument whose type is a subtype of A. The function doesn\u0026rsquo;t have care whether it is called with an argument whose type is A or some subtype of A because the language\u0026rsquo;s OOP semantics guarantee that anything the programmer can do with an object of type A, the programmer can do with and object of subtype of A.\nUnfortunately, using OOP to accomplish such a feat may be like killing a fly with a bazooka (or a laptop, like Alex killed that wasp today).\nInstead, modern languages are using a slimmer mechanism known as an interface or a trait. An interface just defines a list of methods that an implementer of that interface must support. Let\u0026rsquo;s see some real Go code that does this \u0026ndash; it\u0026rsquo;ll clear things up:\ntype Readable interface { Read() } This snippet defines an interface with one function (Read) that takes no parameters and returns no value. That interface is named Readable. Simple.\ntype Book struct { title string } This snippet defines a data structure called a Book \u0026ndash; such structs are the closest that Go has to classes.\nfunc (book Book) Read() { fmt.Printf(\u0026#34;Reading the book %v\\n\u0026#34;, book.title) } This snippet simply says that if variable b is of type Book then the programmer can call b.Read(). Now, for the payoff:\nfunc WhatAreYouReading(r Readable) { r.Read() } This function only accepts arguments that implement (i.e., meet the criteria specified in the definition of) the Readable interface. In other words, with this definition, the code in the body of the function can safely assume that it can can call Read on r. And, for the encore:\nbook := Book{title: \u0026#34;Infinite Jest\u0026#34;} WhatAreYouReading(book) This code works exactly like you\u0026rsquo;d expect. book is a valid argument to WhatAreYouReading because it implements the Read method which, implicitly, means that it implements the Readable interface. But, what\u0026rsquo;s really cool is that the programmer never had to say explicitly that Book implements the Readable interface! The compiler checks automatically. This gives the programmer the ability to generate a list of only the methods absolutely necessary for its parameters to implement to achieve the necessary ends \u0026ndash; and nothing unnecessary. Further, it decouples the person implementing a function from the person using the function \u0026ndash; those two parties do not have to coordinate requirements beforehand. Finally, this functionality means that a structure can implement as few or as many interfaces as its designer wants.\nDip Our Toe Into the Pool of Pointers We only had a few minutes to start pointers, but we did make some headway. There will be more on this in the next lecture!\nIt is important to remember that pointers are like any other type \u0026ndash; they have a range of valid values and a set of valid operations that you can perform on those values. What are the range of valid values for a pointer? All valid memory addresses. And what are the valid operations? Addition, subtraction, dereference and assignment.\n In the diagram, the gray area is the memory of the computer. The blue box is a pointer. It points to the gold area of memory. It is important to remember that pointers and their targets both exist in memory! In fact, in true Inception (Links to an external site.)style, a pointer can pointer to a pointer!\nAt the same time that pointers are types, they also have types. The type of a pointer includes the type of the target object. In other words, if the memory in the gold box held an object of type T, the the green box\u0026rsquo;s type would be \u0026ldquo;pointer to type T.\u0026rdquo; If the programmer dereferences the blue pointer, they will get access to the object in memory in the gold.\nIn an ideal scenario, it would always be the case that the type of the pointer and the type of the object at the target of the pointer are the same. However, that\u0026rsquo;s not always the case. Come to the next lecture to see what can go wrong when that simple fact fails to hold!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-15-2021/",
	"title": "10/15/2021",
	"tags": [],
	"description": "",
	"content": "The hunt for October!\nCorrections None to speak of!!\nIntroduction to Functional Programming We spent Friday beginning our module on Functional Programming (FP)! As we said at the beginning of the semester when we were learning about programming paradigms, FP is very different than imperative programming. In imperative programming, developers tell the computer how to do the operation. While functional programming is not logic programming (where developers just tell the computer what to compute and leave the how entirely to the language implementation), the writer of a program in a functional PL is much more concerned with specifying what to compute than how to compute it.\n Four Characteristics of Functional Programming There are four characteristics that epitomize FP:\n There is no state Functions are central  Functions can be parameters to other functions Functions can be return values from other others Program execution is function evaluation   Control flow is performed by recursion and conditional expressions Lists are a fundamental data type  In a functional programming language, there are no variables, per se. And because there are no variables, there is no state. That does not mean there are no names. Names are still important. It simply means that names refer to expressions themselves and not their values. The distinction will become more obvious as we continue to learn more about writing programs in functional languages.\nBecause there is no state, a functional programming language is not history sensitive. A language that is history sensitive means that results of operations in that language can be affected by operations that have come before it. For example, in an imperative programming language, a function may be history sensitive if it relies on the value of a global variable to calculate its return value. Why does that count as history sensitive? Because the value in the global variable could be affected by prior operations.\nA language that is not history sensitive has referential transparency. We learned the definition of referential transparency before, but now it might make a little more sense. In a language that has referential transparency, a the same function called with the same arguments generates the same result no matter what operations have preceded it.\nIn a functional programming language there are no loops (unless they are added as syntactic sugar) \u0026ndash; recursion is the way to accomplish repetition. Selective execution (as opposed to sequential execution) is accomplished using the conditional expression. A conditional expression is, well, an expression that evaluates to one of two values depending on the value of a condition. We have seen conditional expressions in STIMPL. That a conditional statement can have a value (thus making it a conditional expression) is relatively surprising for people who only have experience in imperative programming languages. Nevertheless, the conditional expressions is a very, very sharp sword in the sheath of the functional programmer.\nA functional program is a series of functions and the execution of a functional program is simply an evaluation of those functions. That sounds abstract at this point, but will become more clear when we see some real functional programs.\nLists are a fundamental data type in functional programming languages. Powerful syntactic tools for manipulating lists are built in to most functional PLs. Understanding how to wield these tools effectively is vital for writing code in functional PLs.\nThe Historical Setting of the Development of Functional PLs The first functional programming language was developed in the mid-1950s by John McCarthy . At the time, computing was most associated with mathematical calculations. McCarthy was instead focused on artificial intelligence which involved symbolic computing. Computer scientists thought that it was possible to represent cognitive processes as lists of symbols. A language that made it possible to process those lists would allow developers to build systems that work like our brains.\n McCarthy started with the goal of writing a system of meta notation that programmers could attach to Fortran. These meta notations would be reduced to actual Fortran programs. As they did their work, they found their way to a program representation built entirely of lists (and lists of lists, and lists of lists of lists, etc). Their thinking resulted in the development of Lisp, a list processing language. In Lisp, data are lists and programs are lists. They showed that list processing, the basis of the semantics of Lisp, is capable of universal computing. In other words, Lisp, and other list processing languages, is/are Turing complete.\nThe inability to execute a Lisp program efficiently on a physical computer based on the von Neumann model has given Lisp (and other functional programming languages) a reputation as slow and wasteful. (N.B.: This is not true today!) Until the late 1980s hardware vendors thought that it would be worthwhile to build physical machines with non-von Neumann architectures that made executing Lisp programs faster. Here is an image of a so-called Lisp Machine.\n LISP We will not study Lisp in this course. However, there are a few aspects of Lisp that you should know because they pervade the general field of computer science.\nFirst, you should know CAR, CDR and CONS \u0026ndash; pronounced car, could-er, and cahns, respectively. CAR is a function that takes a list as a parameter and returns the first element of the list. CDR is a function that takes a list as a parameter and returns the tail, everything but the head, of the list. CONS takes two parameters \u0026ndash; a single element and a list \u0026ndash; and returns a new list with the first argument appended to the front of the second argument.\nFor instance,\n(car (1 2 3)) is 1.\n(cdr (1 2 3)) is (2 3).\nSecond, you should know that, in Lisp, all data are lists and programs are lists.\n(a b c) is a list in Lisp. In Lisp, (a b c) could be interpreted as a list of atoms a, b and c or an invocation of function a with parameters b and c.\nLambda Calculus Lambda Calculus is the theoretical basis of functional programming languages in the same way that the Turing Machine is the theoretical basis of the imperative programming languages. The Lambda Calculus is nothing like \u0026ldquo;calculus\u0026rdquo; \u0026ndash; the word calculus is used here in its strict sense: a method or system of calculation . It is better to think of Lambda Calculus as a programming language rather than a branch of mathematics.\nLambda Calculus is a model of computation defined entirely by function application. The Lambda Calculus is as powerful as a Turning Machine which means that anything computable can be computed in the Lambda Calculus. For a language as simple as the Lambda Calculus, that\u0026rsquo;s remarkable!\nThe entirety of the Lambda Calculus is made up of three entities:\n Expression: a name, a function or an application Function: \\(\\lambda\\) .  Application:    Notice how the elements of the Lambda Calculus are defined in terms of themselves. In most cases it is possible to restrict names in the Lambda Calculus to be any single letter of the alphabet \u0026ndash; a is a name, z is a name, etc. Strictly speaking, functions in the Lambda Calculus are anonymous \u0026ndash; in other words they have no name. The name after the\nin a function in the Lambda Calculus can be thought of as the parameter of the function. Here\u0026rsquo;s an example of a function in the Lambda Calculus:\n\\(\\lambda\\)x . x\nLambda Calculiticians (yes, I just made up that term) refer to this as the identity function. This function simply returns the value of its argument! But didn\u0026rsquo;t I say that functions in the Lambda Calculus don\u0026rsquo;t have names? Yes, I did. Within the language there is no way to name a function. That does not mean that we cannot assign semantic values to those functions. In fact, associating meaning with functions of a certain format is exactly how high-level computing is done with the Lambda Calculus.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-18-2021/",
	"title": "10/18/2021",
	"tags": [],
	"description": "",
	"content": "Lambda lower now. How low can you go?\nCorrections None to speak of!!\n(Recalling) Lambda Calculus Remember that we said Lambda Calculus is the theoretical basis of functional programming languages in the same way that the Turing Machine is the theoretical basis of the imperative programming languages. Again, don\u0026rsquo;t freak out when you hear the phrase \u0026ldquo;calculus\u0026rdquo;. As we said in class, it is better to think of the Lambda Calculus as a programming language rather than a branch of mathematics.\nLambda Calculus is a model of computation defined entirely by function application. The Lambda Calculus is as powerful as a Turning Machine which means that anything computable can be computed in the Lambda Calculus. For a language as simple as the Lambda Calculus, that\u0026rsquo;s remarkable!\nRemember that the entirety of the Lambda Calculus is made up of a small number of entities:\n Expression: a name, a function or an application Function: \\(\\lambda\\) .  Application:    We made the point in class that, without loss of generality, we will assume that all names are single letters from the alphabet. In other words, if you see two consecutive letters, e.g., ab, those are two separate names.\nBound and Free Names and the Tao of Function Application Because the entirety of the Lambda Calculus is function application, it is important that we get it exactly right. Let\u0026rsquo;s recall the simplest example of function application: \\((\\lambda a. a)x = \\left \\lfloor x/a \\right \\rfloor a = x\\) The \\(\\lfloor x/a \\rfloor a\\) means \u0026ldquo;replace all instances of a with x in whatever comes after the \\(\\lfloor \\rfloor\\) \u0026ldquo;. This is so easy. What about this, though?\n\\((\\lambda a. \\lambda b. ba)b\\)\nThe first thing to realize is that the b in the expression that is the body of the nested lambda function is completely separate from the b to which the lambda function is being applied. Why is that? Because the b in the nested lambda function is the \u0026ldquo;parameter\u0026rdquo; to that function. So, what are we to do?\nFirst, let\u0026rsquo;s step back and consider the definitions of free and bound names. Loosely speaking, a name is bound as soon as it is used as a parameter to a lambda function. It continues to be bound in nested expressions but may be rebound! For example,\n\\(\\lambda x. x \\lambda x. x\\)\nThe \u0026ldquo;body\u0026rdquo; of the outer function is \\(x \\lambda x . x\\) and the leftmost x is the x from the outer function\u0026rsquo;s parameter. In other words,\n\\((\\lambda x.x \\lambda x.x) (a) = a \\lambda x.x\\)\nThe substitution of a for x continues no further because x is rebound at the start of the nested lambda function. You will be relieved to know that,\n\\(\\lambda x.x \\lambda x.x = \\lambda x.x \\lambda a.a\\)\nIn fact, renaming like that has a special name: alpha conversion!\nFree names are those that are not bound.\nWow, that got pretty complicated pretty quickly! This is one case where some formalism actually improves the clarity of things, I think. Here is the formal definition of what it means for a name to be bound:\n name is bound in \\(\\lambda name_1.expression\\) if name = name1 or name is bound in expression. name is bound in \\(E_1 E_2\\) if name is bound in either \\(E_1\\) or \\(E_2\\).  Here is the formal definition of what it means for a name to be free:\n name is free in name name is free in \\(\\lambda name_1.expression\\) when name \\(\\ne\\) name1 and name is free in expression name is free in \\(E_1E_2\\) if name is free in either E1 or E2  Note that a name can be free and bound at the same time.\nAll this hullabaloo means that we need to be slightly more sophisticated in our function application. We have to check two boxes before assuming that can treat function application as a simple textual search/replace:\nWhen applying \\(\\lambda x. E_1\\) to E2, we only replace the free instances of x in E1 with E2 and if E2 contains a free name that is bound in E1, we have to alpha convert that bound name in E1 to something that doesn\u0026rsquo;t conflict. There is a good example of this in Section 1.2 of the XXXX that I will recreate here:\n\\((\\lambda x. (\\lambda y . (x\\lambda x. xy)))y\\)\nFirst, note y (our E2 in this case) contains y (a free name) that is bound in \\((\\lambda y. (x \\lambda x.xy))\\)(our E1). In other words, before doing a straight substitution, we have to alpha convert the bound y in E1 to something that doesn\u0026rsquo;t conflict. Let\u0026rsquo;s choose t:\n\\((\\lambda x. (\\lambda t. (x \\lambda x.xt)))y\\)\nNow we can do our substitution! But, be careful: x appears free in \\((\\lambda y. (x \\lambda x.xy)\\) (again, our E1) one time \u0026ndash; its leftmost appearance! So, the substitution would yield:\n\\((\\lambda t. (y \\lambda x.xt)\\)\nVoila!\nCurrying Functions Currying is the process of turning a function that takes multiple parameters into a sequence of functions that each take a single parameter. Currying is only possible in languages that support high-order functions: functions that a) take functions as parameters, b) return functions or c) both. Python is such a language. Let\u0026rsquo;s look at how you would write a function that calculates the sum of three numbers in Python:\ndef sum3(a, b, c): return a + b + c That makes perfect sense!\nLet\u0026rsquo;s see if we can Curry that function. Because a Curried function can only take one parameter and we are Currying a function with three parameters, it stands to reason that we are going to have to generate three different functions. Let\u0026rsquo;s start with the first:\ndef sum1(a): # Something? What something are we going to do? Well, we are going to declare another function inside sum1, call it sum2, that takes a parameter and then use that as the return value of sum1! It looks something like this:\ndef sum1(a): def sum2(b): pass return sum2 That means, if we call sum1 with a single parameter, the result is another function, one that takes a single parameter! So, we\u0026rsquo;ve knocked off two of the three parameters, now we need one more. So, let\u0026rsquo;s write something like this:\ndef sum1(a): def sum2(b): def sum3(c): pass return sum3 return sum2 This means that if we call sum1 with a single parameter and call the result of that with a single parameter, the result is another function, one that also takes a single parameter! What do we want that innermost function to do? That\u0026rsquo;s right: the summation! So, here\u0026rsquo;s our final code:\ndef sum1(a): def sum2(b): def sum3(c): return a + b + c return sum3 return sum2 We\u0026rsquo;ve successfully Curried a three-parameter summation function! There\u0026rsquo;s just one issue left to address? How can we possibly use a and b in the innermost function? Will, I thought you told us that Python was statically scoped! In order for this to work correctly, wouldn\u0026rsquo;t Python have to have something magical and dynamic-scope-like? Well, yes! And, it does. It has closures.\nWhen you return sum2 from the body of sum1, Python closes around the variables that are needed by any code in the implementation of the returned function. Because a is needed in the implementation of sum2 (the function returned by sum1), Python creates a closure around that function which includes the value of a at the time sum2 was returned. It is important to remember that every time sum2 is defined pursuant to an invocation of sum1, a new version of sum2 is returned with a new closure. This closure-creation process repeats when we return sum3 pursuant to an invocation of sum2 (which itself was generated as a result of an invocation of sum1)! Whew.\nBecause we Curried the sum3 function as sum1, we have to call them slightly differently:\nsum3(1, 2, 3) sum1(1)(2)(3) As we learn more about functional programming in Haskell, you will see this pattern more and more and it will become second nature.\nThe \u0026ldquo;good\u0026rdquo; news, if you can call it that, is that functions in the Lambda Calculus always exist in their Curried form. Prove it to yourself by looking back at the way we formally defined the Lambda Calculus.\nBut, because it is laborious to write all those \\(\\lambda\\)s over and over, we will introduce a shorthand for functions in the Lambda Calculus that take more than one parameter:\n\\(\\lambda p_1p_2 \u0026hellip; p_n.expression\\)\nis a function with n parameters named p1 through pn (which are each one letter). Simply put,\n\\(\\lambda x . \\lambda y.xy = \\lambda xy.xy\\)\nfor example.\nDoing Something with Lambda Calculus Remember how we have stressed that you cannot name functions inside the Lambda Calculus but how I have stressed that does not mean we cannot give names to functions from outside the Lambda Calculus? Well, here\u0026rsquo;s where it starts to pay off! We are going learn how to do boolean operations using the Lambda Calculus. Let\u0026rsquo;s assume that anytime we see a lambda function that takes two parameters and reduces to the first, we call that T. When we see a lambda function that takes two parameters and reduces to the second, we call that F:\n\\(T \\equiv \\lambda xy.x\\)\n\\(F \\equiv \\lambda xy.y\\)\nTo reiterate, it is the form that matters. If we see\n\\(\\lambda ab.a\\)\nthat is T too! In what follows, I will type T and F to save myself from writing all those \\(\\lambda\\)s, but remember: T and F are just functions!!\nOkay, let\u0026rsquo;s do something boolean operations. We can define the and operation as\n\\(\\wedge = \\lambda xy.xy F\\)\nLet\u0026rsquo;s give it a whirl. First, let\u0026rsquo;s get on the same page: True and False is False.\n\\(\\wedge TF = (\\lambda xy.xyF)TF = TFF = (\\lambda xy.x) FF = F\\)\nAwesome! Let\u0026rsquo;s try another: True and True is True.\n\\(\\wedge TT = (\\lambda xy.xyF)TT = TTF = (\\lambda xy.x) TF = T\\)\nWe can define the or operation as\n\\(\\lor = \\lambda xy.xTy\\)\nTry your hand at working through a few examples and make sure you get the expected results!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-22-2021/",
	"title": "10/22/2021",
	"tags": [],
	"description": "",
	"content": "Corrections Thanks to Donald\u0026rsquo;s persistence, I researched the mechanism by which Haskell and other pure functional languages\n handle associations between names and expressions, and pass around infinite lists (without having to generate the entire list first \u0026ndash; an obvious impossibility)  thunks are covered below!\nFunction Invocation in Functional Programming Languages In imperative programming languages, it may matter to the correctness of a program the order in which parameters to a function are evaluated. (Note: For the purposes of this discussion we will assume that all operators [+, -, /, etc] are implemented as functions that take the operands as arguments in order to simplify the discussion. In other words, when describe the order of function evaluation we are also talking about the order of operand evaluation.) While the choice of the order in which we evaluate the operands is the language designer\u0026rsquo;s prerogative, the choice has consequences. Why? Because of side effects! For example:\n#include \u0026lt;stdio.h\u0026gt; int operation(int parameter) { static int divisor = 1; return parameter / (divisor++); } int main() { int result = operation(5) + operation(2); printf(\u0026#34;result: %d\\n\u0026#34;, result); return 0; } prints\nresult: 6 whereas\n#include \u0026lt;stdio.h\u0026gt; int operation(int parameter) { static int divisor = 1; return parameter / (divisor++); } int main() { int result = operation(2) + operation(5); printf(\u0026#34;result: %d\\n\u0026#34;, result); return 0; } prints\nresult: 4 In the difference between the two programs we see vividly the role that the static variable plays in the state of the program and its ultimate output.\nBecause of the referential transparency in pure functional programming languages, the designer of such a language does not need to worry about the consequences of the decision about the order of evaluation of arguments to functions. However, that does not mean that the language designer of a pure functional programming language does not have choices to make in this area.\nA very important choice the designer has to make is the time when function arguments are evaluated. There are two options available:\n All function arguments are evaluated before the function is evaluated. Function arguments are evaluated only when their results are needed.  Let\u0026rsquo;s look at an example: Assume that there are two functions: dbl, a function that doubles its input, and average, a function that averages its three parameters:\ndbl x = (+) x x average a b c = (/) ((+) a ((+) b c)) 3 Both functions are written using prefix notation (i.e., (  \u0026hellip; ). We will call these functions like this:\ndbl (average 3 4 5) If the language designer chooses to evaluate function arguments only when their results are needed, the execution of this function call proceeds as follows:\ndbl (average 3 4 5) + (average 3 4 5) (average 3 4 5) + ((/) ((+) 3 ((+) 4 5)) 3) (average 3 4 5) + (4) (average 3 4 5) + (4) ((/) ((+) 3 ((+) 4 5)) 3) + (4) (4) 8 The outermost function is always reduced (expand) before the inner functions. Note: Primitive functions (+, and / in this example) cannot be expanded further so we move inward in evaluation if we encounter such a function for reduction.\nIf, however, the language designer chooses to evaluate function arguments before the function is evaluated, the execution of the function call proceeds as follows:\ndbl (average 3 4 5) dbl ((/) ((+) 3 ((+) 4 5)) 3) dbl 4 + 4 4 8 No matter the designer\u0026rsquo;s choice, the outcome of the evaluation is the same. However, there is something strikingly different about the two. Notice that in the first derivation, the calculation of the average of the three numbers happens twice. In the second derivation, it happens only once! That efficiency is not a fluke! Generally speaking, the method of function invocation where arguments are evaluated before the function is evaluated is faster.\nThese two techniques have technical names:\n applicative order: \u0026ldquo;all the arguments to … procedures are evaluated when the procedure is applied.\u0026rdquo; normal order: \u0026ldquo;delay evaluation of procedure arguments until the actual argument values are needed.\u0026rdquo;  These definitions come from\nAbelson, H., Sussman, G. J.,, with Julie Sussman (1996). Structure and Interpretation of Computer Programs. Cambridge: MIT Press/McGraw-Hill. ISBN: 0-262-01153-0\nIt is obvious, then, that any serious language designer would choose applicative order for their language. There\u0026rsquo;s no reason redeeming value for the inefficiency of normal order. The Implications of Applicative Order\nScheme is a Lisp dialect . I told you that we weren\u0026rsquo;t going to work much with Lisp, but I lied. Sort of. Scheme is an applicative-order language with the same list-is-everything syntax as all other Lisps (see The Daily PL - 10/15/2021). In Scheme, you would define an if function named myif like this:\n(define (myif c t f) (cond (c t) (else f))) c is a boolean and myif returns t when c is true and f when c is false. No surprises.\nWe can define a name a and set its value to 5:\n(define a 5) Now, let\u0026rsquo;s call myif:\n(myif (= a 0) 1 (/ 1 a)) If a is equal to 0, then the call returns 1. Perfect. If a is not zero, the call returns the reciprocal of a. Given the value of a, the result is 1/7.\nLet\u0026rsquo;s define the name b and set its value to 0:\n(define b 0) Now, let\u0026rsquo;s call myif:\n(myif (= b 0) 1 (/ 1 b)) If b is equal to 0, then the call returns 1. If b is not zero, the call returns the reciprocal of b. Given the value of b, the result is 1:\n/: division by zero context...: \u0026#34;/home/hawkinsw/code/uc/cs3003/scheme/applicative/applicative.rkt\u0026#34;: [running body] temp37_0 for-loop run-module-instance!125 perform-require!78 That looks exactly like 1. What happened?\nRemember we said that the language is applicative order. No matter what the value of b, both of the arguments are going to be evaluated before myif starts. Therefore, Scheme attempts to evaluate 1 / b which is 1 / 0 which is division by zero.\nThanks to situations like this, the Scheme programming language is forced into defining special semantics for certain functions, like the built-in if expression. As a result, function invocation is not orthogonal in Scheme \u0026ndash; the general rules of function evaluation in Scheme must make an exception for applying functions like the built-in if expression. Remember that the orthogonality decreases as exceptions in a language\u0026rsquo;s specification increase. Sidebar: Solving the problem in Scheme\nFeel free to skip this section if you are not interested in details of the Scheme programming language. That said, the concepts in this section are applicable to other languages.\nIn Scheme, you can specify that the evaluation of an expression be delayed until it is forced.\n(define d (delay (/ 1 7))) defines d to be the eventual result of the evaluation of the division of 1 by 7. If we ask Scheme to print out d, we see\n#\u0026lt;promise:d\u0026gt; To bring the future tense into the present tense, we force a delayed evaluation:\n(force d) If we ask Scheme to print the result of that expression, we see:\n1/7 Exactly what we expect! With this newfound knowledge, we can rewrite the myif function:\n(define (myif c t f) (cond (c (force t)) (else (force f)))) Now myif can accept ts and fs that are delayed and we can use myif safely:\n(define b 0) (myif (= b 0) (delay 1) (delay (/ 1 b))) #+end_src lisp and we see the reasonable result: #+begin_src 1 Kotlin, a modern language, has a concept similar to delay called lazy . Ocaml, an object-oriented functional programming language, contains the same concept . Swift has some sense of laziness , too!\nWell, We Are Back to Normal Order I guess that we are stuck with the inefficiency inherent in the normal order function application. Going back to the dbl/average example, we will just have to live with invoking average twice.\nOr will we?\nReal-world functional programming languages that are normal order use an interesting optimization to avoid these recalculations! When an expression is passed around and it is unevaluated, Haskell and languages like it represent it as a thunk (Links to an external site.). The thunk data structure is generated in such a way that it can calculate the value of its associated expression some time in the future when the value is needed. Additionally, the thunk then caches (or memoizes) the value so that future evaluations of the associated expression do not need to be repeated.\nAs a result, in the dbl/average example,\n a thunk is created for (average 3 4 5), that thunk is passed to dbl, where it is duplicated during the reduction of dbl, (average 3 4 5) is (eventually) calculated for the first time using the thunk, 4 is stored (cached, memoized) in the thunk, and the cached/memoized value is retrieved from the thunk instead of evaluating (average 3 4 5) for a second time.  A thunk, then, is the mechanism that allows the programmer to have the efficiency of applicative order function invocation with the semantics of the normal order function invocation!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-27-2021/",
	"title": "10/27/2021",
	"tags": [],
	"description": "",
	"content": "The Tail That Wags the Dog There are no loops in functional programming languages. We\u0026rsquo;ve learned that, instead, functional programming languages are characterised by the fact that they use recursion for control flow. As we discussed earlier in the class (much earlier, in fact), when running code on a von Neumann machine, iterative algorithms typically perform faster than recursive algorithms because of the way that the former leverages the properties of the hardware (e.g., spatial and temporal locality of code ). In addition, recursive algorithms typically use much more memory than iterative algorithms.\nWhy? To answer that question, we will need to recall what we learned about stack frames. For every function invocation, an activation record (aka stack frame) is allocated on the run-time stack (aka call stack). The function\u0026rsquo;s parameters, local variables, and return value are all stored in its activation record. The activation record for a function invocation remains on the stack until it has completed execution. In other words, if a function f invokes a function g, then function f\u0026rsquo;s activation record remains on the stack until (at least) g has completed execution.\nConsider some algorithm A. Let\u0026rsquo;s assume that an iterative implementation of A requires n executions of a loop and l local integer variables. If that implementation is contained in a function, an invocation of that function would consume approximately l*sizeof(integer variable) + activation record overhead space on the runtime stack. Let\u0026rsquo;s further assume that a function implementing the recursive version of A also uses l local integer variables and also requires n executions of itself. Each invocation of the implementing function, then, needs l*sizeof(integer variable) + activation record overhead space on the runtime stack. Multiply that by n, the number of recursive calls, and you can see that, at it\u0026rsquo;s peak, executing the recursive version of algorithm A requires n * (l * sizeof(integer variable) + activation record overhead) space on the run-time stack. In other words, the recursive version requires n times more memory! That escalated quickly .\nThat\u0026rsquo;s all very abstract. Let\u0026rsquo;s look at the implications with a real-world example, which we will write in Haskell syntax:\nmyLen [] = 0 myLen (x:xs) = 1 + (myLen xs) myLen is a function that recursively calculates the length of a list. Let\u0026rsquo;s see what the stack would look like when we call myLen [1,2,3]:\n When the recursive implementation of myLen reaches the base case, there are four activation records on the run-time stack.\nAllocating space on the stack takes time. Therefore, the more activation records placed on the stack, the more time the program will take to execute. But, if we are willing to live with a slower program, then there\u0026rsquo;s nothing else to worry about.\nRight?\nWrong. Modern hardware is fast. Modern computers have lots of memory. Unfortunately, they don\u0026rsquo;t have an infinite amount of memory. A program only has a finite amount of stack space. Given a long enough list, myLen could cause so many activation records to be placed on the stack that the amount of stack space is exhausted and the program crashes. In other words, it\u0026rsquo;s not just that a recursive algorithm might execute slower, a recursive algorithm might fail to calculate the correct result entirely!\nTail Recursion - Hope The activation records of functions that recursively call themselves remain on the stack because, presumably, they need the result of the recursive invocation to complete their work. For example, in our myLen function, an invocation of myLen cannot completely calculate the length of the list given as a parameter until the recursive call completes.\nWhat if there was some way to rewrite a recursive function in a way that it did not need to wait on a future recursive invocation to completely calculate its result? If that could happen, then the stack frame of the current invocation of the function could be replaced by the stack frame of the recursive invocation. Why? Because the information contained in the current invocation of the function has no bearing on its overall result \u0026ndash; the only information needed to completely calculate the result of the function is the result of the future recursive invocation! The implementation of a recursive function that matches this specification is known as a tail-recursive function. The book says \u0026ldquo;A function is tail recursive if its recursive call is the last operation in the function.\u0026rdquo;\nWith a tail-recursive function, we get the expressiveness of a recursive definition of the algorithm along with the efficiency of an iterative solution! Ron Popeil, that\u0026rsquo;s a deal!\nRewriting The rub is that we need to figure out a way to rewrite those non-tail recursive functions into tail-recursive versions. I am not aware of any general purpose algorithms for such a conversion. However, there is one technique that is widely applicable: accumulators. It is sometimes possible to add a parameter to a non-tail recursive function and use that parameter to define a tail-recursive version. Seeing an accumulator in action is the easiest way to define the technique. Let\u0026rsquo;s rewrite myLen in a tail-recursive manner using an accumulator:\nmyLen list = myLenImpl 0 list myLenImpl acc [] = acc myLenImpl acc (x:xs) = myLenImpl (1 + acc) xs First, notice how we are turning myLen into a function that simply invokes a worker function whose job is to do the actual calculation! The first parameter to myLenImpl is used to hold a running tally of the length of the list so far. The first invocation of myLenImpl from the implementation of myLen, then, passes 0 as the argument to the accumulator because the running tally of the length so far is, well, 0. The implementation of myLenImpl adds 1 to that accumulator variable for every list item that is stripped from the front of the list. The result is that the result of an invocation of myLenImpl does not rely on the completion of a recursive execution. Therefore, myLenImpl qualifies as a tail-recursive function! Woah.\nLet\u0026rsquo;s look at the difference in the contents of the run-time stack when we use the tail-recursive version of myLen to calculate the length of the list [1,2,3]:\n A constant amount of stack space is being used \u0026ndash; amazing!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-29-2021/",
	"title": "10/29/2021",
	"tags": [],
	"description": "",
	"content": "No one Puts Locals In a Corner One of the really cool things about functional programming languages is their first-class support for functions. In functional programming languages, the programmer can pass functions as parameters to other functions and return functions from functions. Let\u0026rsquo;s take a closer look at functions that \u0026ldquo;generate\u0026rdquo; other functions. JavaScript has the capability to return functions from functions so we\u0026rsquo;ll use that language to explore:\nfunction urlGenerator(prefix) { function return_function(url) { return prefix + \u0026#34;://\u0026#34; + url; } return return_function; } The urlGenerator function takes a single parameter \u0026ndash; prefix. The caller of urlGenerator passes the protocol prefix as the argument (probably either \u0026ldquo;http\u0026rdquo; or \u0026ldquo;https\u0026rdquo;). The return value of urlGenerator is itself a function that takes a single parameter, a url, and returns url prepended with the prefix specified by the call to urlGenerator. An example might help:\nconst httpsUrlGenerator = urlGenerator(\u0026#34;https\u0026#34;); const httpUrlGenerator = urlGenerator(\u0026#34;http\u0026#34;); console.log(httpsUrlGenerator(\u0026#34;google.com\u0026#34;)); console.log(httpUrlGenerator(\u0026#34;google.com\u0026#34;)); generates\n\u0026#34;https://google.com\u0026#34; \u0026#34;http://google.com\u0026#34; In other words, the definition of httpsUrlGenerator is (conceptually)\nfunction httpsUrlGenerator(url) { return \u0026#34;https\u0026#34; + \u0026#34;://\u0026#34; + url; } and the definition of httpUrlGenerator is (conceptually)\nfunction httpUrlGenerator(url) { return \u0026#34;http\u0026#34; + \u0026#34;://\u0026#34; + url; } But that\u0026rsquo;s only a conceptual definition! The real definition continues to contain prefix:\nreturn prefix + \u0026#34;://\u0026#34; + url; But, prefix is locally scoped to the urlGenerator function. So, how can httpUrlGenerator and httpsUrlGenerator continue to use its value after leaving the scope of urlGenerator?\nThe Walls Are Closing In JavaScript, and other languages like it, have the concept of closures. A closure is a context that travels with a function returned by another function. Inside the closure are values for the free variables (remember that definition?) of the returned function. In urlGenerator, the returned function (return_function) uses the free variable prefix. Therefore, the closure associated with return_function contains the value of prefix at the time the closure is created!\nIn the example above, there are two different copies of return_function generated \u0026ndash; one when urlGenerator is called with the argument \u0026ldquo;http\u0026rdquo; and one when urlGenerator is called with the parameter \u0026ldquo;https\u0026rdquo;. Visually, the situation is\n Connections with Other Material Think about the connection between closures and the format of the urlGenerator/return_function functions and other concepts we\u0026rsquo;ve explored previously like partial application and currying.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-4-2021/",
	"title": "10/4/2021",
	"tags": [],
	"description": "",
	"content": "Original is here One day closer to Candy Corn!\nCorrections When we were discussing the nature of the type of pointers, we specified that the range of valid values for a pointer are all memory addresses. In some languages this may be true. However, some other languages specify that the range of valid values for a pointer are all memory addresses and a special null value that explicitly specifies a pointer does not point to a target.\nWe also discussed the operations that you can perform on a pointer-type variable. What we omitted was a discussion of an operation that will fetch the address of a variable in memory. For languages that use pointers to support indirect addressing (see below), such an operation is required. In C/C++, this operation is performed using the address of (\u0026amp;) operator.\nPointers We continued the discussion of pointers that we started on Friday! On Friday we discussed that pointers are just like any other type \u0026ndash; they have valid values and defined operations that the programmer can perform on those values.\nThe Pros of Pointers Though a very famous and influential computer scientist (Links to an external site.) once called his invention of null references a \u0026ldquo;billion dollar mistake\u0026rdquo; (he low balled it, I think!), the presence and power of pointers in a language is important for at least two reasons:\n Without pointers, the programmer could not utilize the power of indirection. Pointers give the programmer the power to address and manage heap-dynamic memory.  Indirection gives the programmer the power to link between different objects in memory \u0026ndash; something that makes writing certain data structures (like trees, graphs, linked lists, etc) easier. Management of heap-dynamic memory gives the programmer the ability to allocate, manipulate and deallocate memory at runtime. Without this power, the programmer would have to know before execution the amount of memory their program will require.\nThe Cons of Pointers Their use as a means of indirection and managing heap-dynamic memory are powerful, but misusing either can cause serious problems.\nPossible Problems when Using Pointers for Indirection As we said in the last lecture, as long as a pointer targets memory that contains the expected type of object, everything is a-okay. Problems arise, however, when the target of the pointer is an area in memory that does not contain an object of the expected type (including garbage) and/or the pointer targets an area of memory that is inaccessible to the program.\nThe former problem can arise when code in a program writes to areas of memory beyond their control (this behavior is usually an error, but is very common). It can also arise because of a use after free. As the name implies, a use-after-free error occurs when a program uses memory after it has been freed. There are two common scenarios that give rise to a use after free:\n Scenario 1:  One part of a program (part A) frees an area of memory that held a variable of type T that it no longer needs Another part of the program (part B) has a pointer to that very memory A third part of the program (part C) overwrites that \u0026ldquo;freed\u0026rdquo; area of memory with a variable of type S Part B accesses the memory assuming that it still holds a variable of Type T   Scenario 2:  One part of a program (part A) frees an area of memory that held a variable of type T that it no longer needs Part A never nullifies the pointer it used to point to that area of memory though the pointer is now invalid because the program has released the space A second part of the program (part C) overwrites that \u0026ldquo;freed\u0026rdquo; area of memory with a variable of type S Part A incorrectly accesses the memory using the invalid pointer assuming that it still holds a variable of Type T    Scenario 2 is depicted visually in the following scenario and intimates why use-after-free errors are considered security vulnerabilities:\n In the example shown visually above, the program\u0026rsquo;s use of the invalid pointer means that the user of the invalid pointer can now access an object that is at a higher privilege level (Restricted vs Regular) than the programmer intended. When the programmer calls a function through the invalid pointer they expect that a method on the Regular object will be called. Unfortunately, a method on the Restricted object will be called instead. Trouble!\nThe latter problem occurs when a pointer targets memory beyond the program\u0026rsquo;s control. This most often occurs when the program sets a variable\u0026rsquo;s address to 0 (NULL, null, nil) to indicate that it is invalid but later uses that pointer without checking its validity. For compiled languages this often results in the dreaded segmentation fault and for interpreted languages it often results in other anomalous behavior (like Java\u0026rsquo;s Null Pointer Exception (NPE)). Neither are good!\nPossible Solutions Wouldn\u0026rsquo;t it be nice if we had a way to make sure that the pointer being dereferenced is valid so we fall victim to some of the aforementioned problems? What would be the requirements of such a solution?\n Pointers to areas of memory that have been deallocated cannot be dereferenced. The type of the object at the target of a pointer always matches the programmer\u0026rsquo;s expectation.  Your author describes two potential ways of doing this. First, are tombstones. Tombstones are essentially an intermediary between a pointer and its target. When the programming language implements pointers and uses tombstones for protection, a new tombstone is allocated for each pointer the programmer generates. The programmer\u0026rsquo;s pointer targets the tombstone and the tombstone targets the pointer\u0026rsquo;s actual target. The tombstone also contains an extra bit of information: whether it is valid. When the programmer first instantiates a pointer to some target a the compiler/interpreter\n generates a tombstone whose target is a sets the valid bit of the tombstone to valid points the programmer\u0026rsquo;s pointer to the tombstone.  When the programmer dereferences their pointer, the compiler/runtime will check to make sure that the target tombstone\u0026rsquo;s valid flag is set to valid before doing the actual dereference of the ultimate target. When the programmer \u0026ldquo;destroys\u0026rdquo; the pointer (by releasing the memory at its target or by some other means), the compiler/runtime will set the target tombstone\u0026rsquo;s valid flag to invalid. As a result, if the programmer later attempts to dereference the pointer after it was destroyed, the compiler/runtime will see that the tombstone\u0026rsquo;s valid flag is invalid and generate an appropriate error.\nThis process is depicted visually in the following diagram.\nTombstones.png\nThis seems like a great solution! Unfortunately, there are downsides. In order for the tombstone to provide protection for the entirety of the program\u0026rsquo;s execution, once a tombstone has been allocated it cannot be reclaimed. It must remain in place forever because it is always possible that the programmer can incorrectly reuse an invalid pointer. As soon as the tombstone is deallocated, the protection that it provides is gone. The other problem is that the use of tombstones adds an additional layer of indirection to dereference a pointer and every indirection causes memory accesses. Though memory access times are small, they are not zero \u0026ndash; the cost of these additional memory accesses add up.\nWhat about a solution that does not require an additional level of indirection? There is a so-called lock-and-key technique. This protection method requires that the pointer hold an additional piece of information beyond the address of the target: the key. The memory at the target of the pointer is also required to hold a key. When the system allocates memory it sets the keys of the pointer and the target to be the same value. When the programmer dereferences a pointer, the two keys are compared and the operation is only allowed to continue if the keys are the same. The process is depicted visually below.\n With this technique, there is no additional memory access \u0026ndash; that\u0026rsquo;s good! However, there are still downsides. First, there is a speed cost. For every dereference there must be a check of the equality of the keys. Depending on the length of the key that can take a significant amount of time. Second, there is a space cost. Every pointer and block of allocated memory now must have enough space to store the key. For systems where memory allocations are done in big chunks, the relative size overhead of storing, say, and 8byte key is not significant. However, if the system allocates many small areas of memory, the relative size overhead is tremendous. Moreover, the more heavily the system relies on pointers the more space will be used to store keys rather than meaningful data.\nWell, let\u0026rsquo;s just make the keys smaller? Great idea. There\u0026rsquo;s only one problem: The smaller the keys the fewer unique key values. Fewer unique key values mean that it is more likely an invalid pointer randomly points to a chunk of memory with a matching key. In this scenario, the protection afforded by the scheme is vitiated. (I just wanted to type that word \u0026ndash; I\u0026rsquo;m not even sure I am using it correctly!)\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-6-2021/",
	"title": "10/6/2021",
	"tags": [],
	"description": "",
	"content": "Original is here.\nI love Reese\u0026rsquo;s Pieces.\nCorrections None to speak of!!\nPointers for Dynamic Memory Management We finished up our discussion of pointers in today\u0026rsquo;s class. In the previous class, we talked about how pointers have two important roles in programming languages:\n indirection \u0026ndash; referring to other objects dynamic memory management \u0026ndash; \u0026ldquo;handles\u0026rdquo; for areas of memory that are dynamically allocated and deallocated by the system.  On Monday we focused on the role of pointers in indirection and how to solve some of the problems that can arise from using pointers in that capacity. In today\u0026rsquo;s class, we focused on the role of pointers in dynamic memory management.\nAs tools for dynamic memory management, the programmer can use pointers to target blocks (N.B.: I am using blocks as a generic term for memory and am not using it in the sense of a block [a.k.a. page] as defined in the context of operating systems) of dynamic memory that are allocated and deallocated by the operating system for use by an application. The programmer can use these pointers to manipulate what is stored in those blocks and, ultimately, release them back to the operating system when they are no longer needed.\nMemory in the system is a finite resource. If a program repeatedly asks for memory from the system without releasing previous allocations back to the system, there will come a time when the memory is exhausted. In order to be able to release existing allocations back to the operating system for reuse by other applications, the programmer must not lose track of those existing allocations. When there is a memory allocation from the operating system to the application that can no longer be reached by a pointer in the application, that memory allocation is leaked. Because the application no longer has a pointer to it, there is no way for the application to release it back to the system. Leaked memory belongs to the leaking application until it terminates.\nFor some programs this is fine. Some applications run for a short, defined period of time. However, there are other programs (especially servers) that are written specifically to operate for extended periods of time. If such applications leak memory, they run the risk of exhausting the system\u0026rsquo;s memory resources and failing (Links to an external site.).\nPreventing Memory Leaks System behavior will be constrained when those systems are written in languages that do not support using pointers for dynamic memory management. However, what we learned (above) is that it is not always easy to use pointers for dynamic memory management correctly. What are some of the tools that programming languages provide to help the programmer manage pointers in their role as managers of dynamic memory.\nReference Counting In a reference-counted memory management system, each allocated block of memory given to the application by the system contains a reference count. That reference count, well, counts the number of references to the object. In other words, for every pointer to an operating-system allocated block of memory, the reference count on that block increases. Every time that a pointer\u0026rsquo;s target is changed, the programming language updates the reference counts of the old target (decrement) and the new target (increment), if there is a new target (the pointer could be changed to null, in which case there is no new target). When a block\u0026rsquo;s reference count reaches zero, the language knows that the block is no longer needed, and automatically returns it to the system! Pretty cool.\n The scenario depicted visually shows the reference counting process. At time (a), the programmer allocates a block of memory dynamically from the operating system and puts an application object in that block. Assume that the application object is a node in a linked list. The first node is the head of the list. Because the programmer has a pointer that targets that allocation, the block\u0026rsquo;s reference count at time (a) is 1. At time (b), the programmer allocates a second block of memory dynamically from the system and puts a second application object in that block \u0026ndash; another node in the linked list (the tail of the list). Because the head of the list is referencing the tail of the list, the reference count of the tail is 1. At time (c) the programmer deletes their pointer (or reassigns it to a different target) to the head of the linked list. The programming language decrements the reference count of the block of memory holding the head node and deallocates it because the reference count has dropped to 0. Transitively, the pointer from the head application object to the tail application object is deleted and the programming language decrements the reference count of its target, the block of memory holding the tail application object (time (d)). The reference count of the block of memory holding the tail application object is now 0 and so the programming language automatically deallocates the associated storage (time (e)). Voila \u0026ndash; an automatic way to handle dynamic memory management.\nThere\u0026rsquo;s only one problem. What if the programmer wants to implement a circularly linked list?\n Because the tail node points to the head node, and the head node points to the tail node, even after the programmer\u0026rsquo;s pointer to the head node is deleted or retargeted, the reference counts of the two nodes will never drop to 0. In other words, even with reference-counted automatic memory management, there could still be a memory leak! Although there are algorithms to break these cycles, it\u0026rsquo;s important to remember that reference counting is not a panacea. Python is a language that manages memory using reference counting.\nGarbage Collection Garbage collection (GC) is another method of automatically managing dynamically allocated memory. In a GC\u0026rsquo;d system, when a programmer allocates memory to store an object and no space is available, the programming language will stop the execution of the program (a so-called GC pause) to calculate which previously allocated memory blocks are no longer in use and can be returned to the system. Having freed up space as a result of cleaning up unused garbage, the allocation requested by the programmer can be satisfied and the execution of the program can continue.\nThe most efficient way to engineer a GC\u0026rsquo;d system is if the programming language allocates memory to the programmer in fixed-size cells. In this scenario, every allocation request from the programmer is satisfied by a block of memory from one of several banks of fixed-size blocks that are stacked back-to-back. For example, a programming language may manage three different banks \u0026ndash; one that holds reserves of X-sized blocks, one that holds reserves of Y-sized blocks and one that holds reserves of Z-sized blocks. When the programmer asks for memory to hold an object that is of size a, the programming language will deliver a block that is just big enough to that object. Because the size of the requested allocation may not be exactly the same size as one of the available fixed-size blocks, space may be wasted.\nThe fixed sizing of blocks in a GC\u0026rsquo;d system makes it easy/fast to walk through every block of memory. We will see shortly that the GC algorithm requires such an operation every time that it stops the program to do a cleanup. Without a consistent size, traversing the memory blocks would require that each block hold a tag indicating its size \u0026ndash; a waste of space and the cause of an additional memory read \u0026ndash; so that the algorithm could dynamically calculate the starting address of the next block.\nWhen the programmer requests an allocation that cannot be satisfied, the programming language stops the execution of the program and does a garbage collection. The classic GC algorithm is called mark and sweep and has three steps:\nEvery block of memory is marked as free using a free bit attached to the block. Of course, this is only true of some of the blocks, but the GC is optimistic! All pointers active at the time the program is paused are traced to their targets. The free bits of those blocks are reset. The blocks that are marked free and released.\nThe process is shown visually below:\n At times (a), (b) and (c), the programmer is allocating and manipulating references to dynamically allocated memory. At time (c), the allocation request for variable z cannot be satisfied because there are no available blocks. A GC pause starts at time (d) and the mark-and-sweep algorithm commences by setting the free bit of every block. At time (e) the pointers are traced and the appropriate free bits are cleared. At time (f) the memory is released from the unused block and its free bit, too, is reset. At time (g) the allocation for variable z can be satisfied, the GC pause completes and the programming language restarts execution of the program.\nThis process seems great, just like reference counting seemed great. However, there is a significant problem: The programmer cannot predict when GC pauses will occur and the programmer cannot predict how long those pauses will take. A GC pause is completely initiated by the programming language and (usually) completely beyond the control of the programmer. Such random pauses of program execution could be extremely harmful to a system that is controlling a system that needs to keep up with interactions from the outside world. For instance, it would be totally unacceptable for an autopilot system to take an extremely long GC pause as it calculates the heading needed to land a plane. There are myriad other systems where pauses are inappropriate.\nThe mark-and-sweep algorithm described above is extremely naive and GC algorithms are the subject of intense research. Languages like go and Java manage memory with a GC and their algorithms are incredibly sophisticated. If you want to know more, please let me know!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-1-2021/",
	"title": "11/1/2021",
	"tags": [],
	"description": "",
	"content": "Your Total Is \u0026hellip; In today\u0026rsquo;s class, we started with writing a simple function to sum the numbers in a list and ended up with the definition of a fundamental operation of functional programming: the fold. Let\u0026rsquo;s start by writing the simple, recursive definition of a sum function in Haskell:\nsimpleSum [] = 0 simpleSum (first:rest) = first + (simpleSum rest) When invoked with the list [1,2,3,4], the result is 10:\n*Summit\u0026gt; simpleSum [1,2,3,4] 10 Exactly what we expected. Let\u0026rsquo;s think about our job security: The boss tells us that they want a function does \u0026ldquo;products\u0026rdquo; all the elements in the list. Okay, that\u0026rsquo;s easy:\nsimpleProduct [] = 1 simpleProduct (first:rest) = first * (simpleProduct rest) When invoked with the list [1,2,3,4], the result is 24:\n*Summit\u0026gt; simpleProduct [1,2,3,4] 24 Notice that there are only minor differences between the two functions: the value returned in the base case (0 or 1) and the operation being performed on head and the result of the recursive invocation.\nI hear some of your shouting at me already: This isn\u0026rsquo;t tail recursive; you told us that tail recursive functions are important. Fine! Let\u0026rsquo;s rewrite the two functions so that they are tail recursive. We will do so using an accumulator and a helper function:\ntrSimpleSum list = trSimpleSumImpl 0 list trSimpleSumImpl runningTotal [] = runningTotal trSimpleSumImpl runningTotal (x:xs) = trSimpleSumImpl (runningTotal + x) xs When invoked with the list [1,2,3,4], the result is 10:\n*Summit\u0026gt; trSimpleSum [1,2,3,4] 10 And, we\u0026rsquo;ll do the same for the function that calculates the product of all the elements in the list:\ntrSimpleProduct list = trSimpleProductImpl 1 list trSimpleProductImpl runningTotal [] = runningTotal trSimpleProductImpl runningTotal (x:xs) = trSimpleProductImpl (runningTotal * x) xs When invoked with the list [1,2,3,4], the result is 24:\n*Summit\u0026gt; trSimpleProduct [1,2,3,4] 24 One of These Things is Just Like The Other Notice the similarities between trSimpleSumImpl and trSimpleProductImpl. Besides the names, the only difference is really the operation that is performed on the runningTotal and the head element of the list. Because we\u0026rsquo;re using a functional programming language, what if we wanted to let the user specify that operation in terms of a function parameter? Such a function would need have to accept two arguments (the up-to-date running total and the head element) and return a new running total. For summing, we might write a sumOperation function:\nsumOperation runningTotal headElement = runningTotal + headElement Next, instead of defining trSimpleSumImpl and trSimpleProductImpl with fixed definitions of their operation, let\u0026rsquo;s define a trSimpleOpImpl that could use sumOperation:\ntrSimpleOpImpl runningTotal operation [] = runningTotal trSimpleOpImpl runningTotal operation (x:xs) = trSimpleOpImpl (operation runningTotal x) operation xs Fancy! Now, let\u0026rsquo;s use trSimpleOpImpl and sumOperation to recreate trSimpleSum from above:\ntrSimpleSum list = trSimpleOpImpl 0 sumOperation list Let\u0026rsquo;s check to make sure that we get the same results: When invoked with the list [1,2,3,4], the result is 10:\n*Summit\u0026gt; trSimpleSum [1,2,3,4] 10 To confirm our understanding of what\u0026rsquo;s going on here, let\u0026rsquo;s visualize the invocations of sumOperation necessary to complete the calculation of trSimpleSum:\nsumOperation 0 1 sumOperation 1 2 sumOperation 3 3 sumOperation 6 4 Let\u0026rsquo;s do a similar thing for trSimpleProduct:\nproductOperation runningTotal headElement = runningTotal * headElement trSimpleProduct list = trSimpleOpImpl 0 productOperation list Let\u0026rsquo;s check to make sure that we get the same results: When invoked with the list [1,2,3,4], the result is 24:\n*Summit\u0026gt; trSimpleProduct [1,2,3,4] 24 Think About the Types: We\u0026rsquo;ve stumbled on a pretty useful pattern! Let\u0026rsquo;s look at its components:\n A \u0026ldquo;driver\u0026rdquo; function (called trSimpleOpImpl) that takes three parameters: an initial value (of a particular type, T), an operation function (see below) and a list of inputs, each of which is of type T. An operation function that takes two parameters \u0026ndash; a running total, of some type R; an element to \u0026ldquo;accumulate\u0026rdquo; on to the running total of type T \u0026ndash; and returns a new running total of type R. A list of inputs, each of which is of type T.  Here are the types of those functions in Haskell:\noperation function: R -\u0026gt; T -\u0026gt; R\nlist of inputs: [T]\ndriver function: T -\u0026gt; (R -\u0026gt; T -\u0026gt; R) -\u0026gt; R\nLet\u0026rsquo;s play around and see what we can write using this pattern. How about a concatenation of a list of strings in to a single string?\nconcatenateOperation concatenatedString newString = concatenatedString ++ newString concatenateStrings list = trSimpleOpImpl \u0026#34;\u0026#34; concatenateOperation list (the ++ just concatenates two strings together). When we run this on [\u0026ldquo;Hello\u0026rdquo;, \u0026ldquo;,\u0026rdquo;, \u0026ldquo;World\u0026rdquo;] the result is \u0026ldquo;Hello, World\u0026rdquo;:\n*Summit\u0026gt; concatenateStrings [\u0026#34;Hello\u0026#34;, \u0026#34;,\u0026#34;, \u0026#34;World\u0026#34;] \u0026#34;Hello,World\u0026#34; So far our Ts and Rs have been the same \u0026ndash; integers and strings. But, the signatures indicate that they could be different types! Let\u0026rsquo;s take advantage of that! Let\u0026rsquo;s use trSimplOpImpl to write a function that returns True if every element in the list is equal to the number 1 and False otherwise. Let\u0026rsquo;s call the operation function continuesToBeAllOnes and define it like this:\ncontinuesToBeAllOnes equalToOneSoFar maybeOne = equalToOneSoFar \u0026amp;\u0026amp; (maybeOne == 1) This function will return True if the list (to this point) has contained all ones (equalToOneSoFar) and the current element (maybeOne) is equal to one. In this case, the R is a boolean and the T is an integer. Let\u0026rsquo;s implement a function named isListAllOnes using continuesToBeAllOnes and trSimplOpImpl:\nisListAllOnes list = trSimpleOpImpl True continuesToBeAllOnes list Does it work? When invoked with the list [1,1,1,1], the result is True:\n*Summit\u0026gt; isListAllOnes [1,1,1,1] True When invoked with the list [1,2,1,1], the result is False:\n*Summit\u0026gt; isListAllOnes [1,2,1,1] False Naming those \u0026ldquo;operation\u0026rdquo; functions every time is getting annoying, don\u0026rsquo;t you think? I bet that we could be lazier!! Let\u0026rsquo;s rewrite isListAllOnes without specifically defining the continuesToBeAllOnes function:\nisListAllOnes list = trSimpleOpImpl True (\\equalToOneSoFar maybeOne -\u0026gt; equalToOneSoFar \u0026amp;\u0026amp; (maybeOne == 1)) list Now we are really getting functional!\nI am greedy. I want to write a function that returns True if any element of the list is a one:\nisAnyElementOne list = trSimpleOpImpl False (\\anyOneSoFar maybeOne -\u0026gt; anyOneSoFar || (maybeOne == 1)) list This is just way too much fun!\nFold the Laundry This type of function is so much fun that it is included in the standard implementation of Haskell! It\u0026rsquo;s called fold! And, in true Haskell fashion, there are two different versions to maximize flexibility and confusion: the fold-left and fold-right operation. The signatures for the functions are the same in both cases:\nfold[l,r] :: operation function -\u0026gt; initial value -\u0026gt; list -\u0026gt; result\nIn all fairness, these two versions are necessary. Why? Because certain operation functions are not associative! It doesn\u0026rsquo;t matter the order in which you add or multiply a series of numbers \u0026ndash; the result of (5 * (4 * (3 *2))) is the same as (((5 * 4) * 3) * 2). The problem is, that\u0026rsquo;s not the case of an operation like division!\nA fold-left operation (foldl) works by starting the operation (essentially) from the first element of the list and the fold-right operation (foldr) works by starting the operation (essentially) from the last element of the list. Furthermore, the choice of foldl vs foldr affects the order of the parameters to the operation function: in a foldl, the running value (which is known as the accumulator in mainstream documentation for fold functions) is the left parameter; in a foldr, the accumulator is the right parameter.\nThis will make more sense visually, I swear:\n*Summit\u0026gt; foldl (\\x y -\u0026gt; x /y ) 1 [3,2,1] 0.16666666666666666  *Summit\u0026gt; foldr (\\x y -\u0026gt; x / y ) 1 [3,2,1] 1.5  Let\u0026rsquo;s use our newfound fold power, to recreate our work from above:\nisAnyElementOne list = foldl (\\anyOneSoFar maybeOne -\u0026gt; anyOneSoFar || (maybeOne == 1)) False list isListAllOnes list = foldl (\\equalToOneSoFar maybeOne -\u0026gt; equalToOneSoFar \u0026amp;\u0026amp; (maybeOne == 1)) True list concatenateStrings list = foldl (\\concatenatedString newString -\u0026gt; concatenatedString ++ newString) \u0026#34;\u0026#34; list "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-10-2021/",
	"title": "11/10/2021",
	"tags": [],
	"description": "",
	"content": "In today\u0026rsquo;s class we learned about a workhorse rule in logic programming: append/3. append/3 can be used to implement many other different rules, including a rule that will generate all the permutations of a list!\nPin the Tail on the List The goal (pun intended) of append is to determine whether two lists, X and Y, are the same as a third list, Z, when appended together. We could use the append query like this:\n?- append([1,2,3], [a,b,c], [1,2,3,a,b,c]). true. or\n?- append([1,2,3], [a,b], [1,2,3,a,b,c]). false. What we will find is that append/3 has a hidden superpower besides its ability to simply answer yes/no queries like the ones above.\nThe definition of append/3 will follow the pattern of other recursively defined rules that we have seen so far. Let\u0026rsquo;s start with the base case. The result of appending an empty list with some list Y, is just list Y. Let\u0026rsquo;s write that down:\nappend([], Y, Y). And now for the recursive case: appending list X to list Y yields some list Z where Z is the first element of the list X following by the result of appending the tail of X with Y. The natural language version of the definition is complicated but I think that the Prolog definition makes it more clear:\nappend([H|T], Y, [H|AppendedList]) :- append(T, Y, AppendedList). Let\u0026rsquo;s see how Prolog attempts to answer the query append([1,2], [a,b], [1,2,a,b]).\n And now let\u0026rsquo;s look at it\u0026rsquo;s operation for the query append([1,2], [a,b], [1,a,b]).\n It\u0026rsquo;s also natural to look at append/3 as a tool to \u0026ldquo;assign\u0026rdquo; a variable to be the result of appending two lists:\n?- append([1,2,3], [a,b,c], Z). Z = [1, 2, 3, a, b, c]. Here we are asking Prolog to assign variable Z to be a list that holds the appended contents of the lists in the first two arguments.\nLook at Append From a Different Angle We\u0026rsquo;ve seen append/3 in action so far in a procedural way \u0026ndash; answering whether two lists are equal to one another and \u0026ldquo;assigning\u0026rdquo; a variable to a list that holds the contents of another two lists appended to one another. But earlier I said that append/3 has some magical powers.\nIf we look at append/3 from a different angle, the declarative angle, we can see how it can be used to generate all the different combinations of two lists that, when appended together, yield a third list! For example,\n?- append(X, Y, [1,2,3]). X = [], Y = [1, 2, 3] ; X = [1], Y = [2, 3] ; X = [1, 2], Y = [3] ; X = [1, 2, 3], Y = [] ; Wow. That\u0026rsquo;s pretty cool! Prolog is telling us that it can figure out three different combinations of lists that, when appended together, will equal the list [1,2,3]. I mean, if that doesn\u0026rsquo;t make your blood boil, I don\u0026rsquo;t know what will.\nLet\u0026rsquo;s Ride the Thoroughbred The power of append/3 makes it useful in so many different ways. When I started learning Prolog, the resource I was using (Learn Prolog Now (Links to an external site.)) spent an inordinate amount of time discussing append/3 and it\u0026rsquo;s utility. It took me a long time to really understand the author\u0026rsquo;s point. A long time.\nPrefix and Suffix Let\u0026rsquo;s take a quick look at how to define a rule prefix/2. prefix/2 takes two arguments \u0026ndash; a possible prefix, PP, and a list, L \u0026ndash; and determines whether PP is a prefix of L. We\u0026rsquo;ve gotten so used to writing recursive definitions, it seems obvious that we would define prefix/2 using that pattern. In the base case, an empty list is a prefix of any list:\nprefix([], _). (Remember that _ is \u0026ldquo;I don\u0026rsquo;t care.\u0026quot;). With that out of the way, we can say that PP is a prefix of L if\nthe head element of PP is the same as the head element of L, and the tail of PP is a prefix of the tail of L:\nprefix([H|Xs], [H|Ys]) :- prefix(Xs, Ys). Fantastic. That\u0026rsquo;s a pretty easy definition and it works in all the ways that we would expect:\n?- prefix([1,2], [1,2,3]). true. ?- prefix([1,2], [2,3]). false. ?- prefix(X, [1,2,3]). X = [] ; X = [1] ; X = [1, 2] ; X = [1, 2, 3] ; But, what if there was a way that we could write the definition of prefix/2 more succinctly! Remember, programmers are lazy \u0026ndash; the fewer keystrokes the better!\nThink about this alternate definition of prefix: PP is a prefix of L, when there is a (possibly empty) list, W (short for \u0026ldquo;whatever\u0026rdquo;), such that PP appended with W is equal to L. Did you see the magic word? Appended! We have append/3, so let\u0026rsquo;s use it:\nprefix(PP, L) :- append(PP, _, L). (Note that we have replaced W from a natural-language definition with _ because we don\u0026rsquo;t care about it\u0026rsquo;s value!)\nWe could go through the same process of defining suffix/2 recursively, or we could cut to the chase and define it in terms of append/3. Let\u0026rsquo;s save ourselves some time: SS is a suffix of L, when there is a (possibly empty) list, W (short for \u0026ldquo;whatever\u0026rdquo;), such that W appended with SS is equal to L. Let\u0026rsquo;s codify that:\nsuffix(SS, L) :- append(_, SS, L). But, does it work?\n?- suffix_append([3,4], [1,2,3,4]). true. ?- suffix_append([3,5], [1,2,3,4]). false. ?- suffix_append(X, [1,2,3,4]). X = [1, 2, 3, 4] ; X = [2, 3, 4] ; X = [3, 4] ; X = [4] ; X = [] ; Permutations We\u0026rsquo;re all friends here, aren\u0026rsquo;t we? Good. I have no problem admitting that I am terrible at thinking about permutations of a set. I have tried and tried and tried to understand the section in Volume 4 of Knuth\u0026rsquo;s TAOCP (Links to an external site.) about generating permutations (Links to an external site.) but it\u0026rsquo;s just too hard for me. Instead, I just play around with them until I grok it. To help me play, I want Prolog\u0026rsquo;s help. I want Prolog to generate for me all the permutations of a given list. We will call this permute/2. Procedurally, permute/2 will say whether its second argument is a permutation of its first argument. Declaratively, permute/2 will generate a list of permutations of elements in the list in its first argument. Let\u0026rsquo;s work back from the end: We\u0026rsquo;ll see how it should work before actually defining it:\n?- permutation([1,2,3], [3,1,2]). true . ?- permutation([1,2,3], L). L = [1, 2, 3] ; L = [1, 3, 2] ; L = [2, 1, 3] ; L = [2, 3, 1] ; L = [3, 1, 2] ; L = [3, 2, 1] ; false. Cool. If I run permute/2 enough times I might start to understand them!\nNow that we know the results of invoking permute/2, how are we going to define it? Again, let\u0026rsquo;s take the easy way out and see the definition and then walk through it piece-by-piece in order to understand its meaning:\npermute([], []). permute(List, [X|Xs]) :- append(W, [X|U], List), append(W, U, ListWithoutX), permute(ListWithoutX, Xs). Well, the first rule is simple \u0026ndash; the permutation of an empty list is just the empty list!\nThe second rule, well, not so much! There are three conditions that must be satisfied for a list defined as [X|Xs] to be a permutation of List.\nThink of the first condition in a declarative sense: \u0026ldquo;Prolog, make me two lists that, when appended together, equal List. Name the first one W. And, while you\u0026rsquo;re at it, make sure that the first element in the second list is X and call the tail of the second list U. Thanks.\u0026rdquo;\nThink of the second condition in a procedural sense: \u0026ldquo;Prolog, append W and U to create a new list named ListWithoutX.\u0026rdquo; The name ListWithoutX is pretty evocative because, well, it is a list that contains every element of List but X.\nFinally, think of the third condition in a declarative sense: \u0026ldquo;I want Xs to be all the possible permutations of ListWithoutX \u0026ndash; Prolog, make it so!\u0026rdquo;\nLet\u0026rsquo;s try to put all this together into a succinct natural-language definition: A list whose first element is X and whose tail is Xs is a permutation of List if:\n X is one of the elements of List, and Xs is a permutation of the list List without element X.  Below is a visualization of the process Prolog takes to generate the first two permutations of [1,2,3]:\n "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-12-2021/",
	"title": "11/12/2021",
	"tags": [],
	"description": "",
	"content": "If the append/3 predicate that we wrote on Wednesday is a horse that we can ride to accomplish many different tasks, then Prolog is like a wild stallion that tends to run in a direction of its choosing. We can use cuts to tame our mustang and make it go where we want it to go!\nThe Possibilities Are Endless Let\u0026rsquo;s start the discussion by writing a merge/3 predicate. The first two arguments are sorted lists. The final argument should unify to the in-order version of the first two arguments merged. Before starting to write some Prolog, let\u0026rsquo;s think about how we could do this.\nLet\u0026rsquo;s deal with the base cases first: When either of the first two lists are empty, the merged list is the non-empty list. We\u0026rsquo;ll write that like\nmerge(Xs, [], Xs). merge([], Ys, Ys). And now, for the recursive cases: We will call the first argument Left, the second argument Right, and the third argument Sorted. The first element of Left can be called HeadLeft and the rest of Left can be called TailLeft. The first element of Right can be called HeadRight and the rest of Right can be called TailRight. In order to merge, there are three cases to consider:\n HeadLeft is less than HeadRight HeadLeft is equal to HeadRight HeadRight is less than HeadLeft  For case (1), the head of the merged result is HeadLeft and the tail of the merged result is the result of merging TailLeft with Right. For case (2), the head of the merged result is HeadLeft and the tail of the merged result is the result of merging TailLeft with TailRight. For case (3), the head of the merged result is HeadRight and the tail of the merged result is the result of merging Left with TailRight.\nIt\u0026rsquo;s far easier to write this in Prolog than English:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X\u0026lt;Y, merge(Xs, [Y|Ys], Zs). merge([X|Xs], [Y|Ys], [X|Zs]) :- X=:=Y, merge(Xs, Ys, Zs). merge([X|Xs], [Y|Ys], [Y|Zs]) :- Y\u0026lt;X, merge([X|Xs], Ys, Zs). (Note: : is the \u0026ldquo;equal to\u0026rdquo; boolean operator in Prolog. See =:=/2 (Links to an external site.) for more information.\nFor merge/3, let\u0026rsquo;s write the base cases after our recursive cases. With that switcheroo, we have the following complete definition of merge/3:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X\u0026lt;Y, merge(Xs, [Y|Ys], Zs). merge([X|Xs], [Y|Ys], [X|Zs]) :- X=:=Y, merge(Xs, Ys, Zs). merge([X|Xs], [Y|Ys], [Y|Zs]) :- Y\u0026lt;X, merge([X|Xs], Ys, Zs). merge(Xs, [], Xs). merge([], Ys, Ys). Let\u0026rsquo;s follow Prolog as it attempts to use our predicate to answer the query\nmerge([1, 3, 5], [2,4,6], M). As we know, Prolog will search top to bottom when looking for ways to unify and the first rule that Prolog sees is applicable:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X\u0026lt;Y, merge(Xs, [Y|Ys], Zs). Why? Because Prolog sees X as 1 and Y as 2 and 1 \u0026lt; 2. Therefore, Prolog will complete its unification for this query by replacing it with another query:\nmerge([3,5], [2,4,6], Zs).  Once Prolog has completed that query, the response comes back:\nM = [1,2,3,4,5,6] Unfortunately, that\u0026rsquo;s not the whole story. While Prolog is off attempting to satisfy the subquery merge([3,5], [2,4,6], Zs)., it believes that there are still several other viable alternatives for satisfying our original query:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X=:=Y, merge(Xs, Ys, Zs). merge([X|Xs], [Y|Ys], [Y|Zs]) :- Y\u0026lt;X, merge([X|Xs], Ys, Zs). merge(Xs, [], Xs). merge([], Ys, Ys). The result is that Prolog will have to use memory to remember those alternatives. As the lists that we ask Prolog to merge get longer and longer, that memory will have an impact on the system\u0026rsquo;s performance. However, we know that those other alternatives will never match and keeping them around is a waste. How do we know that? Well, if X \u0026lt; Y then it cannot be equal to Y and it certainly cannot be greater than Y. Moreover, the lists in the first two arguments cannot be empty. Overall, each of the possible rules for merge/3 are mutually exclusive. You can only choose one.\nIf there were a way to tell Prolog that those other possibilities are impossible after it encounters a matching rule that would save Prolog from having to keep them around. The good news is that there is!\nWe can use the cut (Links to an external site.) operator to tell Prolog that once it has \u0026ldquo;descended\u0026rdquo; down a particular path, there is no sense backtracking beyond that point to look for alternate solutions. The technical definition of a cut is\n Discard all choice points created since entering the predicate in which the cut appears. In other words, commit to the clause in which the cut appears and discard choice points that have been created by goals to the left of the cut in the current clause.\n Let\u0026rsquo;s rewrite our merge/3 predicate to take advantage of cuts and save some overhead:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X\u0026lt;Y, !, merge(Xs, [Y|Ys], Zs). merge([X|Xs], [Y|Ys], [X|Zs]) :- X=:=Y, !, merge(Xs, Ys, Zs). merge([X|Xs], [Y|Ys], [Y|Zs]) :- Y\u0026lt;X, !, merge([X|Xs], Ys, Zs). merge(Xs, [], Xs) :- !. merge([], Ys, Ys) :- !. Returning to the definition of cut, in the first rule we are telling Prolog (through our use of the cut) to disregard all choice points created to the left of the !. In particular, we are telling Prolog to forget about the choice it made that X \u0026lt; Y. The result is that Prolog is no longer under the impression that there are other rules that are applicable. Visually, the situation resembles\nMerge Cut.png\nDr. Cutyll and Mr. Unify Cuts are not always so beneficial. In fact, their use in Prolog is somewhat controversial. A cut necessarily limits Prolog\u0026rsquo;s ability to backtrack. If the Prolog programmer uses a cut in a rule that is meant to be used declaratively (in order to generate values) and procedurally, then the cut may change the results.\nThere are two types of cuts. A green cut is a cut that does not change the meaning of a predicate. The cut that we added in the merge/3 predicate above is a green cut. A red cut is, technically speaking, a cut that is not a green cut. I know that\u0026rsquo;s a satisfying definition. Sorry. The implication, however, is that a red cut is a cut that changes the meaning of predicate.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-15-2021/",
	"title": "11/15/2021",
	"tags": [],
	"description": "",
	"content": "Red Alert At the end of lecture on Friday, we discussed the two different types of cuts \u0026ndash; red and green. A green cut is one that does not alter the behavior of a Prolog program. A red cut does alter the behavior of a Prolog program. The implication was that red cuts are bad and green cuts are good. But, is this always the case?\nTo frame our discussion, let\u0026rsquo;s look at a Prolog program that performs a Bubble Sort on a list of numbers. The predicate, bsort/2, is defined like this:\nbsort(Unsorted, Sorted):- append(Left, [A, B | Right], Unsorted), B\u0026lt;A, append(Left, [B, A | Right], MoreSorted), bsort(MoreSorted, Sorted). bsort(Sorted, Sorted). The first rule handles the recursive case (when there is at least one list item out of order) and the second rule handles the base case (when the list is sorted). According to this definition, how does Prolog handle the query\nbsort([2,1,3,5], M). Prolog (as we are well aware) searches its rule top to bottom. The first rule that it sees is the recursive case and Prolog immediately opts for it. Opting for this case is not without consequences \u0026ndash; a choice point is created. Why? Because Prolog made the choice to attempt to satisfy the query according to the first rule and not the (just as applicable) second rule! Let\u0026rsquo;s call this choice point A.\nNext, Prolog attempts to unify the subquery append(Left, [A, B | Right], Unsorted). The first (of many) unifications that append/3 generates is Left = [], A = 2, B = 1, Right = [3, 5]. By continuing with this particular unification, Prolog creates another choice point. For what reason? Well, Prolog knows that append/3 could generate other potential unifications and it will need to check those to see if they, too, satisfy the original query! Let\u0026rsquo;s call this choice point B. Next, Prolog checks whether A is less than B \u0026ndash; it is. Prolog continues in a like manner to satisfy the final two subgoals of the rule.\nWhen Prolog does satisfy those, it has generated a sorted version of [2,1,3,5]. It reports that result to the querier. Great! There\u0026rsquo;s only one problem. There are still choice points that Prolog wants to explore. In particular, there are choice points A and B. In this case, we can forget about choice point B because there are no other unifications of append/3 that meet the criteria for A\u0026lt;B in Unsorted. In other words, visually the situation looks like this:\n If the querier is using bsort/2 declaratively, this scenario is a problem: Prolog will backtrack to choice point A and then consider the base case rule for bsort/2. In other words, Prolog will also generate\n[2,1,3,5] as a unification! This is clearly not right. What\u0026rsquo;s more, the Prolog programmer could query for\nbsort([3,2,1,4,5], Sorted). in which choice point B will have consequences. Run this query for yourself (using trace), examine the results, and make sure you understand why the results are generated.\nSo, what are we to do? cut/0 (Links to an external site.) to the rescue! Logically speaking, once our bsort/3 rule finds a single element out of order and we adjust that element, the recursive call to itself will handle ordering any other unordered elements. In other words, we can forget any earlier choice points! This is exactly what the cut is intended for!\nLet\u0026rsquo;s rewrite bsort/3 using cuts and see if our problem is sorted (see what I did there?):\nbsort(Unsorted, Sorted):- append(Left, [A, B | Right], Unsorted), B\u0026lt;A, !, append(Left, [B, A | Right], MoreSorted), bsort(MoreSorted, Sorted). bsort(Sorted, Sorted). And now let\u0026rsquo;s see how our earlier troublesome queries perform:\n?- bsort([2,1,3,5], Sorted). Sorted = [1, 2, 3, 5]. ?- bsort([3,2,1,4,5], Sorted). Sorted = [1, 2, 3, 4, 5]. Amazing!\nHere\u0026rsquo;s the rub: The version of our Prolog program with the cut gave different results than the version without. Is this cut a green or a red cut? That\u0026rsquo;s right, it\u0026rsquo;s a red cut. I guess there are such things as good red cuts after all!\nThe Fundamentals As we have said in the past, there is a theoretical underpinning for each of the programming paradigms that we have studied this semester. Logic programming is no different. The theory behind logic programming is first-order predicate logic. Predicate logic is an extension of propositional logic. Propositional logic is based on the evaluation of propositions.\nA proposition is simply any statement that can be true or false. For example,\n Will is a programmer. Alicia is a programmer. Sam is a programmer.  Those statements can be true or false. In propositional logic we can build more complicated propositions from existing propositions using logical connectives like and, or, not, and implication (if \u0026hellip; then). Each of these has an associated truth table to determine the truth of two combined propositions.\nLook closely at the example propositions above and you will notice an underlying theme: they all do with someone (let\u0026rsquo;s call them x) being a programmer. In propositional logic, our ability to reason using that underlying theme is impossible. We can only work with the truth of the statement as a whole.\nIf we add to propositional logic variables, constants, and quantifiers then we get predicate logic and we are able to reason more subtly. Although you might argue that propositional logic has variables, everyone can agree that they are limited \u0026ndash; they can only have two values, true and false. In first-order predicate logic, variables can have domains other than just {T, F}. That\u0026rsquo;s already a huge step!\nQuantifiers \u0026ldquo;define\u0026rdquo; variables and \u0026ldquo;constrain\u0026rdquo; their possible values. There are two quantifiers in first-order predicate logic \u0026ndash; the universal and the existential. The universal is the \u0026ldquo;for all\u0026rdquo; (aka \u0026ldquo;every\u0026rdquo;) quantifier. We can use the universal quantifier to write statements (in logic) like \u0026ldquo;Every person is a Bearcats fan.\u0026rdquo; Symbolically, we write the universal quantifier with the \\(\\forall\\). We can write our obvious statement from above in logic like\n.org-center { margin-left: auto; margin-right: auto; text-align: center; }  \\(\\forall x(person(x) \\Longrightarrow fan(x, bearcats))\\)\n Now we are talking! As for the existential quantifier, it allows us to write statements (in logic) like \u0026ldquo;There is some person on earth that speaks Russian.\u0026rdquo; Symbolically, we write the existential quantifier with the \\(\\exists\\). We can write the statement about our Russian-speaking person as\n.org-center { margin-left: auto; margin-right: auto; text-align: center; }  \\(\\exists x(person(x) \\land speaks(x, russion))\\)\n How are quantifiers embedded in Prolog? Variables in Prolog queries are existentially qualified \u0026ndash; \u0026ldquo;Does there exist …?\u0026ldquo;​ Variables in Prolog rules are universally quantified \u0026ndash; \u0026ldquo;For all ….\u0026rdquo;\nIn first-order predicate logic, there are such things as Boolean-valued functions. This is familiar territory for us programmers. A Boolean-valued function is a function that has 0 or more parameters and returns true or false.\nWith these definitions, we can now define predicates: A predicate is a proposition in which some Boolean variables are replaced by Boolean-valued functions and quantified expressions. Let\u0026rsquo;s look at an example.\n.org-center { margin-left: auto; margin-right: auto; text-align: center; }  \\(p \\Longrightarrow q\\)\n is a proposition where p and q are boolean variables. Replace p with the Boolean-valued function \\(is\\_teacher(x)\\) and q with the quantified expression \\(\\exists y(student(x) \\land teaches(x, russion))\\) and we have the predicate\n.org-center { margin-left: auto; margin-right: auto; text-align: center; }  \\(is\\_teacher(x) \\Longrightarrow \\exists y(student(x) \\land teaches(x,y))\\)\n There is only one remaining question: Why is it called first-order predicate logic and not, say, higher-order predicate logic? \u0026ldquo;First-order\u0026rdquo; here indicates that the predicates in this logic cannot manipulate or reason about predicates themselves. Does this sound familiar? Imperative languages can define functions but they cannot reason about functions themselves while functional languages can!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-17-2021/",
	"title": "11/17/2021",
	"tags": [],
	"description": "",
	"content": "So far in this course we have covered lots of material. We\u0026rsquo;ve learned lots of definitions, explored lots of concepts and programmed in languages that we\u0026rsquo;ve never seen before. In all that time, though, we never really got to the bottom of one thing: What exactly is a language? In the final module of this course, we are going to cover exactly that!\nBack to Basics Let\u0026rsquo;s think way back to August 23 and recall two very important definitions: syntax and semantics. On that day we defined semantics as the effect of each statement\u0026rsquo;s execution on the program\u0026rsquo;s operation. We defined syntax as the rules for constructing structurally valid (note: valid, not correct) computer programs in a particular language. There\u0026rsquo;s that word again \u0026ndash; language.\nBefore starting to define language, let me offer you two alternate definitions for syntax and semantics that draw out the distinction between the two:\n The syntax of a programming language specifies the form of its expressions, statements and program units. The semantics of a programming language specifies the meaning of its expressions, statements and program units.  It is interesting to see those two definitions side-by-side and realize that they are basically identical with the exception of a single word! One final note about the connection between syntax and semantics before moving forward: Remember how a well-designed programming language has a syntax that is evocative of meaning. In other words, a well-designed language might allow variables to contain a symbol like ? which would allow the programmer to indicate that it holds a Boolean.\nBefore we can specify a syntax for a programming language, we need to specify the language itself. In order to do that, we need to start by defining the language\u0026rsquo;s alphabet \u0026ndash; finite set of characters that can be used to write sentences in that language. We usually denote the alphabet of a language with the \\(\\sum\\). It is sometimes helpful to denote the set of all the possible sentences that can be written using the characters in the alphabet. We usually denote that with \\(\\sum\\). For example, say that \\(sum = \\{a, b\\}\\), then \\(\\sum = \\{a, b, aa, ab, ba, aaa, aab, aba, abb, \u0026hellip;\\}\\). Notice that even though \\(|\\sum|\\) is finite (that is, the number of elements in is finite), \\(|\\sum| = \\infty\\).\nThe alphabet of the C++ programming language is large, but it\u0026rsquo;s not infinite. However, the set of sentences that can be written using that alphabet is. But, as we all learn early in our programming career, just because you can write out a program using the valid symbols in the C++ alphabet does not mean the program is syntactically valid. The very job of the compiler is to distinguish between valid and invalid programs, right?\nLet\u0026rsquo;s call the language that we are defining \\(L\\) and say that it\u0026rsquo;s alphabet is \\(\\sum\\). \\(L\\) can be thought of as the set of all valid sentences in the language. Therefore, every sentence that is in \\(L\\) is in \\(\\sum\\) \u0026ndash; \\(L \\subseteq \\sum\\).\n Look closely at the relationship between \\(\\sum\\) and \\(L\\). While \\(L\\) never includes a sentence that is not included in \\(\\sum\\), they can be identical! Think of some languages where any combination of characters from its alphabet are valid sentences! Can you come up with one?\nThe Really Bad Programmer So, how can we determine whether a sentence made up of characters from the alphabet is in the language or not? We have to be able to answer this question \u0026ndash; one of the fundamental jobs of the compiler, after all, is to do exactly that. Why not just write down every single valid sentence in a giant chart and compare the program with that list? If the program is in the list, then it\u0026rsquo;s a valid program! That\u0026rsquo;s easy.\nNot so fast. Aren\u0026rsquo;t there an infinite number of valid C++ programs?\nint main() { if (true) { if (true) { if (true) { ... std::cout \u0026lt;\u0026lt; \u0026#34;Hello, World.\u0026#34;; } } } } Well, dang. There goes that idea.\nI guess we need a concise way to specify how to describe sentences that are in the language and those that are not. Before we start to think about ways to do that, let\u0026rsquo;s think back to Prolog. One of the really neat things about Prolog was the ability to specify something that had, at the same time, an ability to recognize and generate valid solutions to a set of constraints. Even in our simplest Prolog example, we could write down a set of facts and the language could determine whether a student took a particular class (giving us a yes/no answer) or generate a list of the people who were taking a particular class!\nIt would be great to have a tool that does the same thing for languages \u0026ndash; a common description that allows us to create something that recognizes and generates valid sentences in the language. We will do exactly that!\nLanguage Classes The famous linguist Noam Chomsky was the first to recognize how there is a hierarchy of languages. The hierarchy is founded upon the concept of how easy/hard it is to write a concise definition for the language\u0026rsquo;s valid sentences.\n Each level of Chomsky\u0026rsquo;s Hierarchy, as it is called, contains the levels that come before it. Which languages belong to which level of the hierarchy is something that you will cover more in CS4040. (Note: The languages that belong to the Regular level can be specified by regular expressions (Links to an external site.). Something I know some of you have written before!)\nFor our purposes, we are going to be concerned with those languages that belong to the Context-Free level of the hierarchy.\nContext-Free Grammars The tool that we can use to concisely specify a Context-Free language is called a Context-Free Grammar. Precisely, a Context-Free Grammar, , is a set of productions , a set of terminal symbols, , a set of non-terminal symbols, , one of which is named\nand is known as the start symbol.\nThat\u0026rsquo;s a ton to take in. The fact of the matter, however, is that the vocabulary is intuitive once you see and use a grammar. So, let\u0026rsquo;s do that. We will define a grammar for listing/recognizing all the positive integers:\n Now that we see an example of a grammar and its parts and pieces, let\u0026rsquo;s \u0026ldquo;read\u0026rdquo; it to help us understand what it means. Every valid integer in the language can be derived by writing down the start symbol and iteratively replacing every non-terminal according to a production until there are only terminals remaining! Again, that sounds complicated, but it\u0026rsquo;s really not. Let\u0026rsquo;s look at the derivation for the integer 142:\n We start with the start symbol Integer and use the first production to replace it with one of the two options in the production. We choose to use the second part of the production and replace Integer with Integer Digit. Next, we use a production to replace Integer with Integer Digit again. At this point we have Integer Digit Digit. Next, we use one of the productions to replace Integer with Digit and we are left with Digit Digit Digit. Our next move is to replace the left-most Digit non-terminal with a terminal \u0026ndash; the 1. We are left with 1 Digit Digit. Pressing forward, we replace the left-most Digit with 4 and we come to 14Digit. Almost there. Let\u0026rsquo;s replace Digit with 2 and we get 142. Because 142 contains only terminals, we have completed the derivation. Because we can derive 142 from the grammar, we can confidently say that 142 is in the language described by the grammar.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-19-2021/",
	"title": "11/19/2021",
	"tags": [],
	"description": "",
	"content": "(Context-free) Grammars (CFG) are a mathematical description of a language-generation mechanism. Every grammar defines a language. The strings in that language are those that can be derived from the grammar\u0026rsquo;s start symbol. Put another way, a CFG describes a process by which all the sentences in a language can be enumerated, or generated.\nDefining a way to generate all the sentences in a grammar is one way to specify a language. Another way to define a language is to build a tool that separates sentences that are in the language from sentences that are not in the language. This tool is known as a recognizer. There is a relationship between recognizers and generators and we will explore that in future lectures.\nGrammatical Errors On Wednesday we worked with a grammar that purported to describe all the strings that looked like integers:\n We performed a derivation using its productions to prove (by construction) that 142 is in the language. But, let\u0026rsquo;s consider this derivation:\n We have just proven (again, by construction) that 012 is an integer. This seems funny. A number that starts with 0 (that is not just a 0) should not be deemed an integer. Let\u0026rsquo;s see how we can fix this problem.\nIn order to guarantee that we cannot derive a number that starts with a 0 and deem it an integer, we will need to add a few more productions. First, let\u0026rsquo;s reconsider the production for the start symbol Integer. At the very highest level, we know that 0 is an integer. So, our start symbol\u0026rsquo;s production should handle that case.\nWith the base case accounted for, we next consider that an integer cannot start with a zero, but it can start with any other digit between 1 and 9. It seems handy to have a production for a non-terminal that expands to the terminals 1 through 9. We will call that non-terminal Nzd for non-zero digits. After the initial non-zero digit, an integer can contain zero or more digits between 0 and 9 (we can call this the rest of the integer). For brevity, we probably want a production that will expand to all the digits between 0 and 9. Let\u0026rsquo;s call that non-terminal Zd for zero digits. We\u0026rsquo;ll define it either as a 0 or anything in Nzd. If we put all of this together, we get the following grammar:\n Let\u0026rsquo;s look at a few derivations to see if this gets the job done. Is 0 an integer?\n And a single-digit integer?\n How about an integer between 10 and 100, inclusive?\n We are really cooking. Let\u0026rsquo;s try one more. Is a number greater than 100 derivable?\n Think about this grammar and see if you can see any problems with it. Are there integers that cannot be derived? Are there sentences that can be derived that are not integers?\nTrees or Derivations Let\u0026rsquo;s take a look at a grammar that will generate sentences that are simple mathematical expressions using the + and * operators and the numbers 0 through 9:\n Until now we have used proof-by-construction through derivations to show that a particular string is in the language described by a particular grammar. What\u0026rsquo;s really cool is that any derivation can also be written in the form of a tree. The two representations contain the same information \u0026ndash; a proof that a particular sentence is in a language. Let\u0026rsquo;s look at the derivation of the expression 1 + 5 * 2:\n There\u0026rsquo;s only one problem with this derivation: Our choice of which production to apply to expand the start symbol was arbitrary. We could have just as easily used the second production and derived the expression 1 + 5 * 2:\n This is clearly not a good thing. We do not want our choice of productions to be arbitrary. Why? When the choice of the production to expand is arbitrary, we cannot \u0026ldquo;encode\u0026rdquo; in the grammar any semantic meaning. (We will learn how to encode that semantic meaning below). A grammar that produces two or more valid parse trees for a particular sentence is known as an ambiguous grammar.\nLet Me Be Clear The good news is that we can rewrite the ambiguous grammar for mathematical expressions from above and get an unambiguous grammar. Like the way that we rewrote the grammar for integers, rewriting this grammar will involve adding additional productions:\n Using this grammar we have the following parse tree for the expression 1 + 5 * 2:\n Before we think about how to encode semantic meaning in to a parse tree, let\u0026rsquo;s talk about the properties of a parse tree. The root node of the parse tree is always the start symbol of the grammar. The internal nodes of a parse tree are always non-terminals. The leaves of a parse tree are always terminals.\nMaking Meaning If we adopt a convention that reading a parse tree occurs through a depth-first in-order traversal, we can add meaning to these beasts and their associated grammars. First, we can see how associativity is encoded: By writing all the productions so that \u0026ldquo;recursion\u0026rdquo; happens on the left side of any terminals (a so-called left-recursive production), we will arrive at a grammar that is left-associative. The converse is true \u0026ndash; the productions whose recursion happens on the right side of any terminals (a right-recursive production) specifies right associativity. Second, we can see how precedence is encoded: The further away a production is from the start symbol, the higher the precedence. In other words, the precedence is inversely related to the distance from the start symbol. All alternate options for the same production have equal precedence.\nLet\u0026rsquo;s look back at our grammar for Expr. A Term and an addition operation have the same precedence. A Factor and a multiplication operation have the same precedence. The production for an addition operation is left-recursive and, therefore, the grammar specifies that addition is left associative. The same is true for the multiplication operation.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-22-2021/",
	"title": "11/22/2021",
	"tags": [],
	"description": "",
	"content": "Before reading this Daily PL, please re-read the one from Friday, 11/19/2021. The content has been updated since it was originally published to reflect certain errors with the \u0026ldquo;better\u0026rdquo; Integer grammar we developed in class. Thanks, especially, to Jeroen for making sure that I got the grammar correct!\nMaking Meaning In an earlier part of the class we talked about how to describe the dynamic semantics of a program. We worked closely with operational semantics but saw other examples, too (axiomatic and denotational). Although we said that what we are studying in this module is the syntax of a language and not its semantics, a language may have static semantics that the compiler can check during syntax analysis. Syntax analysis is done using a tool known as the parser.\nRemember that syntax and syntax analysis is only concerned with valid programs. If the program, considered as a string of letters of the language\u0026rsquo;s alphabet, can be derived from the language grammar\u0026rsquo;s start symbol, then the program is valid. We all agreed that was the limit of what a syntax analyzer could determine.\nOne form of output of a parser is a parse tree. It is possible to apply \u0026ldquo;decoration\u0026rdquo; to the parse tree in order to verify certain extra-syntactic properties of a program at the time that it is being parsed. These properties are known as a language\u0026rsquo;s static semantics. More formally, Sebesta defines static semantics as the rules for a valid program in a particular language that are difficult (or impossible!) to encode in a grammar. Checking static semantics early in the compilation process is incredibly helpful for programmers as they write their code and allows the stages of the compilation process after syntax analysis to make more sophisticated assumptions about a program\u0026rsquo;s validity.\nOne example of static semantics of a programming language has to do with its type system. Checking a program for type safety is possible at the time of syntax analysis using an attribute grammar. An attribute grammar is an extension of a CFG that adds (semantic) information to its terminals and nonterminals.​ This information is known as attributes. Attached to each production are attribute calculation functions. At the time the program is parsed, the attribute calculation functions are evaluated every time that the associated production is used in a derivation and the results of that invocation are stored in the nodes of the parse tree that represent terminals and nonterminals. Additionally, in an attribute grammar, each production can have a set of predicates. Predicates are simply Boolean-valued functions. When a parser attempts to use a production during parsing, it\u0026rsquo;s not just the attribute calculation functions that are invoked \u0026ndash; the production\u0026rsquo;s predicates are too. If any of those predicates returns false, then the derivation fails. An Assignment Grammar\nTo start class, we looked at a snippet of an industrial-strength grammar \u0026ndash; the one for the C programming language. The snippet we saw concerned the assignment expression: There are lots of details in the grammar for an assignment statement in C and not all of them pertain to our discussion. So, instead of using that grammar of assignment statements, we\u0026rsquo;ll use a simpler one:\n Assume that this is part of a larger grammar for a complete, statically typed programming language. In the subset of the grammar that we are going to work with, there are three terminals: A, B and C. These are variable names available for the programmer in our language. As a program in this language is parsed, the compiler builds a mapping between variables and their types (known as the symbol table), adding an entry for each new variable declaration it encounters. The compiler can \u0026ldquo;lookup\u0026rdquo; the type of a variable using its name thanks to the lookup function.\nOur hypothetical language contains only two numerical types: int and real. A variable can only be assigned the result of an expression if that expression has the same type as the variable. In order to determine the type of an expression, our language adheres to the following rules:\n Let\u0026rsquo;s assume that we are looking at the following program written in our hypothetical language:\nint A; real B; A = A + B; The declarations of the variables A and B are handled by parts of the grammar that we are not showing in our example. Again, when those declarations are parsed, an entry is made in the symbol table so that variable names can be mapped to types. Let\u0026rsquo;s derive A = A + B; using the snippet of the grammar shown above:\n Great! The program passes the syntactic analysis so it\u0026rsquo;s valid!\n Right?\nThis Grammar Goes To 11 Wrong. According to the language\u0026rsquo;s type rules, we can only assign to variables that have the same type as the expression being assigned. The rules say that A + B is a real (a.ii). A is declared as an int. So, even though the program parses, it is invalid!\nWe can solve this using attribute grammars and that\u0026rsquo;s exactly what we are going to do! For our Assign grammar, we will add the following attributes to each of the terminals and nonterminals:\nexpected_type: The type that is expected for the expression. actual_type: The actual type of the expression.\nThe values for the attributes are set according to functions. An attribute whose calculation function uses attribute values from only its children and peer nodes in the parse tree is known as a synthesized attribute. An attribute whose calculation function uses only attribute values from its parent nodes in the parse tree is known as an inherited attribute.\n Let\u0026rsquo;s define the attribute calculation functions for the expected_type and actual_type attributes of the Assign grammar:\n For this production, we can see that the expression\u0026rsquo;s expected_type attribute is defined according to the variable\u0026rsquo;s actual_type which means that it is an inherited attribute.\n For this production, we can see that the expression\u0026rsquo;s actual_type attribute is defined according to the variable\u0026rsquo;s actual_type which means that it is a synthesized attribute.\nAnd now for the most complicated (but not complex) attribute calculation function definition:\n Again, we can see that the expression\u0026rsquo;s actual_type attribute is defined according to its children nodes \u0026ndash; the actual_type of the two variables being added together \u0026ndash; which means that it is a synthesized attribute.\nIf you are thinking that the attribute calculation functions are recursive, you are exactly right! And, you can probably see a problem straight ahead. So far the attribute calculation functions have relied on attributes of peer, children and parent nodes in the parse tree to already have values. Where is our base case?\nGreat question. There\u0026rsquo;s a third type of attribute known as an intrinsic attribute. An intrinsic attribute is one whose value is calculated according to some information outside the parse tree. In our case, the actual_type attribute of a variable is calculated according to the mapping stored in the symbol table and accessible by the lookup function that we defined above.\n That\u0026rsquo;s all for the definition of the attribute calculation functions and is all well and good. Unfortunately, we still have not used our attributes to inform the parser when a derivation has gone off the rails by violating the type rules. We\u0026rsquo;ll define these guardrails predicate functions and show the complete attribute grammar at the same time:\n The equalities after the checkmarks are the predicates. We can read them as \u0026ldquo;If the actual type of the expression is the same as the expected type of the predicate, then the derivation (parse) can continue. Otherwise, it must fail because the assignment statement being parsed violates the type rules.\u0026rdquo; Put The Icing On The Cookie\nThe process of calculating the attributes of a production during parsing is known as decorating the parse tree. Let\u0026rsquo;s look at the parse tree from the assignment statement A = A + B; and see how it is decorated:\n "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-29-2021/",
	"title": "11/29/2021",
	"tags": [],
	"description": "",
	"content": "Recognizing vs Generating We have talked at length about how, depending on your vantage point, you can look at predicates in Prolog as either declarative or procedural. Viewed from a declarative perspective, the predicates will generate all the values of a variable that will make the predicate true. Viewed from the other direction, predicates look more like traditional boolean-valued functions in a procedural (or OOP) programming language. The dichotomy between the declarative and procedural view has parallels in syntax and grammars. From one end of the PL football stadium, grammars are generators; from the other endzone they are recognizers.\nWe have seen the power of the derivation and the parse tree to generate strings that are in a language L defined by a grammar G. We can create a valid sentence in language L by repeated application of the productions in G to the sentential forms derived from G\u0026rsquo;s start symbol. Applying all the productions in all possible combinations will eventually enumerate all valid strings in the language L (don\u0026rsquo;t hold your breath for that process to finish!).\nThe only problem is that (with one modern exception ), our compilers don\u0026rsquo;t actually generate source code for us! It is the programmer \u0026ndash; us! \u0026ndash; who writes the source code and the compiler that checks whether it is a valid program. There are obviously myriad ways in which a programmer can write an invalid program in a particular programming language and the compiler can detect all of them. However, the easiest invalid programs for the compiler to detect are the ones that are not syntactically correct.\nTo reiterate, (most) programming languages specify their syntax using a (context-free) grammar (CFG) \u0026ndash; the theoretical language L that we\u0026rsquo;ve talked about as being defined by a grammar G can actually be a programming language! Therefore, the source code for a program is technically just a particular sequence of terminals from L\u0026rsquo;s alphabet. For that sequence of terminals to be a valid program, it must be the final sentential form in some derivation following the productions of G.\nIn other words, the compiler is not a generator but rather a recognizer.\nParsers Recall from Chapter 2 of Sebesta (or earlier programming courses), the stages of compilation. The stage/tool that recognizes whether a particular sequence of terminals from a language\u0026rsquo;s alphabet is a valid program or not (the recognizer) is called parsing/a parser. Besides recognizing whether a program is valid, parsers convert the source code for a program written in a particular programming language defined according to a specific grammar into a parse tree.\n What\u0026rsquo;s sneaky is that the parsing process is really two processes: lexical analysis and syntax analysis. Lexical analysis turns the bytes on the disk into a sequence of tokens (and associated lexemes). Syntax analysis turns the tokens into a parse tree.\nWe\u0026rsquo;ve seen several examples of languages defined by grammars and those grammars contain productions, terminals and nonterminals. We haven\u0026rsquo;t seen any tokens, though, have we? Well, tokens are an abstract way to represent groups of bytes in a file of source code in an abstract manner. The actual bytes that are in a group that are bundled together stay associated with the token and are known as lexemes. Tokenizing the input makes the syntax analysis process easier. Note: Read Sebesta\u0026rsquo;s discussion about the reason for separating lexical analysis from syntax analysis in Section 4.1 on (approximately pg. 143). Turtles All The Way Down\nRemember the Chomsky Hierarchy of languages? Context-free languages can be described by CFGs. Slightly less complex languages are known as regular languages. Regular languages can be recognized by a logical apparatus known as a finite automata (FA). If you have ever written a regular expression then you have actually written a stylized FA. Cool, right?? You will learn more about FAs in CS4040, but for now it\u0026rsquo;s important to know the broad outlines of the definition of an FA because of the central role they play in lexical analysis. An FA is\n A (finite) set of states, S; A (finite) alphabet, A; A transition function, \\(f : S, A \\rightarrow S\\), that takes two parameters (the current state [an element in S] and an element from the alphabet) and returns a new state; A start state (one of the states in S); and A set of accepting states (a subset of the states in S).  Why does this matter? Because we can describe how to group the bytes on disk into tokens (and lexemes) using regular languages. You probably have a good idea about what is going on, but let\u0026rsquo;s dig in to an example \u0026ndash; that\u0026rsquo;ll help!\nLexical Analysis of Mathematical Expressions For the remainder of this edition, let\u0026rsquo;s go back to the (unambiguous) grammar of mathematical expressions:\n Here\u0026rsquo;s a syntactically correct expression with labeled tokens and lexemes:\n What do you notice? Well, the first thing that I notice is that in most cases, the lexeme value is actually, completely, utterly useless. For instance, what other logical grouping of bytes other than the one that represents the ) will be converted in to the CLOSE_PAREN token?\nThere is, however, one token whose lexeme is very important: the Id token. Why? Because that token can be any number! The actual lexeme of that Id makes a big difference when it comes time to actually evaluate the mathematical expression that we are parsing (if that is, indeed, the goal of the operation!).\nCertitude and Finitude Let\u0026rsquo;s take a stab at defining the FA that we can use to convert a stream of bytes on disk in to a stream of tokens (and associated lexemes). Let yourself slip in to a world where you are nothing more than a robot with a simple memory: you can be in one of two states. Depending on the state that you are in, you are able to perform some specific actions depending on the next character that you see. The two states are BEGIN and NUMBER. When you are in the BEGIN state and you see a ), +, *, or ( then you simply emit the appropriate token and stay in the BEGIN state. If you are in the BEGIN state and you see a digit, you simply store that digit and then change your state to NUMBER. When you are in the NUMBER state, if you see a digit, you store that digit after all the digits you\u0026rsquo;ve previously seen and stay in the same state. However, when you are in the NUMBER state and you see a ), +, *, or (, you must perform three actions:\n Convert the string of digits that you are remembering into a number and emit a Id token with the converted value as the lexeme; Emit the token appropriate to the value you just saw; and Reset your state to BEGIN.  For as long as there are bytes in the input file, you continue to perform those operations robotically. The operations that I just described are the FA\u0026rsquo;s state transition function! See how each operation depends on the current state and an input? Those are the parameters to the state transition function! And see how we specified the state we will be in after considering the input? That\u0026rsquo;s the function\u0026rsquo;s output! Woah!\nWhat\u0026rsquo;s amazing about all those words that I just wrote is that they can be turned into a picture that is far easier to understand:\n Make the connection between the different aspects of the written and graphical description above and the formal components of an FA, especially the state transition function. In the image above, the state transition function is depicted with the gray arrows!\nThe Last Step\u0026hellip; There\u0026rsquo;s just one more step \u0026hellip; we want to convert this high-level description of a tokenizer into actual code. And, we can! What\u0026rsquo;s more, we can use a technique we learned earlier this semester in order to do it! Consider that the tokenizer will work hand-in-hand with the syntax analyzer. As the syntax analyzer goes about its business of creating a parse tree, it will occasionally turn to the lexical analyzer and say, \u0026ldquo;Give me the next token!\u0026rdquo;. In between answers, it would be helpful if the tokenizer could maintain its state.\nIf that doesn\u0026rsquo;t sound like a coroutine, then I don\u0026rsquo;t know what does! Because a coroutine is dormant and not dead between invocations, we can easily program the tokenizer as a coroutine so that it remembers its state (either BEGIN or NUMBER) and other information (like the current digits that it is has seen [if it is in the NUMBER state] and the progress it has made reading the input file). Kismet!\nTo see such an implementation in Python, check here (Links to an external site.).\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-3-2021/",
	"title": "11/3/2021",
	"tags": [],
	"description": "",
	"content": "It\u0026rsquo;s the 3rd day of November and we are about to switch to non-daylight savings time. What better way to celebrate the worst day of the year than switching our attention to a new topic!\nI Do Declare In this module we are going to focus on logic (also known as declarative) programming languages. Users of a declarative programming language declare the outcome they wish to achieve and let the compiler do the work of achieving it. This is in marked contrast to users of an imperative programming language who have to tell the compiler not only the outcome they wish to achieve but also how to achieve it. A declarative programming language does not have program control flow constructs, per se. Instead, a declarative programming language gives the programmer the power to control execution by means of recursion (again?? I know, sorry!) and backtracking. Backtracking is a concept that we will return to later. A declarative program is all about defining facts (either as axioms or as ways to build new facts from axioms) and asking questions about those facts. From the programming language\u0026rsquo;s perspective, those facts have no inherent meaning. We, the programmers, have to impugn meaning on to the facts. Finally, declarative programming languages do have variables, but they are nothing the variables that we know and love in imperative programming languages.\nAs we have worked through the different programming paradigms, we have discussed the theoretical underpinning of each. For imperative programming languages, the theoretical model is the Turing Machine. For the functional programming languages, the theoretical model is the Lambda Calculus. The declarative programming paradigm has a theoretical underpinning, too: first-order predicate calculus. We will talk more about that in class soon!\nIn the Beginning Unlike imperative, object-oriented and functional programming languages, there is really only one extant declarative/logic programming language: Prolog.Prolog was developed by Alain Colmeraurer, Phillipe Roussel, and Robert Kowalski in order to do research in artificial intelligence and natural language processing\n. Its official birthday is 1972.\n Prolog programs are made up of exactly three components:\n Facts Rules Queries  The syntax of Prolog defines the rules for writing facts, rules and queries. Syntactic elements in Prolog belong to one of three categories:\n Atoms: The most fundamental unit of a Prolog program. They are simply symbols. Usually they are simply sequences of characters that begin with a lowercase letter. However, atoms can contain spaces (in which case they are enclosed in \u0026rsquo;s) and they can start with uppercase letters (in which case they are wrapped with \u0026rsquo;s). Variables: Like atoms, but variables always start with uppercase letters. Functors: Like atoms, but functors define relationships/facts.  If Prolog is a logic programming language, there must be some means for specifying logical operations. There is! In the context of specifying a rule, the and operation is written using a ,. In the context of specifying a rule, the or operation is written using a ;. Obviously!\nThe best way to learn Prolog is to start writing some programs. We\u0026rsquo;ll come back to the theory later!\nJust The Facts At its most basic, a Prolog program is a set of facts:\ntakes(jane, cs4999). takes(alicia, cs2020). takes(alice, cs4000). takes(mary, cs1021). takes(bob, cs1021). takes(kristi, cs4000). takes(sam, cs1021). takes(will, cs2080). takes(alicia, cs3050).  In relation to the first-order predicate logic that Prolog models, takes is a logical predicate. We\u0026rsquo;ll refer to them as facts or predicates, depending on what\u0026rsquo;s convenient. Formally, they predicates and facts are written as /. Two (or more) facts/predicates with the same functor but different arities are not the same. For instance, takes/1 and takes/2 are completely different.\nLet\u0026rsquo;s read one of these facts in English:\ntakes(jane, cs4999). could be read as \u0026ldquo;Jane takes CS4999.\u0026rdquo;. As programmers, we know what that means: the student named Jane is enrolled in the class CS4999. However, Prolog does not share our sense of meaning! Prolog simply thinks that we are defining one element of the takes relationship where jane is somehow related to cs4999. As a Prolog programmer, we could just has easily have written\ntennis_shoes(jane, cs4999). tennis_shoes(alicia, cs2020). tennis_shoes(alice, cs4000). tennis_shoes(mary, cs1021). tennis_shoes(bob, cs1021). tennis_shoes(kristi, cs4000). tennis_shoes(sam, cs1021). tennis_shoes(will, cs2080). tennis_shoes(alicia, cs3050). and gotten the same result! But, we programmers want to define something that is meaningful, so we choose to use atoms that reflect their semantic meaning. With nothing more than the facts that we have defined above, we can write queries. In order to interact with queries in real time, we can use the Prolog REPL. Once we have started the Prolog REPL, we will see a prompt like this:\n?- The world awaits \u0026hellip;\nTo load a Prolog file in to the REPL, we will use the consult predicate:\n?- consult(\u0026#39;intro.pl\u0026#39;). true. The Prolog facts, rules and queries in the intro.pl file are now available to us. Assume that intro.pl contains the takes facts from above. Let\u0026rsquo;s make some queries:\n?- takes(bob, cs1021). true. ?- takes(will, cs2080). true. ?- takes(ali, cs4999). false. These are simple yes/no queries and Prolog obliges us with terse answers. But, even with the simple facts shown above, Prolog can be used to make some powerful inferences. Prolog can tell us the names of all the people it knows who are taking a particular class:\n?- takes(Students, cs1021). Students = mary ; Students = bob ; Students = sam. Wow! Here Prolog is telling us that there are three different values of the Students variable that will make the query true: mary, bob and sam. In the lingo, Prolog is unifying Students with the values that will make our query true. Let\u0026rsquo;s go the other way around:\n?- takes(alicia, Classes). Classes = cs2020 ; Classes = cs3050. Here Prolog is telling us that there are two different classes that Alicia is taking. Nice.\nThat\u0026rsquo;s great, but pretty limited: it\u0026rsquo;s kind of terrible if we had to write out each fact explicitly! The good news is that we don\u0026rsquo;t have to do that! We can use a Prolog rule to define facts based on the existence of other facts. Let\u0026rsquo;s define a rule which will tell us the students who are CS majors. To be a CS major, you must be taking (exactly) two classes:\ncs_major(X) :- takes(X, Y), takes(X, Z), Y @\u0026lt; Z. That\u0026rsquo;s lots to take in at first glance. Start by looking at the general format of a rule:\n Okay, so now back to our particular rule that defines what it means to be a CS Major. (For the purposes of this discussion, assume that the @\u0026lt; operator is \u0026ldquo;not equal\u0026rdquo;). Building on what we know (e.g., , is and, :- is implication, etc), we can read the rule like: \u0026ldquo;X is a CS Major if X takes class Y and X takes class Z and class Y and Z are not the same class.\u0026rdquo; Pretty succinct definition.\nTo make the next few examples a little more meaningful, let\u0026rsquo;s update our list of facts before moving on:\ntakes(jane, cs4999). takes(alicia, cs2020). takes(alice, cs4000). takes(mary, cs1021). takes(bob, cs1021). takes(bob, cs8000). takes(kristi, cs4000). takes(sam, cs1021). takes(will, cs2080). takes(alicia, cs3050). With that, let\u0026rsquo;s find out if our rule works as intended!\n?- cs_major(alicia). true ; false. Wow! Pretty cool! Prolog used the rule that we wrote, combined it with the facts that it knows, and inferred that Alicia is a CS major! (For now, disregard the second False \u0026ndash; we\u0026rsquo;ll come back to that!). Like we could use Prolog to generate the list of classes that a particular student is taking, can we ask Prolog to generate a list of all the CS majors that it knows?\n?- cs_major(X). X = alicia ; X = bob ; false. Boom!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-5-2021/",
	"title": "11/5/2021",
	"tags": [],
	"description": "",
	"content": "*The retreat to move forward.\nBacktracking In today\u0026rsquo;s lecture, we discussed the concepts of backtracking and choice points in order to learn how Prolog can determine the meaning of our programs.\nThere is a formal definition of choice points and backtracking from the Prolog glossary:\nbacktracking: Search process used by Prolog. If a predicate offers multiple clauses to solve a goal, they are tried one-by-one until one succeeds. If a subsequent part of the proof is not satisfied with the resulting variable binding, it may ask for an alternative solution, causing Prolog to reject the previously chosen clause and try the next one.\nThere are lots of undefined words in that definition! Let\u0026rsquo;s dig a little deeper.\nA predicate is like a boolean function. A predicate takes one or more arguments and yields true/false. As we said at the outset of our discussion about declarative programming languages, the reason that a predicate may yield true or false depends on the semantics imposed upon it from the outside. A predicate is a term borrowed from first-order logic, a topic that we will return to in later lectures.\nRemember our takes example from previous lectures? takes is a predicate! takes has two arguments and returns a boolean.\nIn Prolog, rules and facts are written to define predicates. A rule defines the conditions under which a predicate is true using a body \u0026ndash; a list of other predicates, logical conjunctives, implications, etc. See The Daily PL - 11/3/2021 for additional information about rules. A fact is a rule without a body and unconditionally defines that a certain relationship is true for a predicate.\nrelated(will, bill). related(ali, bill). related(bill, william). related(X, Y) :- related(X, Z), related(Z, Y).\nIn the example above, related is a predicate defined by facts and rules. The facts and rules above are the clauses of the predicate.\nchoicepoint: A choice point represents a choice in the search for a solution. Choice points are created if multiple clauses match a query or using disjunction (;/2). On backtracking, the execution state of the most recent choice point is restored and search continues with the next alternative (i.e., next clause or second branch of ;/2).\nThat\u0026rsquo;s a mouthful! I think that the best way to understand this is to look at backtracking in action and see where choice points exist.\nGive and Take Remember the takes predicate that we defined in the last class:\ntakes(jane, cs4999). takes(alicia, cs2020). takes(alice, cs4000). takes(mary, cs1021). takes(bob, cs1021). takes(bob, cs8000). takes(kristi, cs4000). takes(sam, cs1021). takes(will, cs2080). takes(alicia, cs3050). We subsequently defined a cs_major predicate:\ncs_major(X) :- takes(X, Y), takes(X, Z), Y @\u0026lt; Z. The cs_major predicate says that a student who takes two CS classes is a CS major. Let\u0026rsquo;s walk through how Prolog would respond to the following query:\ncs_major(X). To start, Prolog realizes that in order to satisfy our query, it has to at least satisfy the query\ntakes(X, Y). So, Prolog starts there. In order to satisfy that query, it searches its knowledge base (its list of facts/rules that define predicates) from top to bottom. X and Y are variables and the first appropriate fact that it finds is\ntakes(jane, cs4999). So, it unifies X with jane and Y with cs4999. Having satisfied that query, Prolog realizes that it must also satisfy the query:\ntakes(X, Z). However, Prolog has already provisionally unified X with jane. So, Prolog really attempts to satisfy the query:\ntakes(jane, Z). Great. For Prolog, attempting to satisfy this query is completely distinct from its attempt to satisfy the query takes(X, Y) which means that Prolog starts searching its knowledge base anew (again, from top to bottom!). The first appropriate fact that it finds that satisfies this query is\ntakes(jane, cs4999).\nSo, it unifies Z with cs4999. Having satisfied that query too, Prolog moves on to the third query:\nY @\u0026lt; Z. Unfortunately, because X and Y are both unified to cs4999, Prolog fails to satisfy that query. In other words, Prolog realizes that its provisional unification of X with jane, Y with cs4999 and Z with cs4999 is not a way to satisfy our original query (cs_major(X)).\nDoes Prolog just give up? Absolutely not! It\u0026rsquo;s persistent. It backtracks! To where?\nWell, according to the definition above, it backtracks to the most-recent choicepoint! In this case, the most recent choicepoint was its provisional unification of Z with cs4999. So, Prolog forgets about that attempt, and restarts the search of its knowledge base.\nWhere does it restart that search, though? This is important: It restarts its search where it left off. In other words, it starts at the first fact at takes(jane, cs4999). Because there are no other facts about classes that Jane takes, Prolog fails again, this time attempting to satisfy the query takes(jane, Z).\nI ask again, does Prolog just give up? No, it backtracks again! This time it backtracks to its most-recent choicepoint. Now, that most recently choicepoint was its provisional unification of X with jane. Prolog forgets that attempt, and restarts the search of its knowledge base! Again, because this is the continuation of a previous search, Prolog begins where it left off in its top-to-bottom search of its knowledge base. The next fact that it see is\ntakes(alicia, cs2020) So, Prolog provisionally unifies X with alicia and Y with cs2020. Having satisfied that query (for a second time!), Prolog realizes that it must also satisfy the query:\ntakes(X, Z). However, Prolog has provisionally unified X with alicia. So, Prolog really attempts to satisfy the query:\ntakes(alicia, Z). Great. For Prolog, attempting to satisfy this query is completely distinct from its attempt to satisfy the query takes(X, Y) and its previous attempt to satisfy the query takes(jane, Z). Therefore, Prolog starts searching its knowledge base anew (again, from top to bottom!). The first appropriate fact that it finds that satisfies this query is\ntakes(alicia, cs2020). So, it unifies Z with cs2020. Having satisfied that query too, Prolog moves on to the third query:\nY @\u0026lt; Z. Unfortunately, because Z and Y are both unified to cs2020, Prolog fails to satisfy that query. In other words, Prolog realizes that its provisional unification of X with alicia, Y with cs2020 and Z with cs2020 is not a way to satisfy our original query (cs_major(X)). Again, Prolog does not give up and it backtracks to its most recent choicepoint. The good news is that Prolog can satisfy the query\ntakes(alicia, Z) a second way by unifying Z with cs3050. Prolog proceeds to the final part of the rule\nX @\u0026lt; Y which can be satisfied this time because cs3050 and cs2020 are different!\nVictory!\nProlog was able to satisfy the original query when it unified X with alicia, Y with cs2020 and Z with cs3050.\nBelow is a visualization of the description given above:\n A Prolog user at the REPL (or a Prolog program using this rule) could ask for all the ways that this query is satisfied. And, if the user/program does, then Prolog will backtrack as if it did not find a satisfactory unification for Z, Y or X (in that order!).* 11/5/2021\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/11-8-2021/",
	"title": "11/8/2021",
	"tags": [],
	"description": "",
	"content": "In the true spirit of a picture being worth a thousand words , think of this Daily PL as a graphic novel.\nGoing Over Backtracking Again (see what I did there?) In today\u0026rsquo;s lecture, we went deeper into the discussion of backtracking and saw its utility. In particular, we discussed the following Prolog program for generating all the integers.\ngenerate_integer(0). generate_integer(X) :- generate_integer(Y), X is Y + 1. This is an incredibly succinct way to declare what it means to be an integer. This generator function is attributable to Programming in Prolog by Mellish and Clocksin (Links to an external site.). In other words, we know that it\u0026rsquo;s a reasonable definition.\nAs discussed in the description of Assignment 3, the best way to learn Prolog, I think, is to play with it! So, let\u0026rsquo;s see what this can do!\n?- generate_integer(5). true ; In other words, it can be used to determine whether a given number is an integer. Awesome.\n?- generate_integer(X). X = 0 ; X = 1 ; X = 2 ; X = 3 ; X = 4 ; X = 5 ; X = 6 Woah, look at that \u0026hellip; generate_integer/1 can do double duty and generate all the integers, too. Pretty cool!\nThe generation of the numbers using this definition is possible thanks to the power of backtracking. If you need a refresher on backtracking and/or choice points, I recommend previous Daily PLs.\nThe (im)possibility of using words to describe the backtracking involved in generate_integer/1, led me to create the following diagram that attempts to illustrate the process. I encourage you to look at the diagram, run generate_integer/1 in swipl with trace enabled and ask any questions that you have! Again, this is not a simple concept, but once you understand what is going on, Prolog will begin to make more sense!\n It may be necessary to use a high-resolution version of the diagram if you are curious about the details. Such a version is available in SVG format here.\nChasing Our Tails Yes, I can hear you! I know, generate_integer/1 is not tail recursive. We learned that tail recursion is a useful optimization in functional programming languages (and even imperative programming languages). Does that mean that it\u0026rsquo;s an important optimization in Prolog?\nTo find out, I timed how long it took Prolog to answer the query\n?- generate_integer(50000). The answer? On my desktop computer, it took 1 minute and 48 seconds.\nIf we want something for comparison, we\u0026rsquo;ll have to come up with a tail-recursive version of generate_integer/1. Let\u0026rsquo;s call it generate_integer_tr/1 (creative, I know), and define it like:\ngenerate_integer_tr(X) :- next_integer(0,X). next_integer(J, J). next_integer(J, L) :- K is J + 1, next_integer(K, L). The fact next_integer(J, J) is a \u0026ldquo;trick\u0026rdquo; to define a base case for our definition in the same way that generate_integer(0) was a base case in the definition of generate_integer/1. To get a sense for the need for next_integer(J, J) think about what happens when the first query of next_integer(0,X) is performed in order to satisfy the query generate_integer_tr(50000). In this case, the next_integer(J, J) fact matches (convince yourself why! Hint: there are no restrictions on J). As a result, J unifies with the 0, and the X unifies with the J. That\u0026rsquo;s great, but (X = ) 0 does not equal 50000. So, Prolog does what?\nIn unison: BACKTRACKS.\nThe choice point is Prolog\u0026rsquo;s selection of next_integer(J, J), so Prolog tries again at the next possible fact/rule: next_integer(J, L) :- K is J + 1, next_integer(K, L). J is unified with 0, K is unified with 1 (J + 1) and Prolog must now satisfy a new goal: next_integer(1, L). Because this query for next_integer/1 is completely different than the one it is currently attempting to satisfy, Prolog starts the search anew at the top of the list of facts. The first fact to match? next_integer(J, J). Therefore, J unifies with 1, L unifies with J (making L 1), and X (from the initial query) unifies with L. Unfortunately, the result again does not satisfy our query. But, Prolog persists and backtracks but only as far as the second attempt to satisfy next_integer (using next_integer(J,J)). In much the same way that generate_integer/1 worked, Prolog continues to progress, then backtrack, then progress, then backtrack \u0026hellip; while solving the generate_integer_tr(50000) query.\nThe difference between the two functions is that in next_integer/2, the recursive act is the last thing that is done. In other words, generate_integer_tr/1 is tail recursive.\nDoes this impact performance? Absolutely! On my desktop. Prolog can answer the query generate_integer_tr(50000). in 0.029 seconds. Yeow!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/12-1-2021/",
	"title": "12/1/2021",
	"tags": [],
	"description": "",
	"content": "Peanut Butter and Jelly To reiterate, the goal of a parser is to convert the source code for a program written in a particular programming language defined according to a specific grammar into a parse tree. Parsing occurs in two parts: lexical analysis and syntax analysis. The lexical analyzer (what we studied in the previous lecture) converts the program\u0026rsquo;s source code (in the form of bytes on disk) into a sequence of tokens. The syntax analyzer, the subject of this lecture, turns the sequence of tokens in to a parse tree. The lexical analyzer and the syntax analyzer go hand-in-hand: As the syntax analyzer goes about its business of creating a parse tree, it will periodically turn to the lexical analyzer and say, \u0026ldquo;Give me the next token!\u0026rdquo;.\nWe saw algorithms for building a lexical analyzer directly from the language\u0026rsquo;s CFG. It would be great if we had something similar for the syntax analyzer. In today\u0026rsquo;s lecture we are going to explore just one of the many techniques for converting a CFG into code that will build an actual parse tree. There are many such techniques, some more general and versatile than others. Sebesta talks about several of these. If you take a course in compiler construction you will see those general techniques defined in detail. In this class we only have time to cover one particular technique for translating a CFG into code that constructs parse trees and it only works for a subset of all grammars.\nWith those caveats in mind, let\u0026rsquo;s dig in to the details!\nDescent Into Madness A recursive-descent parser is a type of parser that can be written directly from the structure of a CFG \u0026ndash; as long as that CFG meets certain constraints. In a recursive descent parser built from a CFG G, there is a subprogram for every nonterminal in G. Most of these subprograms are recursive. A recursive-descent parser builds a parse tree from the top down, meaning that it begins with G\u0026rsquo;s start symbol and attempts to build a parse tree that represents a valid derivation whose final sentential form matches the program\u0026rsquo;s tokens. There are other parsers that work bottom up, meaning that they start by analyzing a sentential form made up entirely of the program\u0026rsquo;s tokens and attempt to build a parse tree that that \u0026ldquo;reduces\u0026rdquo; to the grammar\u0026rsquo;s start symbol. That the subprograms are recursive and that the parse tree is built top down, the recursive-descrent parser is aptly named.\nWe mentioned before that there are limitations on the types of languages that a recursive-descent parser can recognize. In particular, recursive-descent parsers can only recognize LL grammars. Those are grammars whose parse trees represent a leftmost derivation and can be built from a single left-to-right scan of the input tokens. To be precise, the first L represents the left-to-right scan of the input and the second L indicates that the parser generates a leftmost derivation. There is usually another restriction \u0026ndash; how many lookahead tokens are available. A lookahead token is the next token that the lexical analyzer will return as it scans through the input. Limiting the number of lookahead tokens reduces the memory requirements of the syntax analyzer but restricts the types of languages that those syntax analyzers can recognize. The number of lookahead tokens is written after LL and in parenthesis. LL(1) indicates 1 token of lookahead. We will see the practical impact of this restriction later in this edition.\nAll of these words probably seem very arbitrary, but I think that an example will make things clear!\nOld Faithful Let\u0026rsquo;s return to the grammar for mathematical expressions that we have been examining throughout this module:\n We will assume that there are appropriately named tokens for each of the terminals (e.g, the ) token is CLOSE_PAREN) and that any numbers are tokenized as ID with the lexeme set appropriately.\nAccording to the definition of a recursive-descent parser, we want to write a (possibly recursive) subprogram for each of the nonterminals in the grammar. The job of each of these subprograms is to parse the the upcoming tokens into a parse tree that matches the nonterminal. For example, the (possibly recursive) subprogram for Expr, Expr, parses the upcoming tokens into a parse tree for an expression and returns that parse tree. To facilitate recursive calls among these subprograms, each subprogram returns the root of the parse tree that it builds. The parser begins by invoking the subprogram for the grammar\u0026rsquo;s start symbol. The return value of that function call will be the root node of the parse tree for the entire input expression. Any recursive calls to other subprograms from within the subprogram for the start symbol will return parts of that overall parse tree.\nI am sure that you see why each of the subprograms usually contains some recursive function calls \u0026ndash; the nonterminals themselves are defined recursively.\nHow would we write such a (possibly recursive) subprogram to build a parse tree rooted at a Factor from a sequence of tokens?\nThere are two productions for a Factor so the first thing that the Factor subprogram does is determine whether it is parsing, for example, (5+2) \u0026ndash; a parenthesized expression \u0026ndash; or 918 \u0026ndash; a simple ID. In order to differentiate, the function simply consults the lookahead token. If that token is an open parenthesis then it knows that it is going to be evaluating the production Factor \\(\\rightarrow ( Expr )\\). On the other hand, if that token is an ID, then it knows that it is going to be evaluating the production Factor \\(\\rightarrow\\) id. Finally, if the current token is neither an open parenthesis nor an ID, then that\u0026rsquo;s an error!\nLet\u0026rsquo;s assume that the current token is an open parenthesis. Therefore, Factor knows that it should be parsing the production Factor \\(\\rightarrow ( Expr )\\). Remember how we said that in a recursive-descent parser, each nonterminal is represented by a (possibly recursive) subprogram? Well, that means that we can assume there is a subprogram for parsing an Expr (though we haven\u0026rsquo;t yet defined it!). Let\u0026rsquo;s call that mythical subprogram Expr. As a result, the Factor subprogram can invoke Expr which will return a parse tree rooted at that expression. Pretty cool! Before continuing to parse, Factor will check Expr\u0026rsquo;s return value \u0026ndash; if it is an error node, then parsing stops and Factor simply returns that error.\nOtherwise, after successfully parsing an expression (by invoking Expr) the parser expects the next token to be a close parenthesis. So, Factor checks that fact. If everything looks good, then Factor simply returns the node generated by Expr \u0026ndash; there\u0026rsquo;s no need to generate another node that just indicates an expression is wrapped in parenthesis. If the token after parsing the expression is not a close parenthesis, then Factor returns an error node.\nNow, what happens if the lookahead token is an ID? That\u0026rsquo;s simple \u0026ndash; Factor will just generate a node for that ID and return it!\nFinally, if neither of those is true, Factor simply returns an error node.\nLet\u0026rsquo;s make this a little more concrete by writing that in pseudocode. We will assume the following are defined:\n Node(T, X, Y, Z \u0026hellip;): A polymorphic function that generates an appropriately typed node (according to T) in the parse tree that \u0026ldquo;wraps\u0026rdquo; the tokens X, Y, Z, etc. We will play fast and loose with this notation. Error(X): A function that generates an error node because token X was unexpected \u0026ndash; an error node in the final parse tree will generate a diagnostic message. tokenizer(): A function that returns and consumes the next token from the lexical analyzer. lookahead(): A function that returns the lookahead token.   def Factor: if lookhead() == OPEN_PAREN: # Parsing a ( Expr ) # # Eat the lookahead and move forward. curTok = nextToken() # Build a parse tree rooted at an expression, # if possible. nestedExpr = Expr() # There was an error parsing that expression; # we will return an error! if type(nestedExpr) == Error: return nestedExpr # Expression parsing went well. We expect a ) # now. if lookahead() == CLOSE_PAREN: # Eat that close parenthesis. nextToken() # Return the root of the parse tree of the # nested expression. return nestedExpr else: # We expected a ) and did not get it. return Error(lookahead()) else if lookahead() == ID: # Parsing a ID # curTok = nextToken() return Node(curTok) else: # Parsing error! return Error(lookahead()) Writing a function to parse a Factor is relatively straightforward. To get a sense for what it would be like to parse an expression, let\u0026rsquo;s write a part of the Expr subprogram:\ndef Expr: ... leftHandExpr = Expr() if type(leftHandExpr) == Error: return leftHandExpr if lookahead() != PLUS: curTok = nextToken() return Error(curTok) rightHandTerm = Term() if type(rightHandTerm) == Error: return rightHandTerm return Node(Expr, leftHandExpr, rightHandTerm) ... What stands out is an emerging pattern that each of the subprograms will follow. Each subprogram is slowly matching the items from the grammar with the actual tokens that it sees. The subprogram associated with each nonterminal parses the nonterminals used in the production, \u0026ldquo;eats\u0026rdquo; the terminals in those same productions and converts it all into nodes in a parse tree. The subprogram that calls subprograms recursively melds together their return values into a new node that will become part of the overall parse tree, one level up.\nWe Are Homefree I don\u0026rsquo;t know about you, but I think that\u0026rsquo;s pretty cool \u0026ndash; you can build a series of collaborating subprograms named after the nonterminals in a grammar that call each other and, bang!, through the power of recursion, a parse tree is built! I think we\u0026rsquo;re done here.\nOr are we?\nLook carefully at the definition of Expr given in the pseudocode above. What is the name of the first subprogram that is invoked? That\u0026rsquo;s right, Expr. When we invoke Expr again, what is the first subprogram that is invoked? That\u0026rsquo;s right, Expr again. There\u0026rsquo;s no base case \u0026ndash; this spiral will continue until there is no more space available on the stack!\nIt seems like we may have run head-on into a fundamental limitation of recursive-descent parsing: The grammars that it parses cannot contain productions that are left recursive. A production A \\(\\rightarrow \\ldots\\) is (indirect) left recursive \u0026ldquo;when A derives itself as its leftmost symbol using one or more derivations.\u0026rdquo; In other words, \\(A \\rightarrow^{+} A \\ldots \\) is indirectly recursive where \\(\\rightarrow^{+} \\)indicates one or more productions. For example, the production for A in grammar\n is indirect left recursive because A → B → A \\mathtt{b}.\nA production A \\(\\rightarrow \\ldots \\)is direct left recursive when A derives itself as its leftmost symbol using one derivation (e.g., \\(A \\rightarrow A \\ldots\\). The production for Expr in our working example is direct left recursive.\nWhat are we to do?\nNote: The definitions of left recursion are taken from:\nAllen B Tucker and Robert Noonan. 2006. Programming Languages (2nd. ed.). McGraw-Hill, Inc., USA.\nFormalism To The Rescue Stand back, we are about to deploy math!\n There is an algorithm for converting productions that are direct-left recursive into productions that generate the same languages and are not direct-left recursive. In other words, there is hope for recursive-descent parsers yet! The procedure is slightly wordy, but after you see an example it will make perfect sense. Here\u0026rsquo;s the overall process:\nFor the rule that is direct-left recursive, A, rewrite all productions of A as \\(A \\rightarrow A\\alpha_1 | A\\alpha_2 | \\ldots | A\\alpha_n | \\beta_1 | \\ldots | \\beta_n \\) where all (non)terminals β_1 \\ldots β_n are not direct-left recursive. Rewrite the production as\n\\(A \\rightarrow \\beta_{1}A' | \\beta_{2}A' | \\ldots | \\beta_{n}A' \\\\ A' \\rightarrow \\alpha_{1}A' | \\alpha_{2}A' | \\ldots | \\alpha_{n}A' | \\varepsilon\\)\nwhere \\(\\varepsilon\\) is the erasure rule and matches an empty token.\nI know, that\u0026rsquo;s hard to parse (pun intended). An example will definitely make things easier to grok:\nIn\n\\(Expr \\rightarrow Expr + Term | Term\\)\nA is Expr, \\(\\alpha_1is + Term, \\beta_1 is Term\\). Therefore,\nA \\(\\rightarrow \\beta_{1}A' \\\\ A' \\rightarrow \\alpha_{1}A' | \\varepsilon \\)\nbecomes\n\\(Expr \\rightarrow Term Expr' \\\\ Expr' \\rightarrow +TermExpr' | \\varepsilon\\)\nNo sweat! It\u0026rsquo;s just moving pieces around on a chessboard!\nThe Final Countdown We can make those same manipulations for each of the productions in the grammar in our working example and we arrive here:\n\\(\\mathit{Expr} \\rightarrow \\mathit{Term}\\mathit{Expr'} \\\\ \\mathit{Expr'} \\rightarrow + \\mathit{Term}\\mathit{Expr'} | \\epsilon \\\\ \\mathit{Term} \\rightarrow \\mathit{Factor}\\mathit{Term'} \\\\ \\mathit{Term'} \\rightarrow * \\mathit{Factor}\\mathit{Term'} | \\epsilon \\\\ \\mathit{Factor} \\rightarrow ( \\mathit{Expr} ) | \\mathit{id}\\)\nNow that we have a non direct-left recursive grammar we can easily write a recursive-descent parser for the entire grammar. The source code is available online and I encourage you to download and play with it!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/8-23-2021/",
	"title": "8/23/2021",
	"tags": [],
	"description": "",
	"content": "Definitions:  programming language: A system of communicating computational ideas between people and computing machines. high-level programming language: A programming language that is independent of any particular machine architecture. syntax: The rules for constructing structurally valid (note: valid, not correct) computer programs in a particular language. names: A means of identifying \u0026ldquo;entities\u0026rdquo; in a programming language. types: A type denotes the kinds of values that a program can manipulate. (A more specific definition will come later in the course). semantics: The effect of each statement\u0026rsquo;s execution on the program\u0026rsquo;s operation.  Concepts: There are four fundamental components of every programming language: syntax, names, types, and semantics.\nPractice:   For your favorite programming language, attempt to explain the most meaningful parts of its syntax. Think about what are valid identifiers in the language, how statements are separated, whether blocks of code are put inside braces, etc.\n  Next, ask yourself how your favorite language handles types. Are variables in the language given types explicitly or implicitly? If the language is compiled, are types assigned to variables before the code is compiled or as the program executes? These are issues that we will discuss in detail in Module 2.\n  Finally, think about how statements in your favorite programming language affect the program\u0026rsquo;s execution. Does the language have loops? if-then statements? function composition? goto?\n  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/8-25-2021/",
	"title": "8/25/2021",
	"tags": [],
	"description": "",
	"content": "The value of studying PLs  Every new language that you learn gives you new ways to think about and solve problems.  There is a parallel here with natural languages. Certain written/spoken languages have words for concepts that others do not. Linguists have said that people can only conceive ideas for which there are words. In certain programming languages there may be constructs (\u0026ldquo;words\u0026rdquo;) that give you a power to solve problems and write algorithms in new, interesting ways.   You can choose the right tool for the job.  When all you have is a hammer, everything looks like a nail.   Makes you an increasingly flexible programmer.  The more you know about the concepts of programming languages (and the PLs that you know) the easier it is to learn new languages.   Using PLs better Studying PLs will teach you about how languages are implemented. This \u0026ldquo;awareness\u0026rdquo; can give you insight into the \u0026ldquo;right way\u0026rdquo; to do something in a particular language. For instance, if you know that recursion and looping are equally performant and computationally powerful, you can choose to use the one that improves the readability of your code. However, if you know that iteration is faster (and that\u0026rsquo;s important for your application) then you will choose that method for invoking statements repeatedly.  Programming domains  We write programs to solve real-world problems. The problems that we are attempting to solve lend themselves to programming languages with certain characteristics. Some of those real-world problems are related to helping others solve real-world problems (systems programs):  e.g., operating systems, utilities, compilers, interpreters, etc. There are a number of good languages for writing these applications: C, C++, Rust, Python, Go, etc.   But, most of programs are designed/written to solve actually real-world problems:  scientific calculations: these applications need to be fast (parallel?) and mathematically precise (work with numbers of many kinds). Scientific applications were the earliest programming domain and inspired the first high-level programming language, Fortran. artificial intelligence: AI applications manipulate symbols (in particular, lists of symbols) as opposed to numbers. This application requirement gave rise to a special type of language designed especially for manipulating lists, Lisp (List Processor). world wide web: WWW applications must embed code in data (HTML). Because of how WWW applications advance so quickly, it is important that languages for writing these applications support rapid iteration. Common languages for writing web applications are PERL, Python, JavaScript, Ruby, Go, etc. business: business applications need to produce reports, process character-based data, describe and store numbers with specific precision (aka, decimals). COBOL has traditionally been the language of business applications, although new business applications are being written in other languages these days (Java, the .Net languages). machine learning: machine learning applications require sophisticated math and algorithms and most developers do not want to rewrite these when good alternatives are available. For this reason, a language with a good ecosystem of existing libraries makes an ideal candidate for writing ML programs (Python). game development: So-called AAA games must be fast enough to generate lifelike graphics and immersive scenes in near-real time. For this reason, games are often written in a language that is expressive but generates code that is optimized, C++.    This list is non-exhaustive, obviously!\nThe John von Neumann Model of Computing  This computing model has had more influence on the development of PLs than we can imagine. There are two hardware components in this Model (the processor [CPU] and the memory) and they are connected by a pipe.  The CPU pipes data and instructions (see below) to/from the memory (fetch). The CPU reads that data to determine the action to take (decode). The CPU performs that operation (execute). Because there is only one path between the CPU and the memory, the speed of the pipe is a bottleneck on the processor\u0026rsquo;s efficiency.   The Model is interesting because of the way that it stores instructions and data together in the same memory. It is different than the Harvard Architecture where programs and data are stored in different memory. In the Model, every bit of data is accessible according to its address. Sequential instructions are placed nearby in memory.  For instance, in     for (int i = 0; i \u0026lt; 100; i++) { statement1; statement2; statement3; } statement1, statement2 and statement3 are all stored one after the other in memory.\n Modern implementations of the Model make fetching nearby data fast. Therefore, implementing repeated instructions with loops is faster than implementing repeated loops with recursion. Or is it? This is a particular case where learning about PL will help you as a programmer!  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/8-27-2021/",
	"title": "8/27/2021",
	"tags": [],
	"description": "",
	"content": "Programming Paradigms  A paradigm is a pattern or model. A programming paradigm is a pattern of problem-solving thought that underlies a particular genre of programs and languages.  According to their syntax, names and types, and semantics, it is possible to classify languages into one of four categories (imperative, object-oriented, functional and logic). That said, modern researchers in PL are no longer as convinced that these are meaningful categories because new languages are generally a collection of functionality and features and contain bits and pieces from each paradigm.   The paradigms:  Imperative: Imperative languages are based on the centrality of assignment statements to change program state, selection statements to control program flow, loops to repeat statements and procedures for process abstraction (a term we will learn later).  These languages are most closely associated with the von Neumann architecture, especially assignment statements that approximate the piping operation at the hardware level. Examples of imperative languages include C, Fortran, Cobol, Perl.   Object-oriented: Object-oriented languages are based upon a combination of data abstraction, data hiding, inheritance and message passing.  Objects respond to messages by modifying their internal data \u0026ndash; in other words, they become active. The power of inheritance is that an object can reuse an implementation without having to rewrite the code. These languages, too, are closely associated with the von Neumann architecture and (usually) inherit selection statements, assignment statements and loops from imperative programming languages. Examples of object-oriented languages include Smalltalk, Ruby, C++, Java, Python, JavaScript.   Functional: Functional programming languages are based on the concept that functions are first-class objects in the language \u0026ndash; in other words, functions are just another type like integers, strings, etc.  In a functional PL, functions can be passed to other functions as parameters and returned from functions. The loops and selection statements of imperative programming languages are replaced with composition, conditionals, and recursion in functional PLs. A subset of functional PLs are known as pure functional PLs because functions those languages have no side-effects (a side-effect occurs in a function when that function performs a modification that can be seen outside the function \u0026ndash; e.g., changing a value of a parameter, changing a global variable, etc). Examples of functional languages include Lisp, Scheme, Haskell, ML, JavaScript, Python.   Logic: Simply put, logic programming languages are based on describing what to compute and not how to compute it.  Prolog (and its variants) are really the only logic programming language in widespread use.      Language Evaluation Criteria (New Material Alert) There are four (common) criteria for evaluating a programming language:\n Readability: A metric for describing how easy/hard it is to comprehend the meaning of a computer program written in a particular language.   Overall simplicity: The number of basic concepts that a PL has.\n Feature multiplicity: Having more than one way to accomplish the same thing. Operator overloading: Operators perform different computation depending upon the context (i.e., the type of the operands) Simplicity can be taken too far. Consider machine language.    Orthogonality: How easy/hard it is for the constructs of a language to be combined to build higher-level control and data structures.\n Alternate definition: The mutual independence of primitive operations. Orthogonal example: any type of entity in a language can be passed as a parameter to a function. Non-orthogonal example: only certain entities in a language can be used as a return value from a function (e.g., in C/C++ you cannot return an array). This term comes from the mathematical concept of orthogonal vectors where orthogonal means independent. The more orthogonal a language, the fewer exceptional cases there are in the language\u0026rsquo;s semantics. The more orthogonal a language, the slower the language: The compiler/interpreter must be able to compute based on every single possible combination of language constructs. If those combinations are restricted, the compiler can make optimizations and assumptions that will speed up program execution.    Data types: Data types make it easier to understand the meaning of variables.\n e.g., the difference between int userHappy = 0; and bool userHappy = True;    Syntax design\n A PL\u0026rsquo;s reserved words should make things clear. For instance, it is easier to match the beginnings and endings of loops in a language that uses names rather than { }s. The PL\u0026rsquo;s syntax should evoke the operation that it is performing.  For instance, a + should perform some type of addition operation (mathematical, concatenation, etc)       Writeability  Includes all the aspects of Readability, and Expressiveness: An expressive language has relatively convenient rather than cumbersome way of specifying computations.   Reliability: How likely is it that a program written in a certain PL is correct and runs without errors.  Type checking: a language with type checking is more reliable than one without type checking; type checking is testing for operations that compute on variables with incorrect types at compile time or runtime.  Type checking is better done at runtime. A strongly typed programming language is one that is always able to detect type errors either at compile time or runtime.   Exception handling (the ability of a program to intercept runtime errors and take corrective action) and aliasing (when two or more distinct names in a program point to the same resource) affect the PL\u0026rsquo;s reliability.        In truth, there are so many things that affect the reliability of a PL.               The easier a PL is to read and write, the more reliable the code is going to be.   Cost: The cost of writing a program in a certain PL is a function of  The cost to train programmers to use that language The cost of writing the program in that language The time/speed of execution of the program once it is written The cost of poor reliability The cost of maintenance \u0026ndash; most of the time spent on a program is in maintaining it and not developing it!    "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/8-30-2021/",
	"title": "8/30/2021",
	"tags": [],
	"description": "",
	"content": "Today we learned a more complete definition of imperative programming languages and studied the defining characteristics of variables. Unfortunately we did not get as far as I wanted during the class which means that there is some new material in this edition of the Daily PL!\nImperative Programming Languages Any language that is an abstraction of the von Neumann Architecture can be considered an imperative programming language.\nThere are 5 calling cards of imperative programming languages:\n state, assignment statements, and expressions: Imperative programs have state. Assignment statements are used to modify the program state with computed values from expressions  state: The contents of the computer\u0026rsquo;s memory as a program executes. expression: The fundamental means of specifying a computation in a programming language. As a computation, they produce a value. assignment statement: A statement with the semantic effect of destroying a previous value contained in memory and replacing it with a new value. The primary purpose of the assignment statement is to have a side effect of changing values in memory. As Sebesta says, \u0026ldquo;The essence of the imperative programming languages is the dominant role of the assignment statement.\u0026rdquo;   variables: The abstraction of the memory cell. loops: Iterative form of repetition (for, while, do \u0026hellip; while, foreach, etc) selection statements: Conditional statements (if/then, switch, when) procedural abstraction: A way to specify a process without providing details of how the process is performed. The primary means of procedural abstraction is through definition of subprograms (functions, procedures, methods).  Variables There are 6 attributes of variables. Remember, though, that a variable is an abstraction of a memory cell.\n type: Collection of a variable\u0026rsquo;s valid data values and the collection of valid operations on those values. name: String of characters used to identify the variable in the program\u0026rsquo;s source code. scope: The range of statements in a program in which a variable is visible. Using the yet-to-be-defined concept of binding, there is an alternative definition: The range of statements where the name\u0026rsquo;s binding to the variable is active. lifetime: The period of time during program execution when a variable is associated with computer memory. address: The place in memory where a variable\u0026rsquo;s contents (value) are stored. This is sometimes called the variable\u0026rsquo;s l-value because only a variable associated with an address can be placed on the left side of an assignment operator. value: The contents of the variable. The value is sometimes call the variable\u0026rsquo;s r-value because a variable with a value can be used on the right side of an assignment operator.  Looking forward to Binding (New Material Alert) A binding is an association between an attribute and an entity in a programming language. For example, you can bind an operation to a symbol: the + symbol can be bound to the addition operation.\nBinding can happen at various times:\n Language design (when the language\u0026rsquo;s syntax and semantics are defined or standardized) Language implementation (when the language\u0026rsquo;s compiler or interpreter is implemented) Compilation Loading (when a program [either compiled or interpreted] is loaded into memory) Execution  A static binding occurs before runtime and does not change throughout program execution. A dynamic binding occurs at runtime and/or changes during program execution.\nNotice that the six \u0026ldquo;things\u0026rdquo; we talked about that characterize variables are actually attributes!! In other words, those attributes have to be bound to variables at some point. When these bindings occur is important for users of a programming language to understand. We will discuss this on Wednesday! blob:https://1492301-4.kaf.kaltura.com/903896d9-2341-4dd3-9709-ca344de08719\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-1-2021/",
	"title": "9/1/2021",
	"tags": [],
	"description": "",
	"content": "Welcome to the Daily PL for September 1st, 2021! As we turn the page from August to September, we started the month discussing variable lifetime and scope. Lifetime is related to the storage binding and scope is related to the name binding. Before we learned that new material, however, we went over an example of the different bindings and their times in an assignment statement.\nBinding Example Consider a Python statement like this:\nvrb = arb + 5 Recall that a binding is an association between an attribute and an entity. What are some of the possible bindings (and their times) in the statement above?\n The symbol + (entity) must be bound to an operation (attribute). In a language like Python, that binding can only be done at runtime. In order to determine whether the operation is a mathematical addition, a string concatenation or some other behavior, the interpreter needs to know the type of arb which is only possible at runtime. The numerical literal 5 (entity) must be bound to some in-memory representation (attribute). For Python, it appears that the interpreter chooses the format for representing numbers in memory (https://docs.python.org/3/library/sys.html#sys.int%5Finfo (Links to an external site.), https://docs.python.org/3/library/sys.html#sys.float%5Finfo (Links to an external site.)) which means that this binding is done at the time of language implementation. The value (attribute) of the variables vrb and arb (entities) are bound at runtime. Remember that the value of a variable is just another binding.  This is not an exhaustive list of the bindings that are active for this statement. In particular, the variables vrb and arb must be bound to some address, lifetime and scope. Discussing those bindings requires more information about the statement\u0026rsquo;s place in the source code.\nVariables' Storage Bindings The storage binding is related to the variable\u0026rsquo;s lifetime (the time during which a variable is bound to memory). There are four common lifetimes:\n  static: Variable is bound to storage before execution and remains bound to the same storage throughout program execution.\n Variables with static storage binding cannot share memory with other variables (they need their storage throughout execution). Variables with static storage binding can be accessed directly (in other words, their access does not require redirection through a pointer) because the address of their storage is constant throughout execution. Direct addressing means that accesses are faster. Storage for variables with static binding does not need to be repeatedly allocated and deallocated throughout execution \u0026ndash; this will make program execution faster. In C++, variables with static storage binding are declared using the static keyword inside functions and classes. Variables with static storage binding are sometimes referred to as history sensitive because they retain their value throughout execution.    stack dynamic: Variable is bound to storage when it\u0026rsquo;s declaration statements are elaborated (the time when a declaration statement is executed).\n Variables with stack dynamic storage bindings make recursion possible because their storage is allocated anew every time that their declaration is elaborated. To fully understand this point it is necessary to understand the way that function invocation is handled using a runtime stack. We will cover this topic next week. Stay tuned! Variables with stack dynamic storage bindings cannot be directly accessed. Accesses must be made through an intermediary which makes them slower. Again, this will make more sense when we discuss the typical mechanism for function invocation. The storage for variables with stack dynamic storage bindings are constantly allocated and deallocated which adds to runtime overhead.  Variables with stack dynamic storage bindings are not history sensitive.\n  Explicit heap dynamic: Variable is bound to storage by explicit instruction from the programmer. E.g., new / malloc in C/C++.\n The binding to storage is done at runtime when these explicit instructions are executed. The storage sizes can be customized for the use. The storage is hard to manage and requires careful attention from the programmer. The storage for variables with explicit heap dynamic storage bindings are constantly allocated and deallocated which adds to runtime overhead.    Implicit heap dynamic: Variable is bound to storage when it is assigned a value at runtime.\n All storage bindings for variables in Python are handled in this way. https://docs.python.org/3/c-api/memory.html (Links to an external site.) When a variable with implicit heap dynamic storage bindings is assigned a value, storage for that variable is dynamically allocated. Allocation and deallocation of storage for variables with implicit heap dynamic storage bindings is handled automatically by the language compiler/interpreter. (More on this when we discuss memory management techniques in Module 3).    Variables' Name Bindings See the Pl for the Video.\nThis new material is presented above as Episode 1 of PL After Dark. Below you will find a written recap!\nScope is the range of statements in which a variable is visible (either referencable or assignable). Using the vocabulary of bindings, scope can also be defined as the collection of statements which can access a name binding. In other words, scope determines the binding of a name to a variable.\nIt is easy to get fooled into thinking that a variable\u0026rsquo;s name is intrinsic to the variable. However, a variable\u0026rsquo;s name is just another binding like address, storage, value, etc. There are two scopes that most languages employ:\n local: A variable is locally scoped to a unit or block of a program if it is declared there. In Python, a variable that is the subject of an assignment is local to the immediate enclosing function definition. For instance, in   def add(a, b): total = a + b return total total is a local variable.\n global: A variable is globally scoped when it is not in any local scope (terribly unhelpful, isn\u0026rsquo;t it?) Using global variables breaks the principles of encapsulation and data hiding.  For a variable that is used that is not local, the compiler/interpreter must determine to which variable the name refers. Determining the name/variable binding can be done statically or dynamically:\nStatic Scoping This is sometimes also known as lexical scoping. Static scoping is the type of scope that can be determined using only the program\u0026rsquo;s source code. In a statically scoped programming language, determining the name/variable binding is done iteratively by searching through a block\u0026rsquo;s nesting static parents. A block is a section of code with its own scope (in Python that is a function or a class and in C/C++ that is statements enclosed in a pair of {}s). The static parent of a block is the block in which the current block was declared. The list of static parents of a block are the block\u0026rsquo;s static ancestors.\nDynamic Scoping Dynamic scoping is the type of scope that can be determined only during program execution. In a dynamically scoped programming language, determining the name/value binding is done iteratively by searching through a block\u0026rsquo;s nesting dynamic parents. The dynamic parent of a block is the block from which the current block was executed. Very few programming languages use dynamic scoping (BASH, PERL [optionally]) because it makes checking the types of variables difficult for the programmer (and impossible for the compiler/interpreter) and because it increases the \u0026ldquo;distance\u0026rdquo; between name/variable binding and use during program execution. However, dynamic binding makes it possible for functions to require fewer parameters because dynamically scoped non local variables can be used in their place.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-10-2021/",
	"title": "9/10/2021",
	"tags": [],
	"description": "",
	"content": "In today\u0026rsquo;s edition of the Daily PL we will recap our discussion from today that covered expressions, order of evaluation, short-circuit evaluation and referential transparency.\nExpressions An expression is the means of specifying computations in a programming language. Informally, it is anything that yields a value. For example,\n 5 is an expression (value 5) 5 + 2 is an expression (value 7) Assuming fun is a function that returns a value, fun() is an expression (value is the return value) Assuming f is a variable, f is an expression (the value is the value of the variable)  Certain languages allow more exotic statements to be expressions. For example, in C/C++, the = operator yields a value (the value of the expression on the right operand). It is this choice by the language designer that allows a C/C++ programmer to write\nint a, b, c, d; a = b = c = d = 5; to initialize all four variables to 5.\nWhen we discuss functional programming languages, we will see how many more things are expressions that programmers typically think are simply statements.\nOrder of Evaluation Programmers learn the associativity and precedence of operations in their languages. That knowledge enables them to mentally calculate the value of statements like 5 + 4 * 3 / 2.\nWhat programmers often forget to learn about their language, is the order of evaluation of operands. Take several of those constants from the previous expression and replace them with variables and function calls:\n5 + a() * c / b() The questions abound:\n Is a() executed before the value of variable c is retrieved? Is b() executed before c()? Is b() executed at all?  In a language with functional side effects, the answer to these questions matter. Why? Consider that a could have a side effect that changes c. If the value of c is retrieved before the execution of a() then the expression will evaluate to a certain value and if the value of c is retrieved after execution of a() then the expression will evaluate to a different value.\nCertain languages define the order of evaluation of operands (Python, Java) and others do not (C/C++). There are reasons why defining the order is a good thing:\n The programmer can depend on that order and benefit from the consistency The program\u0026rsquo;s readability is improved. The program\u0026rsquo;s reliability is improved.  But there is at least one really good reason for not defining that order: optimizations. If the compiler/interpreter can move around the order of evaluation of those operands, it may be able to find a way to generate faster code!\nShort-circuit Evaluation Languages with short-circuit evaluation take these potential optimizations one step further. For a boolean expression, the compiler will stop evaluating the expression as soon as the result is fixed. For instance, in a() \u0026amp;\u0026amp; b(), if a() is false, then the entire statement will always be false, no matter the value of b(). In this case, the compiler/interpreter will simply not execute b(). On the other hand, in a() || b() if a() is true, then the entire statement will always be true, no matter the value of b(). In this case, the compiler/interpreter will simply not execute b().\nA programmer\u0026rsquo;s reliance on this type of behavior in a programming language is very common. For instance, this is a common idiom in C/C++:\nint *variable = nullptr; ... if (variable != nullptr \u0026amp;\u0026amp; *variable \u0026gt; 5) { ... } In this code, the programmer is checking to see whether there is memory allocated to variable before they attempt to read that memory. This is defensive programming thanks to short-circuit evaluation.\nReferential Transparency Most of these issues would not be a problem if programmer\u0026rsquo;s wrote functions that did not have side effects (remember that those are called pure functions). There are languages that will not allow side effects and those languages support referential transparency: A function has referential transparency if its value (its output) depends only on the value of its parameter(s). In other words, if given the same inputs, a referentially transparent function always gives the same output.\nPut It All Together Try you hand at the practice quiz Expressions, precedence, associativity and coercions to check your understanding of the material we covered in class on Friday and the material from your assigned reading! For the why, check out relational.cpp .\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-13-2021/",
	"title": "9/13/2021",
	"tags": [],
	"description": "",
	"content": "In today\u0026rsquo;s edition of the Daily PL we will recap our discussion from today that covered subprograms, polymorphism and coroutines!\nSubprograms A subprogram is a type of abstraction. It is process abstraction where the how of a process is hidden from the user who concerns themselves only with the what. A subprogram provides process abstraction by naming a collection of statements that define parameterized computations.​ Again, the collection of statements determines how the process is implemented. Subprogram parameters give the user the ability to control the way that the process executes. There are three types of subprograms:\n Procedure: A subprogram that does not return a value. Function: A subprogram that does return a value. Method: A subprogram that operates with an implicit association to an object; a method may or may not return a value.  Pay close attention to the book\u0026rsquo;s explanation and definitions of terms like parameter, parameter profile, argument, protocol, definition, and declaration.\nSubprograms are characterized by three facts:\n A subprogram has only one entry point Only one subprogram is active at any time Program execution returns to the caller upon completion  Polymorphism Polymorphism allows subprograms to take different types of parameters on different invocations. There are two types of polymorphism:\n ad-hoc polymorphism: A type of polymorphism where the semantics of the function may change depending on the parameter types. parametric polymorphism: A type of polymorphism where subprograms take an implicit/explicit type parameter used to define the types of their subprogram\u0026rsquo;s parameters; no matter the value of the type parameter, in parametric polymorphism the subprogram\u0026rsquo;s semantics are always the same.  Ad-hoc polymorphism is sometimes call function overloading (C++). Subprograms that participate in ad-hoc polymorphism share the same name but must have different protocols. If the subprograms' protocols and names were the same, how would the compiler/interpreter choose which one to invoke? Although a subprogram\u0026rsquo;s protocol includes its return type, not all languages allow ad-hoc polymorphism to depend on the return type (e.g., C++). See the various definitions of add in the C++ code here: subprograms.cpp . Note how they all have different protocols. Further, note that not all the versions of the function add perform an actual addition! That\u0026rsquo;s the point of ad-hoc polymorphism \u0026ndash; the programmer can change the meaning of a function.\nFunctions that are parametrically polymorphic are sometimes called function templates (C++) or generics (Java, soon to be in Go, Rust). A parametrically polymorphic function is like the blueprint for a house with a variable number of floors. A home buyer may want a home with three stories \u0026ndash; the architect takes their variably floored house blueprint and \u0026ldquo;stamps out\u0026rdquo; a version with three floors. Some \u0026ldquo;new\u0026rdquo; languages call this process monomorphization (Links to an external site.). See the definition of minimum in the C++ code here: subprograms.cpp . Note how there is only one definition of the function. The associated type parameter is T. The compiler will \u0026ldquo;stamp out\u0026rdquo; copies of minimum for different types when it is invoked. For example, if the programmer writes\nauto m = minimum(5, 4); then the compiler will generate\nint minimum(int a, int b) { return a \u0026lt; b ? a : b; } behind the scenes.\nCoroutines Just when you thought that you were getting the hang of subprograms, a new kid steps on the block: coroutines. Sebesta defines coroutines as a subprogram that cooperates with a caller. The first time that a programmer uses a coroutine, they call it at which point program execution is transferred to the statements of the coroutine. The coroutine executes until it yields control. The coroutine may yield control back to its caller or to another coroutine. When the coroutine yields control, it does not cease to exist \u0026ndash; it simply goes dormant. When the coroutine is again invoked \u0026ndash; resumed \u0026ndash; the coroutine begins executing where it previously yielded. In other words, coroutines have\n multiple entry points full control over execution until they yield the property that only one is active at a time (although many may be dormant)  Coroutines could be used to write a card game. Each player is a coroutine that knows about the player to their left (that is, a coroutine). The PlayerA coroutine performs their actions (perhaps drawing a card from the deck, etc) and checks to see if they won. If they did not win, then the PlayerA coroutine yields to the PlayerB coroutine who performs the same set of actions. This process continues until a player no longer has someone to their left. At that point, everything unwinds back to the point where PlayerA was last resumed \u0026ndash; the signal that a round is complete. The process continues by resuming PlayerA to start another round of the game. Because each player is a coroutine, it never ceased to exist and, therefore, retains information about previous draws from the deck. When a player finally wins, the process completes. To see this in code, check out cardgame.py .\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-20-2021/",
	"title": "9/20/2021",
	"tags": [],
	"description": "",
	"content": "This is an issue of the Daily PL that you are going to want to make sure that you keep safe \u0026ndash; definitely worth framing and passing on to your children! You will want to make sure you remember where you were when you first learned about \u0026hellip;\nFormal Program Semantics Although we have not yet learned about it (we will, don\u0026rsquo;t worry!), there is a robust set of theory around the way that PL designers describe the syntax of their language. You can use regular expressions, context-free grammars, parsers (recursive-descent, etc) and other techniques for defining what is a valid program.\nOn the other hand, there is less of a consensus about how a program language designer formally describes the semantics of programs written in their language. The codification of the semantics of a program written in a particular is known as formal program semantics. In other words, formal program semantics are a precise mathematical description of the semantics of an executing program.​ Sebesta uses the term dynamic semantics which is defines as the \u0026ldquo;meaning[] of the expressions, statements and program units of a programming language.\u0026rdquo;\nThe goal of defining formal program semantics is to understand and reason about the behavior of programs. There are many, many reasons why PL designers want a formal semantics of their language. However, there are two really important reasons: With formal semantics it is possible to prove that\n two programs calculate the same result (in other words, that two programs are equivalent), and a program calculates the correct result.  The alternative to formal program semantics are standards promulgated by committees that use natural language to define the meaning of program elements. Here is an example of a page from the standard for the C programming language:\n If you are interested, you can find the C++ language standard , the Java language standard , the C language standard , the Go language standard and the Python language standard all online.\nTesting vs Proving There is absolutely a benefit to testing software. No doubt about it. However, testing that a piece of software behaves a certain way does not prove that it operates a certain way.\n\u0026ldquo;Program testing can be used to show the presence of bugs, but never to show their absence!\u0026quot; - Edsger Dijkstra\nThere is an entire field of computer science known as formal methods whose goal is to understand how to write software that is provably correct. There are systems available for writing programs about which things can be proven. There is PVS, Coq ,Isabelle , and TLA+ , to name a few. PVS is used by NASA to write its mission-critical software and even it makes an appearance in the movie The Martian .\nThree Types of Formal Semantics There are three common types of formal semantics. It is important that you know the names of these systems, but we will only focus on one in this course!\n Operational Semantics: The meaning of a program is defined by how the program executes on an idealized virtual machine. Denotational Semantics: Program units \u0026ldquo;denote\u0026rdquo; mathematical functions and those functions transform the mathematically defined state of the program. Axiomatic Semantics: The meaning of the program is based on proof rules for each programming unit with an emphasis on proving the correctness of a program.  We will focus on operational semantics only!\nOperational Semantics Program State We have referred to the state of the program throughout this course. We have talked about how statements in imperative languages can have side effects that affect the value of the state and we have talked about how the assignment statement\u0026rsquo;s raison d\u0026rsquo;etre is to change a program\u0026rsquo;s state. For operational semantics, we have to very precisely define a program\u0026rsquo;s state.\nAt all times, a program has a state. A state is just a function whose domain is the set of defined program variables and whose range is V * T where V is the set of all valid variable values (e.g., 5, 6.0, True, \u0026ldquo;Hello\u0026rdquo;, etc) and T is the set of all valid variable types (e.g., Integer, Floating Point, Boolean, String, etc). ​In other words, you can ask a state about a particular variable and, if it is defined, the function will return the variable\u0026rsquo;s current value and its type.\nIt is important to note that PL researchers have math envy. They are not mathematicians but they like to use Greek symbols. So, here we go:\n\\begin{equation*} \\sigma(x) = (v, \\tau) \\end{equation*}\nThe state function is denoted with the σ . τ always represents some arbitrary variable type. Generally, v represents a value. So, you can read the definition above as \u0026ldquo;Variable x has value v and type τ in state σ.\u0026rdquo;\nProgram Configuration Between execution steps (a term that we will define shortly), a program is always in a particular configuration:\n\\begin{equation*} \u0026lt;e, \\sigma\u0026gt; \\end{equation*}\nThis means that the program in state σ\nis about to evaluate expression e.\nProgram Steps A program step is an atomic (indivisible) change from one program configuration to another. Operational semantics defines steps using rules. The general form of a rule is\n\\begin{equation*} \\frac{premises}{conclusion} \\end{equation*}\nThe conclusion is generally written like \u0026lt;e, σ\u0026gt; ⟶ (v, τ, σ). This statement means that, when the premises hold, the rule evaluates to a value (v), type (τ) and (possibly modified) state (σ') after a single step of execution of a program in configuration \u0026lt;e, σ\u0026gt;. Note that rules do not yield configurations. All this will make sense when we see an example.\nExample 1: Defining the semantics of variable access. In STIMPL, the expression to access a variable, say i, is written like Variable(\u0026ldquo;i\u0026rdquo;). Our operational semantic rule for evaluating such an access should \u0026ldquo;read\u0026rdquo; something like: When the program is about to execute an expression to access variable i in a state σ, the value of the expression will be the triple of i\u0026rsquo;s value, i\u0026rsquo;s type and the unchanged state σ.\u0026rdquo; In other words, the evaluation of the next step of a program that is about to access a value is the value and type of the variable being accessed and the program\u0026rsquo;s state is unchanged.\nLet\u0026rsquo;s write that formally!\n\\begin{equation*} \\(\\frac{\\sigma(x) \\rightarrow (v, \\tau)} {\u0026lt;\\text{Variable}(x), \\sigma \u0026gt; \\rightarrow (v, \\tau, \\sigma)}\\) \\end{equation*}\nState Update How do we write down that the state is being changed? Why would we want to change the state? Let\u0026rsquo;s answer the second question first: we want to change the state when, for example, there is an assignment statement. If σ (\u0026ldquo;i\u0026rdquo;) = (4, Integer) and then the program evaluated an expression like Assign(Variable(\u0026ldquo;i\u0026rdquo;), IntLiteral(2)), we don\u0026rsquo;t want the σ\nfunction to return (4, Integer) any more! We want it to return (2, Integer). We can define that mathematically like:\n\\begin{equation*} \\sigma[(v,\\tau)/x](y)= \\begin{cases} \u0026amp; \\sigma(y) \\quad y \\ne x \\ \u0026amp;(v,\\tau) \\quad y=x \\end{cases} \\end{equation*}\nThis means that if you are querying the updated state for the variable that was just reassigned (x), then return its new value and type (m and τ ). Otherwise, just return the value that you would get from accessing the existing σ\n.\nExample 2: Defining the semantics of variable assignment (for a variable that already exists). In STIMPL, the expression to overwrite the value of an existing variable, say i, with, say, an integer literal 5 is written like Assign(Variable(\u0026quot;i\u0026quot;), IntLiteral(5)). Our operational semantic rule for evaluating such an assignment should \u0026ldquo;read\u0026rdquo; something like: When the program is about to execute an expression to assign variable i to the integer literal 5 in a state σ and the type of the variable i in state σ is Integer, the value of the expression will be the triple of 5, Integer and the changed state σ' which is exactly the same as state σ\nexcept where (5, Integer) replaced i\u0026rsquo;s earlier contents.\u0026quot; That\u0026rsquo;s such a mouthful! But, I think we got it. Let\u0026rsquo;s replace some of those literals with symbols for abstraction purposes and then write it down!\n\\begin{equation*} \\frac{\u0026lt;e, \\sigma\u0026gt; \\longrightarrow (v, \\tau, \\sigma'), \\sigma(x) \\longrightarrow (*,, \\tau)} {\u0026lt;\\text{Assign(Variable)}(x, e), \\sigma \u0026gt; \\longrightarrow (v, \\tau, \\sigma' [(v, \\tau)/x])} \\end{equation*}\nLet\u0026rsquo;s look at it step-by-step:\n\\begin{equation*} \u0026lt;Assign(Variable(x),e),\\sigma\u0026gt; \\end{equation*}\nis the configuration and means that we are about to execute an expression that will assign value of expression e to variable x. But what is the value of expression e? The premise\n\\begin{equation} \u0026lt;e,\\sigma\u0026gt;⟶(v,\\tau, \\sigma′) \\end{equation}\ntells us that the value and type of e when evaluated in state σ is v, and τ. Moreover, the premise tells us that the state may have changed during evaluation of expression e and that subsequent evaluation should use a new state, σ\n\u0026lsquo;. Our mouthful above had another important caveat: the type of the value to be assigned to variable x must match the type of the value already stored in variable x. The second premise\n\\begin{equation*} \\sigma′(x)\\longrightarrow(*, \\tau) \\end{equation*}\ntells us that the types match \u0026ndash; see how the τs are the same in the two premises? (We use the * to indicate that we don\u0026rsquo;t care what that value is!)\nNow we can just put together everything we have and say that the expression assigning the value of expression e to variable x evaluates to\n\\begin{equation*} (v,\\tau,\\sigma′[(v,\\tau)/x]) \\end{equation*}\nThat\u0026rsquo;s Great, But Show Me Code! Well, Will, that\u0026rsquo;s fine and good and all that stuff. But, how do I use this when I am implementing STIMPL? I\u0026rsquo;ll show you! Remember the operational semantics for variable access:\n\\begin{equation*} \\(\\frac{\\sigma(x) \\rightarrow (v, \\tau)} {\u0026lt;\\text{Variable}(x), \\sigma \u0026gt; \\rightarrow (v, \\tau, \\sigma)}\\) \\end{equation*}\nCompare that with the code for it\u0026rsquo;s implementation in the STIMPL skeleton that you are provided for Assignment 1:\ndef evaluate(expression, state): ... case Variable(variable_name=variable_name): value = state.get_value(variable_name) if value == None: raise InterpSyntaxError(f\u0026#34;Cannot read from {variable_name}before assignment.\u0026#34;) return (*value, state) At this point in the code we are in a function named evaluate whose first parameter is the next expression to evaluate and whose second parameter is a state. Does that sound familiar? That\u0026rsquo;s because it\u0026rsquo;s the same as a configuration! We use pattern matching to select the code to execute. The pattern is based on the structure of expression and we match in the code above when expression is a variable access. Refer to Pattern Matching in Python for the exact form of the syntax. The state variable is an instance of the State object that provides a method called get_value (see Assignment 1: Implementing STIMPL for more information about that function) that returns a tuple of (v, τ) In other words, get_value works the same as σ. So,\nvalue = state.get_value(variable_name) is a means of implementing the premise of the operational semantics.\nreturn (*value, state) yields the final result! Pretty cool, right?\nLet\u0026rsquo;s do the same analysis for assignment:\n\\(\\frac{\u0026lt;e,\\sigma\u0026gt;\\longrightarrow(v,\\tau,\\sigma′),\\sigma′(x)\\longrightarrow(*,\\tau)}{\u0026lt;Assign(Variable(x),e),σ\u0026gt;\\longrightarrow(v,\\tau,σ′[(v,\\tau)/x])}\\)\nAnd here\u0026rsquo;s the implementation:\ndef evaluate(expression, state): ... case Assign(variable=variable, value=value): value_result, value_type, new_state = evaluate(value, state) variable_from_state = new_state.get_value(variable.variable_name) _, variable_type = variable_from_state if variable_from_state else (None, None) if value_type != variable_type and variable_type != None: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Assignment: Cannot assign {value_type}to {variable_type}\u0026#34;\u0026#34;\u0026#34;) new_state = new_state.set_value(variable.variable_name, value_result, value_type) return (value_result, value_type, new_state) First, look at\nvalue_result, value_type, new_state = evaluate(value, state) which is how we are able to find the values needed to satisfy the left-hand premise. value_result is v, value_type is τ and new_state is σ\u0026rsquo;.\nvariable_from_state = new_state.get_value(variable.variable_name) is how we are able to find the values needed to satisfy the right-hand premise. Notice that we are using new_state (σ') to get variable.variable_name (x). There is some trickiness in_, variable_type = variable_from_state if variable_from_state else (None, None) to set things up in case we are doing the first assignment to the variable (which sets its type), so ignore that for now! Remember that in our premises we guaranteed that the type of the variable in state σ' matches the type of the expression:\nif value_type != variable_type and variable_type != None: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Assignment: Cannot assign {value_type}to {variable_type}\u0026#34;\u0026#34;\u0026#34;) performs that check!\nnew_state = new_state.set_value(variable.variable_name, value_result, value_type) generates a new, new state (σ′[(v,τ)/x]) and\nreturn (value_result, value_type, new_state) yields the final result!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-22-2021/",
	"title": "9/22/2021",
	"tags": [],
	"description": "",
	"content": "Like other popular newspapers that do in-depth analysis of popular topics (Links to an external site.), this edition of the Daily PL is part 2/2 of an investigative report on \u0026hellip;\nFormal Program Semantics In our previous class, we discussed the operational semantics of variable access and variable assignment. In this class we explored the operational semantics of the addition operator and the if/then statement.\nA Quick Review of Concepts At all times, a program has a state. A state is just a function whose domain is the set of defined program variables and whose range is V * T where V is the set of all valid variable values (e.g., 5, 6.0, True, \u0026ldquo;Hello\u0026rdquo;, etc) and T is the set of all valid variable types (e.g., Integer, Floating Point, Boolean, String, etc). ​In other words, you can ask a state about a particular variable and, if it is defined, the function will return the variable\u0026rsquo;s current value and its type.\nHere is the formal definition of the state function:\n\\begin{equation*} \\(\\sigma(x) = (v, \\tau)​\\) \\end{equation*}\nThe state function is denoted with the σ . τ always represents some arbitrary variable type. Generally, v represents a value. So, you can read the definition above as \u0026ldquo;Variable x has value v and type τ in state σ.\u0026rdquo;\nBetween execution steps, a program is always in a particular configuration:\n\\begin{equation*} \u0026lt;e, \\sigma\u0026gt; \\end{equation*}\nThis notation means that the program in state σ is about to evaluate expression e.\nA program step is an atomic (indivisible) change from one program configuration to another. Operational semantics defines steps using rules. The general form of a rule is\n\\begin{equation*} \\frac{premises}{conclusion} \\end{equation*}\nThe conclusion is generally written like \u0026lt;e, σ\u0026gt; ⟶ (v, τ, σ) which means that when the premises hold, the expression e evaluated in state σ evaluates to a value (v), type (τ) and (possibly modified) state (σ') after a single step of execution.\nDefining the Semantics of the Addition Expression In STIMPL, the expression to \u0026ldquo;add\u0026rdquo; two values n1 and n2 is written like Add(n1, n2). By the rules of the STIMPL language, for an addition to be possible, n1 and n2 must\n have the same type and have Integer, Floating Point or String type.  Because every unit in STIMPL has a value, we will define the operational semantics using two arbitrary expressions, e1 and e2. The program configuration to which we are giving semantics is\n\\begin{equation*} \u0026lt;Add(e_1),e_2),\\sigma\u0026gt; \\end{equation*}\nBecause our addition operator applies when its operands are three different types, we technically need three different rules for its evaluation. Let\u0026rsquo;s start with the operational semantics for add when its operands are of type Integer:\n\\begin{equation*} \\frac{\u0026lt;e_1,\\sigma\u0026gt;⟶(v_1,Integer,\\sigma),\u0026lt;e_2,\\sigma\u0026gt;⟶(v_2,Integer,\\sigma \\prime)}{\u0026lt;Add(e1,e2),σ\u0026gt;⟶(v1+v2,Integer,\\sigma\\prime)} \\end{equation*}\nLet\u0026rsquo;s look at the premises. First, there is\n\\begin{equation*} \u0026lt;e_1,\\sigma\u0026gt;⟶(v1,Integer,\\sigma \\prime) \\end{equation*}\nwhich means that, when evaluated in state σ, expression e1 has the value v1 and type Integer and may modify the state (to σ'). Notice that we are not using τ for the resulting type of the evaluation? Why? Because using τ indicates that this rule applies when the evaluation of e1 in state σ evaluates to any type (which we \u0026ldquo;assign\u0026rdquo; to τ in case we want to use it again in a later premise). Instead, we are explicitly writing Integer which indicates that this rule only defines the operational semantics for Add(e1, e2) in state σ when the expression e1 evaluates to a value of type Integer in state σ\n.\nAs for the second premise\n\\begin{equation*} \u0026lt;e_2,\\sigma \\prime\u0026gt;⟶(v_2,Integer,\\sigma\\prime \\prime) \\end{equation*}\nwe see something very similar. Again, our premise prescribes that, when evaluated in state σ' (note the ' there), e2\u0026rsquo;s type is an Integer. It is for this reason that we can be satisfied that this rule only applies when the types of the Add\u0026rsquo;s operands match and are integers! We \u0026ldquo;thread through\u0026rdquo; the (possibly) modified σ' when evaluating e2 to enforce the STIMPL language\u0026rsquo;s definition that operands are evaluated strictly left-to-right.\nAs for the conclusion,\n\\begin{equation*} (v_1+v_2,Integer,\\sigma \\prime \\prime) \\end{equation*}\nshows the value of this expression. We will assume here that + works as expected for two integers. Because the operands are integers, we can definitively write that the type of the addition will be an integer, too. We use σ'' as the resulting state because it\u0026rsquo;s possible that evaluation of the expressions of both e1 and e2 caused side effects.\nThe rule that we defined covers only the operational semantics for addition of two integers. The other cases (for floating-point and string types) are almost copy/paste.\nNow, how does that translate to an actual implementation?\ndef evaluate(expression, state): match expression: ... case Add(left=left, right=right): result = 0 left_result, left_type, new_state = evaluate(left, state) right_result, right_type, new_state = evaluate(right, new_state) if left_type != right_type: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Add: Cannot add {left_type}to {right_type}\u0026#34;\u0026#34;\u0026#34;) match left_type: case Integer() | String() | FloatingPoint(): result = left_result + right_result case _: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Cannot add {left_type}s\u0026#34;\u0026#34;\u0026#34;) return (result, left_type, new_state) In this snippet, the local variables left and right are the equivalent of e1 and e2, respectively, in the operational semantics. After initializing a variable to store the result, the evaluation of the premises is accomplished. new_state matches σ'' after being assigned and reassigned in those two evaluations. Next, the code checks to make sure that the types of the operands matches. Finally, if the types of the operands is an integer, then the result is just a traditional addition (+ in Python).\nYou can see the implementation for the other types mixed in this code as well. Convince yourself that the code above handles all the different cases where an Add is valid in STIMPL.\nDefining the Semantics of the If/Then/Else Expression In STIMPL, we write an If/Then/Else expression like If(c, t, f) where c is any boolean-typed expression, t is the expression to evaluate if the value of c is true and f is the expression to evaluate if the value of c is false. The value/type/updated state of the entire expression is the value/type/updated state that results from evaluating t when c is true and the value/type/updated state that results from evaluating f when c is false. This means that we are required to write two different rules to completely define the operational semantics of the If/Then/Else expression: one for the case where c is true and the other for the case when c is false. Sounds like the template that we used for the Add expression, doesn\u0026rsquo;t it? Because the two cases are almost the same, we will only go through writing the rule for when the condition is true:\n\\begin{equation*} \\frac{\u0026lt;c,\\sigma\u0026gt;\\longrightarrow(True,Boolean,\\sigma \\prime),\u0026lt;t,\\sigma \\prime\u0026gt;\\longrightarrow(v,\\tau,\\sigma\\prime \\prime)}{\u0026lt;If(c,t,f),σ\u0026gt;⟶(v,\\tau, \\sigma \\prime \\prime)} \\end{equation*}\nAs in the premises for the operational semantics of the Add operator, the first premise in the operational semantics above uses literals to guarantee that the rule only applies in certain cases:\n\\begin{equation*} \u0026lt;c,\\sigma \\prime\u0026gt;\\longrightarrow(True,Boolean,\\sigma\\prime \\prime) \\end{equation*}\nmeans that the rule only applies when c, evaluated in state σ, has a value of True and a boolean type. We use the second premise\n\\begin{equation*} \u0026lt;t,\\sigma\\prime\u0026gt;⟶(v,\\tau,\\sigma \\prime \\prime) \\end{equation*}\nto \u0026ldquo;get\u0026rdquo; some values that we will use in the conclusion. v and τ are the value and the type, respectively, of t when it is evaluated in state σ'. Note that we evaluate t in state σ' because the evaluation of the condition statement may have modified state σ and we want to thread that through. Evaluation of t in state σ' may modify σ', generating σ''. The combination of these premises are combined to define that the entire expression evaluates to\n\\begin{equation*} (v,\\tau,\\sigma\\prime \\prime) \\end{equation*}\nAgain, the pattern is the same for writing the operational semantics when the condition is false.\nLet\u0026rsquo;s look at how this translates into actual working code:\ndef evaluate(expression, state): match expression: ... case If(condition=condition, true=true, false=false): condition_value, condition_type, new_state = evaluate(condition, state) if not isinstance(condition_type, Boolean): raise InterpTypeError(\u0026#34;Cannot branch on non-boolean value!\u0026#34;) result_value = None result_type = None if condition_value: result_value, result_type, new_state = evaluate(true, new_state) else: result_value, result_type, new_state = evaluate(false, new_state) return (result_value, result_type, new_state) The local variables condition, true and false match c, t and f, respectively from the rule in the operational semantics. The first step in the implementation is to determine the value/type/updated state when c is evaluated in state σ. Immediately after doing that, the code checks to make sure that the condition statement has boolean type. Remember how our rule only applies when this is the case? Next, depending on whether the condition evaluated to true or false, the appropriate next expression is evaluated in the σ' state (new_state). It is the result of that evaluation that is the ultimate value of the expression and what is returned.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-24-2021/",
	"title": "9/24/2021",
	"tags": [],
	"description": "",
	"content": "As we conclude the penultimate week of September, we are turning the page from imperative programming and beginning our work on object-oriented programming!\nThe Definitions of Object-Oriented Programming We started off by attempting to describe object-oriented programming using two different definitions:\n A language with support for abstraction of abstract data types (ADTs).​ (from Sebesta) A language with support for objects, containers of data (attributes, properties, fields, etc.) and code (methods).​ (from Wikipedia (Links to an external site.))  As graduates of CS1021C and CS1080C, the second definition is probably not surprising. The first definition, however, leaves something to be desired. Using Definition (1) means that we have to a) know the definition of abstraction and abstract data types and b) know what it means to apply abstraction to ADTs.\nAbstraction (Reprise) There are two fundamental types of abstractions in programming: process and data. We have talked about the former but the latter is new. When we talked previously about process abstractions, we did attempt to define the term abstraction but it was not satisfying.\nSebesta formally defines abstraction as the view or representation of an entity that includes only the most significant attributes. This definition seems to align with our notion of abstraction especially the way we use the term in phrases like \u0026ldquo;abstract away the details.\u0026rdquo; It didn\u0026rsquo;t feel like a good definition to me until I thought of it this way:\nConsider that you and I are both humans. As humans, we are both carbon based and have to breath to survive. But, we may not have the same color hair. I can say that I have red hair and you have blue hair to point out the significant attributes that distinguish us. I need not say that we are both carbon based and have to breath to survive because we are both human and we have abstracted those facts into our common humanity.\nWe returned to this point at the end of class when we described how inheritance is the mechanism of object-oriented programming that provides abstraction over ADTs. Abstract Data Types (ADTs)\nNext, we talked about the second form of abstraction available to programmers: data abstraction. As functions, procedures and methods are the syntactic and semantic means of abstracting processes in programming languages, ADTs are the syntactic and semantic means of abstracting data in programming languages. ADTs combine (encapsulate) data (usually called the ADT\u0026rsquo;s attributes, properties, etc) and operations that operate on that data (usually called the ADT\u0026rsquo;s methods) into a single entity.\nWe discussed that hiding is a significant advantage of ADTs. ADTs hide the data being represented and allow that data\u0026rsquo;s manipulation only through pre-defined methods, the ADT\u0026rsquo;s interface. The interface typically gives the ADT\u0026rsquo;s user the ability to manipulate/access the data internal to the type and perform other semantically meaningful operations (e.g., sorting a list).\nWe brainstormed some common ADTs:\n Stack Queue List Array Dictionary Graph Tree  These are are so-called user-defined ADTs because they are defined by the user of a programming language and composed of primitive data types.\nNext, we tackled the question of whether primitives are a type of ADT. A primitive type like floating point numbers would seem to meet the definition of an abstract data type:\n It\u0026rsquo;s underlying representation is hidden from the user (the programmer does not care whether FPs are represented according to IEEE754 or some other specification) There are operations that manipulate the data (addition, subtraction, multiplation, division).  The Requirements of an Object-Oriented Programming Language ADTs are just one of the three requirements that your textbook\u0026rsquo;s author believes are required for a language to be considered object oriented. Sebesta believes that, in addition to ADTs, an object-oriented programming language requires support for inheritance and dynamic method binding.\nInheritance It is inheritance where OOPs provide abstraction for ADTs. Inheritance allows programmers to abstract ADTs into common classes that share common characteristics. Consider three ADTs that we identified: trees, linked lists and graphs. These three ADTs all have nodes (of some kind or another) which means that we could abstract them into a common class: node-based things. A graph would inherit from the node-based things ADT so that its implementer could concentrate on what makes it distinct \u0026ndash; its edges, etc.\nDon\u0026rsquo;t worry if that is too theoretical. It does not negate the fact that, through inheritance, we are able to implement hierarchies that can be \u0026ldquo;read\u0026rdquo; using \u0026ldquo;is a\u0026rdquo; the way that inheritance is usually defined. With inheritance, cats inherit from mammals and \u0026ldquo;a cat is a mammal\u0026rdquo;.\nSubclasses inherit from ancestor classes. In Java, ancestor classes are called superclasses and subclasses are called, well, subclasses. In C++, ancestor classes are called base classes and subclasses are called derived classes. Subclasses inherit both data and methods.\nDynamic Method Binding In an OOP, a variable that is typed as Class A can be assigned anything that is actually a Class A or subclass thereof. We have not officially covered this yet, but in OOP a subclass can redefine a method defined in its ancestor.\nAssume that every mammal can make a noise. That means that every dog can make a noise just like every cat can make a noise. Those noises do not need to be the same, though. So, a cat \u0026ldquo;overrides\u0026rdquo; the mammal\u0026rsquo;s default noise and implements their own (meow). A dog does likewise (bark). A programmer can define a variable that holds a mammal and that variable can contain either a dog or a cat. When the programmer invokes the method that causes the mammal to make noise, then the appropriate method must be called depending on the actual type in the variable at the time. If the mammal held a dog, it would bark. If the mammal held a cat, it would meow.\nThis resolution of methods at runtime is known as dynamic method binding.\nOOP Example with Inheritance and Dynamic Method Binding abstract class Mammal { protected int legs = 0; Mammal() { legs = 0; } abstract void makeNoise(); } class Dog extends Mammal { Dog() { super(); legs = 4; } void makeNoise() { System.out.println(\u0026#34;bark\u0026#34;); } } class Cat extends Mammal { Cat() { super(); legs = 4; } void makeNoise() { System.out.println(\u0026#34;meow\u0026#34;); } } public class MammalDemo { static void makeARuckus(Mammal m) { m.makeNoise(); } public static void main(String args[]) { Dog fido = new Dog(); Cat checkers = new Cat(); makeARuckus(fido); makeARuckus(checkers); } } This code creates a hierarchy with Mammal at the top as the superclass of both the Dog and the Cat. In other words, Dog and Cat inherit from Mammal. The abstract keyword before class Mammal indicates that Mammal is a class that cannot be directly instantiated. We will come back to that later. The Mammal class declares that there is a method that each of its subclasses must implement \u0026ndash; the makeNoise function. If a subclass of Mammal fails to implement that function, it will not compile. The good news is that Cat and Dog do both implement that function and define behavior in accordance with their personality!\nThe function makeARuckus has a parameter whose type is a Mammal. As we said above, in OOP that means that I can assign to that variable a Mammal or anything that inherits from Mammal. When we call makeARuckus with an argument whose type is Dog, the function relies of dynamic method binding to make sure that the proper makeNoise function is called \u0026ndash; the one that barks \u0026ndash; even though makeARuckus does not know whether m is a generic Mammal, a Dog or a Cat. It is because of dynamic method binding that the code above generates\nbark meow as output.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-27-2021/",
	"title": "9/27/2021",
	"tags": [],
	"description": "",
	"content": "It\u0026rsquo;s the last week of September but the first full week of OOP. Let\u0026rsquo;s do this!\nOverriding in OOP Recall the concept of inheritance that we discussed in the last class. Besides its utility as a formalism that describes the way a language supports abstraction of ADTs (and, therefore, makes it a plausibly OO language), inheritance provides a practical benefit in software engineering. Namely, it allows developers to build hierarchies of types.\nHierarchies are composed of pairs of classes \u0026ndash; one is the superclass and the other is the subclass. A superclass could conceivably be itself a subclass. A subclass could itself be a superclass. In terms of a family tree, we could say that the subclass is a descendant of the superclass (Note: remember that the terms superclass and subclass are not always the ones used by the languages themselves; C++ refers to them as base and derived classes, respectively).\nA subclass inherits both the data and methods from its superclass(es). However, as Sebesta says, \u0026ldquo;\u0026hellip; the features and capabilities of the [superclass] are not quite right for the new use.\u0026rdquo; Overriding methods allows the programmer to keep most of the functionality of the baseclass and customize the parts that are \u0026ldquo;not quite right.\u0026rdquo;\nAn overridden method is defined in a subclass and replaces the method with the same name (and usually protocol) in the parent.\nThe official documentation and tutorials for Java describe overriding in the language this way:\u0026ldquo;An instance method in a subclass with the same signature (name, plus the number and the type of its parameters) and return type as an instance method in the superclass overrides the superclass\u0026rsquo;s method.\u0026quot; The exact rules for overriding methods in Java are online at the language specification .\nLet\u0026rsquo;s make it concrete with an example:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } In this example, Car is the superclass of Tesla and Chevrolet. The Car class defines a method named ignite. That method will ignite the engine of the car \u0026ndash; an action whose mechanics differ based on the car\u0026rsquo;s type. In other words, this is a perfect candidate for overriding. Both Tesla and Chevrolet implement a method with the same name, return value and parameters, thereby meeting Java\u0026rsquo;s requirements for overriding. In Java, the @Override is known as an annotation. Annotations are \u0026ldquo;a form of metadata [that] provide data about a program that is not part of the program itself.\u0026quot; Annotations in Java are attached to particular syntactic units. In this case, the @Override annotation is attached to a method and it tells the compiler that the method is overriding a method from its superclass. If the compiler does not find a method in the superclass(es) that is capable of being overridden by the method, an error is generated. This is a good check for the programmer. (Note: C++ offers similar functionality through the override specifier (Links to an external site.).)\nLet\u0026rsquo;s say that the programmer actually implemented the Tesla class like this:\nclass Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite(int testing) { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } The ignite method implemented in Tesla does not override the ignite method from Car because it has a different set of parameters. The @Override annotation tells the compiler that the programmer thought they were overriding something. An error is generated and the programmer can make the appropriate fix. Without the @Override annotation, the code will compile but produce incorrect output when executed.\nAssume that the following program exists:\npublic class CarDemo { public static void main(String args[]) { Car c = new Car(); Car t = new Tesla(); Car v = new Chevrolet(); c.ignite(); t.ignite(); v.ignite(); } } This code instantiates three different cars \u0026ndash; the first is a generic Car, the second is a Tesla and the third is a Chevrolet. Look carefully and note that the type of each of the three is actually stored in a variable whose type is Car and not a more-specific type (ie, Tesla or Chevy). This is not a problem because of dynamic dispatch. At runtime, the JVM will find the proper ignite function and invoke it according to the variable\u0026rsquo;s actual type and not its static type. Because ignite is overridden by Chevy and Tesla, the output of the program above is:\nIgniting a generic car\u0026#39;s engine! Igniting a Tesla\u0026#39;s engine! Igniting a Chevrolet\u0026#39;s engine! Most OOP languages provide the programmer the option to invoke the method they are overriding from the superclass. Java is no different. If an overriding method implementation wants to invoke the functionality of the method that it is overriding, it can do so using the super keyword.\nclass Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } With these changes, the program now outputs:\nIgniting a generic car\u0026#39;s engine! Igniting a generic car\u0026#39;s engine! Igniting a Tesla\u0026#39;s engine! Igniting a generic car\u0026#39;s engine! Igniting a Chevrolet\u0026#39;s engine! New material alert: What if the programmer does not want a subclass to be able to customize the behavior of a certain method? For example, no matter how you subclass Dog, it\u0026rsquo;s noise method is always going to bark \u0026ndash; no inheriting class should change that. Java provides the final keyword to guarantee that the implementation of a method cannot be overridden by a subclass. Let\u0026rsquo;s change the code for the classes from above to look like this:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } void start() { System.out.println(\u0026#34;Starting a car ...\u0026#34;); if (this.ignite()) { System.out.println(\u0026#34;Ignited the engine!\u0026#34;); } else { System.out.println(\u0026#34;Did NOT ignite the engine!\u0026#34;); } } final boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } Notice that ignite in the Car class has a final before the return type. This makes ignite a final method : \u0026ldquo;A method can be declared final to prevent subclasses from overriding or hiding it\u0026rdquo;. (C++ has something similar \u0026ndash; the final specifier .) Attempting to compile the code above produces this output:\nCarDemo.java:30: error: ignite() in Tesla cannot override ignite() in Car boolean ignite() { ^ overridden method is final CarDemo.java:43: error: ignite() in Chevrolet cannot override ignite() in Car boolean ignite() { ^ overridden method is final 2 errors Subclass vs Subtype In OOP there is fascinating distinction between subclasses and subtypes. All those classes that inherit from other classes are considered subclasses. However, they are not all subtypes. For a type/class S to be a subtype of type/class T, the following must hold\nAssume that ϕ(t) is some provable property that is true of t, an object of type T. Then ϕ(s)\nmust be true as well for s, an object of type S.\nThis formal definition can be phrased simply in terms of behaviors: If it is possible to pass objects of type T as arguments to a function that expects objects of type S without any change in the behavior, then S is a subtype of T. In other words, a subtype behaves exactly like the \u0026ldquo;supertype\u0026rdquo;.\nBarbara Liskov who pioneered the definition and study of subtypes put it this way (Links to an external site.): \u0026ldquo;If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2, then S is a subtype of T.\u0026rdquo;\nOpen Recursion Open recursion in an OO PL is a fancy term for the combination of a) functionality that gives the programmer the ability to refer to the current object from within a method (usually through a variable named this or self) and b) dynamic dispatch. . Thanks to open recursion, some method A of class C can call some method B of the same class. But wait, there\u0026rsquo;s more! (Links to an external site.) Continuing our example, in open recursion, if method B is overriden in class D (a subclass of C), then the overriden version of the method is invoked when called from method A on an object of type D even though method A is only implemented by class C. Wild! It is far easier to see this work in real life than talk about it abstractly. So, consider our cars again:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } void start() { System.out.println(\u0026#34;Starting a car ...\u0026#34;); if (this.ignite()) { System.out.println(\u0026#34;Ignited the engine!\u0026#34;); } else { System.out.println(\u0026#34;Did NOT ignite the engine!\u0026#34;); } } boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } The start method is only implemented in the Car class. At the time that it is compiled, the Car class has no awareness of any subclasses (ie, Tesla and Chevrolet). Let\u0026rsquo;s run this code and see what happens:\npublic class CarDemo { public static void main(String args[]) { Car c = new Car(); Car t = new Tesla(); Car v = new Chevrolet(); c.start(); t.start(); v.start(); } } Here\u0026rsquo;s the output:\nStarting a car ... Igniting a generic car\u0026#39;s engine! Ignited the engine! Starting a car ... Igniting a Tesla\u0026#39;s engine! Ignited the engine! Starting a car ... Igniting a Chevrolet\u0026#39;s engine! Did NOT ignite the engine! Wow! Even though the implementation of start is entirely within the Car class and the Car class knows nothing about the Tesla or Chevrolet subclasses, when the start method is invoked on object\u0026rsquo;s of those types, the call to this\u0026rsquo;s ignite method triggers the execution of code specific to the type of car!\nHow cool is that?\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-3-2021/",
	"title": "9/3/2021",
	"tags": [],
	"description": "",
	"content": "Welcome to The Daily PL for 9/3/2021. We spent most of Friday reviewing material from Episode 1 of PL After Dark and going over scoping examples in C++ and Python. Before continuing, make sure that you have viewed Episode 1 of PL After Dark.\nScope We briefly discussed the difference between local and global scope.\nIt is easy to get fooled into thinking that a variable\u0026rsquo;s name is intrinsic to the variable. However, a variable\u0026rsquo;s name is just another binding like address, storage, value, etc.\nAs a programmer, when a variable is local determining the name/variable binding is straightforward. Determining the name/variable binding becomes more complicated (and more important) when source code uses a non-local name to reference a variable. In cases like this, determining the name/variable binding depends on whether the language is statically or dynamically scoped.\nStatic Scoping This is sometimes also known as lexical scoping. Static scoping is the type of scope that can be determined using only the program\u0026rsquo;s source code. In a statically scoped programming language, determining the name/variable binding is done iteratively by searching through a block\u0026rsquo;s nesting static parents. A block is a section of code with its own scope (in Python that is a function or a class and in C/C++ that is statements enclosed in a pair of {}s). The static parent of a block is the block in which the current block was declared. The list of static parents of a block are the block\u0026rsquo;s static ancestors.\nHere is pseudocode for the algorithm of determining the name/variable binding in a statically scoped programming language:\ndef resolve(name, current_scope) -\u0026gt; variable s = current_scope while (s != InvalidScope) if s.contains(name) return s.variable(name) s = s.static_parent_scope() return NameError For practice doing name/variable binding in a statically scoped language, play around with an example in Python: static_scope.py\nConsider this \u0026hellip; Python and C++ have different ways of creating scopes. In Python and C++ a new scope is created at the beginning of a function definition (and that scope contains the function\u0026rsquo;s parameters automatically). However, Python and C++ differ in the way that scopes are declared (or not!) for variables used in loops. Consider the following Python and C++ code (also available at loop_scope.cpp and loop_scope.py :\ndef f(): for i in range(1, 10): print(f\u0026#34;i (in loop body): {i}\u0026#34;) print(f\u0026#34;i (outside loop body): {i}\u0026#34;) void f() { for (int i = 0; i\u0026lt;10; i++) { std::cout \u0026lt;\u0026lt; \u0026#34;i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // The following statement will cause a compilation error  // because i is local to the code in the body of the for  // loop.  // std::cout \u0026lt;\u0026lt; \u0026#34;i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } In the C++ code, the for loop introduces a new scope and i is in that scope. In the Python code, the for loop does not introduce a new scope and i is in the scope of f. Try to run the following Python code also available here at loop_scope_error.py to see why this distinction is important:\ndef f(): print(f\u0026#34;i (outside loop body): {i}\u0026#34;) for i in range(1, 10): print(f\u0026#34;i (in loop body): {i}\u0026#34;) Dynamic Scoping Dynamic scoping is the type of scope that can be determined only during program execution. In a dynamically scoped programming language, determining the name/value binding is done iteratively by searching through a block\u0026rsquo;s nesting dynamic parents. The dynamic parent of a block is the block from which the current block was executed. Very few programming languages use dynamic scoping (BASH, Perl [optionally] are two examples) because it makes checking the types of variables difficult for the programmer (and impossible for the compiler/interpreter) and because it increases the \u0026ldquo;distance\u0026rdquo; between name/variable binding and use during program execution. However, dynamic binding makes it possible for functions to require fewer parameters because dynamically scoped non local variables can be used in their place.\ndef resolve(name, current_scope) -\u0026gt; variable s = current_scope while (s != InvalidScope) if s.contains(name) return s.variable(name) s = s.dynamic_parent_scope() return NameError For practice doing name/variable binding in a dynamically scoped language, play around with an example in Python: dynamic_scope.py . Note that because Python is intrinsically a statically scoped language, the example includes some hacking of the Python interpreter to emulate dynamic scoping. Compare the dynamic in the aforementioned Python code with the resolve function in the pseudocode and see if there are differences!\nReferencing Environment (New Material Alert) The referencing environment of a statement contains all the name/variable bindings visible at that statement. NOTE: The example in the book on page 224 is absolutely horrendous \u0026ndash; disregard it entirely. Consider the example online here: referencing_environment.py . Play around with that code and make sure that you understand why certain variables are in the referencing environment and others are not.\nIn case you think that this is theoretical and not useful to you as a real, practicing programmer, take a look at the official documentation of the Python execution model and see how the language relies on the concept of referencing environments: naming-and-binding .\nScope and Lifetime Are Not the Same (New Material Alert) It is common for programmers to think that the scope and the lifetime of a variable are the same. However, this is not always true. Consider the following code in C++ (also available at scope_new_lifetime.cpp)\n#include \u0026lt;iostream\u0026gt; void f(void) { static int variable = 4; } int main() { f(); return 0; } In this program, the scope of variable is limited to the function f. However, the lifetime of variable is the entire program. Just something to keep in mind when you are programming!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-8-2021/",
	"title": "9/8/2021",
	"tags": [],
	"description": "",
	"content": "Welcome to The Daily PL for September 8, 2021. I\u0026rsquo;m not lying when I say that this is the best. edition. ever. There is new material included in this edition which will be covered in a forthcoming episode of PL After Dark. When that video is available, this post will be updated!\nRecap The Type Characteristics of a Language\nIn today\u0026rsquo;s lecture we talked about types (again!). In particular, we talked about the two independent axis of types for a programming language: whether a PL is statically or dynamically typed and whether it is strongly or weakly typed. In other words, the time of the binding of type/variable in a language is independent of that language\u0026rsquo;s ability to detect type errors.\n A statically typed language is one where the type/variable binding is done before the code is run and does not change throughout program execution. A dynamically typed language is one where the type/variable binding is done at runtime and/or may change throughout program execution.    A strongly typed language is one where type errors are always detected (either at before or during program execution) A weakly typed language is one that is, well, not strongly typed.   In order to have a completely a satisfying definition of strongly typed language, we defined type error as any error that occurs when an operation is attempted on a type for which it is not well defined. In Python, \u0026quot;3\u0026quot; + 5 results in a TypeError: can only concatenate str (not \u0026quot;int\u0026quot;) to str. In this example, the operation is + and the types are str and int.\nCertain strongly typed languages appear to be weakly typed because of coercions. A coercion occurs when the language implicitly converts a variable of one type to another. C++ allows the programmer to define operations that will convert the type of a variable from, say, type a to type b. If the compiler sees an expression using a variable of type b where only a variable of type a is valid, then it will invoke that conversion operation automatically. While this adds to the language\u0026rsquo;s flexibility, the conversion behavior may hide the fact that a type error exists and, ultimately, make code more difficult to debug. Note that coercions are done implicitly \u0026ndash; a change between types done at the explicit request of the programmer is know as a (type)cast.\nFinally, before digging in to actual types, we defined type system: A type system is the set of types supported by a language and the rules for their usage.\nAggregate Data Types Aggregate data types are data types composed of one or more basic, or primitive, data types. Do not ask me to write a specific definition for primitive data type \u0026ndash; it will only get us into a circular mess :-)\nArray An array is a homogeneous (i.e., all its elements must be of the same type) aggregate data type in which an individual element is accessed by its position (i.e., index) in the aggregate. There are myriad design decisions associated with a language\u0026rsquo;s implementation of arrays (the type of the index, whether their size must be fixed or whether it can be dynamic, etc.) One of those design decisions is the way that a language lays out a two dimensional array in memory. There are two options: row-major order and column-major order. For a second, forget the concept of rows and columns altogether and consider that you access two dimensional arrays by letters and numbers. See the following diagram:\nThe memory of actual computers is linear. Therefore, two dimensional arrays must be flattened. In \u0026ldquo;letter major\u0026rdquo; order, the slices of the array identified by letters are stored in memory one after the other. In \u0026ldquo;number major\u0026rdquo; order, the slices of the array identified by numbers are stored in memory one after another. Notice that, in \u0026ldquo;letter major\u0026rdquo; order, the numbers \u0026ldquo;change fastest\u0026rdquo; and that, in \u0026ldquo;number major\u0026rdquo; order, the letters \u0026ldquo;change fastest\u0026rdquo;.\nSubstitute \u0026ldquo;row\u0026rdquo; for \u0026ldquo;letter\u0026rdquo; and \u0026ldquo;column\u0026rdquo; for \u0026ldquo;number\u0026rdquo; and, voila, you understand!! The C programming language stores arrays in row-major order; Fortran stores arrays in column-major order.\nKeep in mind that this description is only one way (or many) to store two dimensional arrays. There are (Links to an external site.) others (Links to an external site.).\nAssociative Arrays, Records, Tuples, Lists, Unions, Algebraic Data Types, Pattern Matching, List Comprehensions, and Equivalence All that, and more, in Episode 2 of PL After Dark!\nNote: In this video, I said that Python\u0026rsquo;s Lists function as arrays and that Python does not have true arrays. Your book implies as much in the section on Lists. However, I went back to check, and it does appear that there is a standard module in Python that provides arrays, in certain cases. Take a look at the documentation here: python arrays . The commonly used NumPy package also provides an array type: numpy arrays . While the language, per se, does not define an array type, the presence of the modules (particularly the former) is important to note. Sorry for the confusion!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/ecology/ch1/",
	"title": "Chapter 1",
	"tags": [],
	"description": "",
	"content": "Terms Ecology The study of the interactions between an organism and its biological and physical environment.\nTrait A combination of characteristics in an individual. A trait that increases evolutionary fitness is an adaptation.\n Hair color resistance to antibiotics.  Population All of the organisms of a particular species in a particular area.\n The population of snow hares in the Himalayas.  Fitness The degree to which an organism is likely to survive in an environment. The fitter the organism, the more likely they are to survive in an environment suited to their adaptations. This is more useful in the context of the population.\nLevels of Ecological organization  organism population community ecosystem  Proximate Vs Ultimate Proximate Factors Direct or Immediate causes/effects on the organism. The How.\n Changes in photoperiod Temperature changes  Ultimate Factors The Indirect reason for an effect on the organism. The Why.\nScientific Method and Hypothesis testing Good hypothesis lead to testable predictions. It is also falsifiable.\n Observation Hypothesis Measurements Results Conclusion  Predictions A prediction is an observation or a result that is expected if the hypothesis is true.\nHypotheses and tests  Focus on a population Why does the population exist? What unique challenges does it face? Are circumstances changing? Will the population grow, shrink, disturbed  Resources  reproduction Survival Mate Food Shelter  *\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/ecology/ch-2/",
	"title": "Chapter 2",
	"tags": [],
	"description": "",
	"content": "review  mutation allele gene frequencies genotype phenotype  Important terms for the course  Evolution is the change of populations (species) over time due to heritable changes. Natural Selection is the \u0026ldquo;selection\u0026rdquo; of heritable traits due to their fitness Adaptation is the tendency for natural selection to select changes most well suited to the environment. Fitness is the likelihood that an organism will survive (pass its genes down) in its current environment.  Mechanisms for evolutionary change  Natural Selection Genetic drift Gene Flow Mutation pressure  Natural section, genetic drift, gene flow genetic drift The random change of allele frequency based on non-random mating. Only affects populations with limited populations (non-random mating)\nGene flow Individuals can move from population to population, or form a new population. These fluctuations can change the allele frequencies in a population.\nIsolation Differentiation is affected by Isolation. Isolation prevents gene flow.\nHardy Weinberg  \\(P^2 + 2PQ + Q^2\\)  Adaptation is an undirected process Traits are not created to adapt to the environment. Traits instead are randomly generated (through mutation) and the traits that are well suited to the environment are selected for.\nPhenotypic Plasticity Development of physiological variation among phenotypes induced directly by the environment.\n Occurs during the development of an individual. Himalayan Rabbits are a good example, they have areas of black fur if born in a cold environment.  Adaptive landscape An adaptive landscape is a three dimensional plot, where the x and y axis refer to allele frequency and the z axis refers to fitness. \u0026ldquo;Peaks\u0026rdquo; and \u0026ldquo;Valleys\u0026rdquo; show how different allele frequencies affect fitness and why genetic drift can be helpful for adaptation.\nRed Queen Hypothesis Increased fitness in the short term does not always end well for the species. This discrepancy of fitness between two related organisms (parasites/hosts, predators/prey) is related to generational rate. One of the species in the relationship cannot keep up evolutionarily with its pair species. This may result in extinction of prey or predator or both.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/ecology/ch-3/",
	"title": "Chapter 3",
	"tags": [],
	"description": "",
	"content": "Adaptation to environmental factors  Change factors that limit growth (amount of food, composition of air) Change Tolerance Range (Temperature tolerance, ph tolerance ) Change Behavior to adapt beyond Range ( avoid cold areas )  Limiting environmental factors  physical factors (strength, ability to fly) Physical resources Tolerance bounds  optimality and Principle of allocation Adaptation to one challenge may decrease for another. The more an organism adapts to a stable environment (optimality), the less able it is to adapt to new environments that pose new challenges (moving from hot to cold, fresh water to salt water or vice versa)\n Large beaks make it easier to break large nuts, but harder to eat insects out of trees. Homeostasis allows humans to survive in a variety of environmental conditions, but requires more (is limited by) food.  Normal Distribution and measures of variation Many biological factors rely on modifications to the normal distribution.\n quantified as a probability or confidence interval (95% of samples fall within this range of values) \\(\\text{Sample Variance} = \\frac{\\sum_1^n (\\bar x - x_i)^2}{n-1} = s^2\\) \\(\\text{Sample Mean} = \\frac{\\sum_1^n}{n} = \\bar x\\) \\(\\text{confidence statistic (percentage)} = 1-\\alpha\\) \\(\\text{confidence interval} = \\bar x \\pm t_{1-\\alpha} * s/\\sqrt n\\) \\(\\text{Standard Error (Estimate of the mean)} = \\frac{s^2}{n}\\)  Description of the plots used in ecology  y axis refers to the frequency of individuals (higher is more) x axis refers to some limiting factor (food, temperature) Peak refers to the mean of the sample  P Value The P value is the probability that the Null hypothesis is true. This value is calculated with a statistical test (t test when using a sample).\nExample  \\(H_0 = \\text{Cancer is randomly occuring}\\) \\(H_a = \\text{Cancer is caused by radiation damage}\\) \\(P \u0026lt; 0.05\\) There is less than a 5 percent chance that cancer is randomly occurring.  behavioral Thermoregulation. Animals will move to areas that are conducive to their optimal temperature range.\n Snakes bask in the sun to warm up (ectotherms) Humans will go inside (or put on clothes) to warm up (endotherms) Monkeys will bathe in hot springs in cold climates. (endotherms)  Adaptive radiation The tendency for species to diversify to fill open ecological niches.\n Early Plants Finches  Osmoregulation (water stress) and photosynthesis Organisms alter their internal concentration of solute (or consume water) to regulate their internal water content.\n Skin and scales prevent unintended evaporation of water C3 pathway is shared by all plants (the calvin cycle) C4 pathway allows plants to save water by closing the stomata when necessary.  Adaptation vs Plasticity Some animals have larger optimal ranges of physical resources than others (plasticity)\n Evolution thorugh natural selection adapts organisms to their environments Plasticity refers to the ability of an organism to survive in different environments Humans could be said to have high plasticity due to our tool making Octopi make estensive use of RNA editing sites to alter their expression Octopus RNA editing sites (plasticity?)  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/",
	"title": "Discrete Structures",
	"tags": [],
	"description": "",
	"content": "Discrete Structures as taught by Ken Berman\nSet Theory Sets unordered collection (group) of zero or more distinct objects\n set theory (operations about sets) S,T,U,A,B \u0026hellip; for sets List elements in curly braces {a, b, c} Sets are equal if and only if they contain the same elements.  Set Builder  \\(\\{x:P(x)\\}\\), \\(\\{x|P(x)\\}\\). The set of all x such that P(x) {x: x is an integer where x\u0026gt;0 and x\u0026lt;5}  Set relations  \\(\\forall\\) for all \\(\\rightarrow\\) implies \\(\\leftrightarrow\\) if and only if \\(\\exists\\) exists \\(\\nexists\\) does not exist \\(\\wedge\\) and \\(\\vee\\) or \\(x\\in S\\) x in S \\(x \\notin S\\) x not in S  Empty set  \\(\\varnothing = \\{\\}\\)  Subset and superset  \\(S\\subseteq T\\) subset \\(S \\supseteq T\\) superset \\(S\\subset T\\) proper set \\(S \\supset T\\) proper superset  Cardinality  \\(|S|\\) Cardinality of S or the number of elements in S  Universal Set  \\(U\\) is the set containing all other sets (in the problem) universe of discorse  Union  \\(\\{a,b,c\\} \\cup \\{c,e\\} = \\{a,b,c,e\\}\\)  Intersection  \\(\\{a,b,c\\} \\cap \\{c,e\\} = \\{c\\}\\)  Disjointedness \\(A \\cap B = \\varnothing\\)\nSet difference  \\(A - B\\) \\(\\{a,b,c\\} - \\{c,e\\} = \\{a, b\\}\\) Set of all elements in A but not B Compliment with universal set  Compliment  \\(\\bar A = U - A\\)  Symmetric difference  \\(A \\bigoplus B = A \\cup B - A \\cap B\\)  Cartesian product  \\(A \\times B = \\{(a,b) | a \\in A \\text{\\: and \\:} b \\in B\\}\\) \\(|A| \\times |B| = |A \\times B|\\)  Generalized Union and Intersection Union or intersection of many sets.\nStandard Proof techniques Disproof by Counterexample Shows that a conjecture is not true by pointing out an example where the conjecture does not hold.\n No nickels 1 quarter + 5 pennies 3 dimes Greedy method is not appropriate with limited change  Proof by Contradiction Proof that the opposite cannot be true.\nSquare root of 2 is irrational  \\(\\sqrt 2 = a/b\\) \\(a/b\\) is simplified a or b or both must be odd (otherwise could be simplified) \\(2 = a^2/b^2\\) \\(a^2 = 2 ** b^2\\) \\(a^2\\) must be even (2 times any number is even) \\(a\\) is even as well (odd times odd is odd) \\(a = 2 ** k\\) where k is a / 2 \\(2 = (2 ** k)^2/b^2 \\rightarrow b^2 = 2k^2\\) \\(b\\) is also odd by this method \\(a\\) and \\(b\\) cannot be odd \\(\\sqrt 2\\) cannot be rational  Trees  set of nodes first node is root every other node has a \u0026ldquo;parent\u0026rdquo; node  Two Trees  Every node that is not a leaf has 2 child nodes  Binary Trees  Every node has a maximum of 2 children  Logic Boolean operators    Negation NOT Unary \\(\\neg\\)     Conjunction AND Binary \\(\\wedge\\)   Disjunction OR Binary \\(\\vee\\)   Exclusive OR XOR Binary \\(\\bigoplus\\)   Implication IMPLIES Binary \\(\\rightarrow\\)   Bi-conditional IFF Binary \\(\\leftrightarrow\\)    Negation    p \\(\\neg p\\)     T F   F T    Conjunction    p q \\(p \\wedge q\\)     F F F   F T F   T F F   T T T    Disjunction    p q \\(p \\vee q\\)     F F F   F T T   T F T   T T T    Exclusive Or    p q \\(p \\bigoplus q\\)     F F F   F T T   T F T   T T F    Implication    p q \\(p \\rightarrow q\\)     F F T   F T T   T F F   T T T    Bi-conditional    p q \\(p \\leftrightarrow q\\)     F F T   F T F   T F F   T T T    Normal forms Disjunctive Normal Form (DNF)    p q r \\(f\\) Clause Conjunction     F F F T \\(\\neg p \\wedge \\neg q \\vee \\neg r\\)   F F T F    F T F T \\(\\neg p \\wedge \\neg q \\wedge r\\)   F T T T \\(\\neg p \\wedge q \\wedge r\\)   T F F F    T F T F    T T F T \\(p \\wedge q \\wedge \\neg r\\)   T T T T \\(p \\wedge q \\wedge r\\)     Take all of the true statements in the table and write a clause for them Concatenate all of the true clauses together with a disjunction statement \\(\\vee\\) \\(\\neg f \\Leftrightarrow (\\neg p \\wedge \\neg q \\wedge \\neg r) \\vee (\\neg p \\wedge q \\wedge \\neg r) \\vee ( \\neg p \\wedge q \\wedge r) \\vee (p \\wedge q \\wedge r) \\vee (p \\wedge q \\wedge \\neg r) \\vee (p \\wedge q \\wedge r)\\)  Conjunctive Normal Form (CNF)  Negate the DNF form \\(\\neg (\\neg f) \\Leftrightarrow f\\) Use demorgans law to distribute  Expression Trees A binary tree representation of the logical expression\n Set relations reflexive reflexive if, for every element \\(a \\in A\\) we have \\(aRa \\Rightarrow (a, a) \\in R\\)\n \\( A = \\{(a, a): a \\in A\\}\\)  Symmetric symmetric iff \\((x,y) \\in R \\wedge (y,x) \\in R\\)\nTransitive Iff R relates \\(a\\) to \\(b\\) and \\(b\\) to \\( c\\) then \\(a \\) relates to \\(c\\)\n \\(a \u0026lt; b \u0026lt; c \\rightarrow a \u0026lt; c\\) \\(a = b = c \\rightarrow a = c\\)  Modular arithmetic  \\(x \\equiv y (\\text{mod} \\: n) \\leftrightarrow (x-y) \\: \\text {mod} \\: n = 0\\)  Addition Tables  Z mod 4    + 0 1 2 3     0 \\((0 + 0) \\mod 4 = 0\\) 1 2 3   1 \\((1 + 0) \\mod 4 = 1\\) 2 3 0   2 \\((2 + 0) \\mod 4 = 1\\) 3 0 1   3 \\((3 + 0) \\mod 4 = 3\\) 0 1 2      Multiplication tables  Z mod 4    x 0 1 2 3     0 \\((0 \\cdot 0) \\mod 4 = 0\\) 0 0 0   1 \\((1 \\cdot 0) \\mod 4 = 0\\) 1 2 3   2 \\((2 \\cdot 0) \\mod 4 = 0\\) 2 0 2   3 \\((3 \\cdot 0) \\mod 4 = 0\\) 3 2 1      Exam 1 review All-Slides\nSet Theory Union  \\(S = A \\cup B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F T   F T T   F F F    Intersection  \\(S = A \\cap B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F F   F T F   F F F    Difference  \\(S = A - B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F T   F T T   F F F    Symmetric difference  \\(S = A \\bigoplus B\\) \\((a \\in S \\iff (a \\in A \\quad \\text{and} \\quad a \\ni B)\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T F   T F T   F T T   F F F    Demorgans law \\(\\neg (A \\cup B) = \\neg A \\cap \\neg B\\)\nPrinciple of Inclusion-Exclusion \\(|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\)\nProof Techniques  Counterexample Contradiction Induction Trees  Trees  n nodes n-1 edges leaf nodes = intermediate nodes + 1 Total nodes = intermediate nodes + leaf nodes  power sets  \\(A= \\{a, b, c\\}\\) \\(P(A) = \\varnothing , \\{a\\}, \\{b\\}, \\{c\\}, \\{a, b\\}, \\{a, c\\}, \\{b, c\\}, \\{a, b, c\\}\\) \\(|P(A)| = 2^{|A|} = 2^3 = 8\\)  Propositional logic  All F = contradiction All T = Tautology CNF conjunction of all disjunction clauses, unsatisfiable when all combinations of clauses are present DNF disjunction of all conjunction clauses Logic  NP and NP-completeness  P = problem that can be solved in polynomial time NP = non-deterministic polynomial (unknown if it can be solved in polynomial time) NP-complete = any NP problem A can be reduced to problem B  Functions and relations  One to one -\u0026gt; (injective) Onto () -\u0026gt; surjective One to one and Onto -\u0026gt; Bijective Density Equivalence relations  Reflexive, \\(a, a \\in R \\: \\text{for every a in A}\\) Symmetric, \\((b, a \\in R\\: \\text{ whenver} \\: a, b \\in R\\) Transitive, \\((a, b) \\in R \\text{ and } (b, c) \\in R \\text{ then } (a, c) \\in R \\text{ where } a, b, c \\in A \\)   Asymmetric, \\((a, b) \\in R \\text{ implies } (b, a) \\not\\in R\\) AntiSymmetric, assymetric except for the case \\((a, b) \\in R \\rightarrow (b, a) \\in R\\) where \\(b\\) is equal to \\(a\\) Poset (partially ordered set)  reflexive Antisymmetric Transitive    Mod Arithmetic  \\((x + y) \\mod k = (x \\mod k \\quad + \\quad y \\mod k) \\mod k \\) \\(b^{n-1} = 1 \\mod n \\)  Exam 2 review  All-Slides after the first exam. All Topics most formulas are in this one.  RSA Public Key Cryptosystem Extended GCD to compute private key  \\(\\varphi(n) = (p-1)(q-1)\\) \\(se + t\\varphi(n) = g = 1 = gcd(e, \\varphi(n))\\) \\(se \\equiv 1(\\mod \\varphi (n))\\) \\(s = e^{-1}(\\mod \\varphi(n))\\)     R implementation of GCD\nThis is an implementation of Eculid\u0026rsquo;s recursive GCD algorithm. Should be easy to convert to python.\neuclid \u0026lt;- function(a, b) { print(c(a, b)) if (b == 0) { return(a) } euclid(b, a %% b) }   Intro to Graph Theory, Euler\u0026rsquo;s Degree Formula  A Graph is a series of vertices (nodes) that are connected by edges Degree (in this class) is equal to the number of edges that a node is connected to Complete graph is a graph where every node is connected to every other node. A subgraph is a graph made from a subset of nodes in another graph An induced subgraph must have the same edges that the parent graph had.  Graph Isomorphism, Path, Coloring  Isomorphic graphs are identical except for node position, connections are the same nodes in colored Graphs are colored to be different than all of the adjacent nodes. A path is a sequence of vertices connected by edges within a graph. Vertices may be repeated. A path is the same as a trail. Simple paths are paths where vertices are not repeated.  Planar Graphs and Euler\u0026rsquo;s Polyhedron Formula  Supplemental Notes For Planar Graph (Kuratowski) Planar graphs are graphs that can be represented isomorphically without any overlapping edges. \\(\\sum_{g \\in F}\\deg(g) = 2m\\) where g is a vertex in face F, and m is the number of edges 5 regular polyhedra  Tetrahedron Cube Dodecahedron Icosahedron Octahedron    Spanning Trees and Eulerian Circuits  Eulerian path contains all edges in a graph exactly once Eulerian circuit is a circuit that contains all edges exactly once. Simple path that contains every vertex in the graph is a Hamiltonian Path Hamiltonian cycle is a cycle that contains every vertex in the path  Hypercubes and hamiltonian Cycles Implementation of Graphs and Digraphs Digraphs The Web Digraph and PageRank Intro to Combinatorics and Counting Permutations and Combinations Identities, Binomial Theorem, Pascals Triangle Exam 3 review Link to all Slides\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/ecology/",
	"title": "Ecology and Evolution",
	"tags": [],
	"description": "",
	"content": "Ecology and Evolution\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/exam-1-review/",
	"title": "Exam 1 review",
	"tags": [],
	"description": "",
	"content": "All-Slides\nSet Theory Union  \\(S = A \\cup B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F T   F T T   F F F    Intersection  \\(S = A \\cap B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F F   F T F   F F F    Difference  \\(S = A - B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F T   F T T   F F F    Symmetric difference  \\(S = A \\bigoplus B\\) \\((a \\in S \\iff (a \\in A \\quad \\text{and} \\quad a \\ni B)\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T F   T F T   F T T   F F F    Demorgans law \\(\\neg (A \\cup B) = \\neg A \\cap \\neg B\\)\nPrinciple of Inclusion-Exclusion \\(|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\)\nProof Techniques  Counterexample Contradiction Induction Trees  Trees  n nodes n-1 edges leaf nodes = intermediate nodes + 1 Total nodes = intermediate nodes + leaf nodes  power sets  \\(A= \\{a, b, c\\}\\) \\(P(A) = \\varnothing , \\{a\\}, \\{b\\}, \\{c\\}, \\{a, b\\}, \\{a, c\\}, \\{b, c\\}, \\{a, b, c\\}\\) \\(|P(A)| = 2^{|A|} = 2^3 = 8\\)  Propositional logic  All F = contradiction All T = Tautology CNF conjunction of all disjunction clauses, unsatisfiable when all combinations of clauses are present DNF disjunction of all conjunction clauses Logic  NP and NP-completeness  P = problem that can be solved in polynomial time NP = non-deterministic polynomial (unknown if it can be solved in polynomial time) NP-complete = any NP problem A can be reduced to problem B  Functions and relations  One to one -\u0026gt; (injective) Onto () -\u0026gt; surjective One to one and Onto -\u0026gt; Bijective Density Equivalence relations  Reflexive, \\(a, a \\in R \\: \\text{for every a in A}\\) Symmetric, \\((b, a \\in R\\: \\text{ whenver} \\: a, b \\in R\\) Transitive, \\((a, b) \\in R \\text{ and } (b, c) \\in R \\text{ then } (a, c) \\in R \\text{ where } a, b, c \\in A \\)   Asymmetric, \\((a, b) \\in R \\text{ implies } (b, a) \\not\\in R\\) AntiSymmetric, assymetric except for the case \\((a, b) \\in R \\rightarrow (b, a) \\in R\\) where \\(b\\) is equal to \\(a\\) Poset (partially ordered set)  reflexive Antisymmetric Transitive    Mod Arithmetic  \\((x + y) \\mod k = (x \\mod k \\quad + \\quad y \\mod k) \\mod k \\) \\(b^{n-1} = 1 \\mod n \\)  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/exam-2-review/",
	"title": "Exam 2 review",
	"tags": [],
	"description": "",
	"content": " All-Slides after the first exam. All Topics  RSA Public Key Cryptosystem Extended GCD to compute private key  \\(\\varphi(n) = (p-1)(q-1)\\) \\(se + t\\varphi(n) = g = 1 = gcd(e, \\varphi(n))\\) \\(se \\equiv 1(\\mod \\varphi (n))\\) \\(s = e^{-1}(\\mod \\varphi(n))\\)  R implementation of GCD This is an implementation of Eculid\u0026rsquo;s recursive GCD algorithm. Should be easy to convert to python.\neuclid \u0026lt;- function(a, b) { print(c(a, b)) if (b == 0) { return(a) } euclid(b, a %% b) } Intro to Graph Theory, Euler\u0026rsquo;s Degree Formula  A Graph is a series of vertices (nodes) that are connected by edges Degree (in this class) is equal to the number of edges that a node is connected to Complete graph is a graph where every node is connected to every other node. A subgraph is a graph made from a subset of nodes in another graph An induced subgraph must have the same edges that the parent graph had.  Graph Isomorphism, Path, Coloring Planar Graphs and Euler\u0026rsquo;s Polyhedron Formula Spanning Trees and Eulerian Circuits Hypercubes and hamiltonian Cycles Implementation of Graphs and Digraphs Digraphs The Web Digraph and PageRank Intro to Combinatorics and Counting Permutations and Combinations Identities, Binomial Theorem, Pascals Triangle "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/hard-vs-micro/",
	"title": "HardWiredVsMicroProgrammed;",
	"tags": [],
	"description": "",
	"content": "Intro Describes a \u0026ldquo;big Picture\u0026rdquo; of computer systems through an example of a \u0026ldquo;simple\u0026rdquo; computer. This computer is Big Endian (reads the leftmost (most significant) bit first)\nThe Basic Computer Two Principal functional parts  Data path section where processing occurs Control section decodes instructions and leaves control sequence for Data path section  Two types of control units  Hard wired controllers Micro programmed controllers  figure 1   Description of figure 1 Data Path Sections\n   Single 12-bit-wide bus\nexchanges information between pairs of registers\n     Registers + 256 X 12 bit RAM\ncontrolled by 16 control signals\n   Load(L) signals\nActive L clocks or loads bus contents into register on next rising pulse from the system clock\n     Enabled(E) signals\nActive E signal enables the tristate outputs of the register or makes contents of register available to bus\n     register tranfer from example\nContents of Register A to B\n Requires EA signal (contents of A can be read by bus) Requires LB signal (contents of bus can be moved into B)      Arithmetic-Logic-Unit (ALU) A circuit capable of adding or subtracting two 12 bit numbers When the first bit of the Accumulator(ACC) is 1 (Negative Flag) it is considered a negative number (represented using 2s compliment)\n Two input registers  Accumulator (ACC) Register B   Two control signals  Add (A) Subtract (S)    RAM memory    Registers\n   Memory Address Register (MAR)\nRegister where address from memory is temporarily stored\n     Memory Data Register (MDR)\nRegister where data (word) corresponding to the address in MAR is temporarily stored\n       Control Signals\n   Read (R)\ncopies Word stored in RAM at address specified by MAR to MDR\n     Write (W)\ncopies contents (Word) of MDR to RAM at address specified by MAR\n       Steps to write and read data to the RAM\n   Read\n 8-bit address is stored in register MAR Active R signal is supplied to the RAM Data is copied from RAM at address specified by MAR to MDR register       Write\n 8-bit address is stored in register MAR a Word is stored in register MDR Active W signal is supplied to RAM Data is copied from MDR to RAM at address specified by MAR       Note about I/O implementation\nIn this specific computer design, all I/O devices are memory mapped This means that several memory locations are reserved for the I/O devices writing and reading to these I/O devices works in the same way as any other location in the ram.\n    Program Counter (PC) Register    Increment Program Counter (IP) control signal\nWhen PC recieves an IP signal it increments contents of PC by 1\n     Instruction Register (IR)\nHolds the instruction that is about to be executed and provides opcode to the controller/sequencer\n  Computer\u0026rsquo;s Instruction Set A set of all instructions that the computer can process.\nInstruction A 12-bit word made up of a 4-bit opcode and an 8-bit operand address\noperation code (opcode) specifies the action to be taken by the computer\noperand The data that the opcode operates on. Located at a memory location specified by the 8 bit address in the instruction.\nTable 1  Table 1 description    Op-Code\nThe base 10 identity of the 4 bit code(in this computer) at the beginning of every instruction. Specifies the action that will be performed, such as loading, moving values. Not shown is the Fetch Instruction (an instruction used to load the next instruction) and can be thought of as having the opcode 0000, but it really doesn\u0026rsquo;t require one (in Hard-Wired)\n     Opcode are specified in assembly by the Mnemonic\nThe Mnemonic makes the assembly code much more human readable\n LDA (Load accumlulator(ACC)) is equivalent to op-code 0001 STA (Store ACC) is equivalent to op-code 0002 The codes 8(1000) - 15(1111) are all equivalent to the HLT Mnemonic       Register Transfers\nThis column shows how data is transferred from one register to another until the action specified by the opcode has been accomplished.\n LDA 00100010 is an example instruction LDA (Loads data from RAM to accumlulator register) by following the steps required to access data from RAM  MAR \u0026lt;\u0026ndash; IR moves (copies) the contents of the Instruction Register(IR) to the Memory Address Register MDR \u0026lt;\u0026ndash; RAM(MAR) moves the data at address specified by MAR to the MDR register ACC \u0026lt;\u0026ndash; MDR moves the contents of MDR to ACC ACC \u0026lt;\u0026ndash; RAM is then accomplished.         Active Control Signals\nThis column shows the signals that are required (power to signal pin) to perform the specified register transfers. Signal Pins are shown in Figure 1\n MAR \u0026lt;\u0026ndash; IR requires the EI (enable instruction register) and the LM (Load MAR) signals MDR \u0026lt;\u0026ndash; RAM(MAR) requires the R (read) signal    Hard-Wired Control Unit Figure 2  Description of Figure 2 Internal organization of a hard-wired control unit A hard wired version of our example computer. A control unit consists of a Ring counter, an instruction decoder, and a Control matrix\n   Instruction Register (IR)\nContains the current instruction (opcode + operand) The instruction register sends the Op-code (first 4 bits) to the instruction decoder. Each line to the Instruction Decoder represents a bit of the opcode.\n     Instruction Decoder\n Receives the opcode sent to it by the IR Interprets the opcode as a specific signal. Sends a signal to the control matrix corresponding to the opcode from the IR Each line to the Control matrix represents a different signal (a set of pins)  Figure 4\n     Negative Flag\nThe leading bit (\u0026ldquo;negative flag\u0026rdquo;) of the ACC register is fed into the control matrix allowing for Boolean logic within the control matrix.\n     Ring Counter\nSix Consecutive active signals that cycle continuously with every beat of the system clock\n Ring Pulse or Ring Counter Pulse When the signal becomes active (ring pulse T0 means when T0 becomes active)  A useful article about ring counters Table 2\n     Control Matrix\nMost important part of the control unit The control matrix sends out signals to every register in the computer as shown in Figure 1 and facilitates all of the instructions listed in Table 1 using those signals.\n  Figure 3 A visualization of the inner workings of the ring counter Figure 4 A visualization of the inner working of the Instruction Decoder. Shows how each opcode corresponds to an output line. Table 2 Times at which each Control Signal must be active in order to execute the hard-wired Basic Computer\u0026rsquo;s instructions. Fetch instruction The fetch instruction is executed every time the Ring Counter loops. This facilitates the next instruction being sent to the Control unit. This instruction is actually executed during the same ring counter loop as any other instructions. So when an opcode is sent to the control matrix, fetch is executed And whatever instruction is specified by the opcode. This ensures that the next instruction is fetched by the end of the ring counter loop.\nHow does the computer choose which signals to use for an opcode?   LM signal for example, the signal to load data into MAR according to Figure 1 LM has a T3 on the LDA and SDA Rows and a T0 on the Fetch row\n An AND operation is performed between each Tx and its instruction signal (in the column). An OR operation is performed between each AND operation. As this can be represented with bits (1s or 0s) These operations can be simplified to an arithmetic expression LM = T0 + T3*LDA + T3*STA. T0 does not need an AND because the fetch instruction is executed every ring counter cycle. According to the expression: LM is active when T0 is active and when T3 and (LDA or STA) is active    JN(jump negative) Row All Tx in this row have an AND operation with the value of NF(Negative Flag) as well as the instruction signal JN. This provides another level of conditional logic based on the value of the ACC negative flag. The arithmetic expression for the LP column is: LP = T3*JN*NF + T3*JMP(Jump)\n  A list of all the conditional expressions for the control signals (where * = AND, + = OR) is located in Figure 6\n  Figure 5 A hard wired example of how the control matrix would work Figure 6 A list of possible conditional expressions for the example computer. Micro-programmed Control Unit In the Hard Wired control unit example, the signals that come from the control matrix do so because of an actual circuit that is wired to perform the conditional logic shown in Figure 5. In a Micro-Programmed Control Unit an opcode is sent to the Control unit where it fetches a list of Micro-Instructions that together perform the instruction from a memory. The control unit can be thought of as a Computer within a computer.\nMicro-routine A micro-routine is a set of Micro-Instructions that implement an instruction.\nMicro-Instruction Similar to the instruction in a hard-wired computer, the micro-instruction operates on the hard wired circuits within the control unit. A micro-instruction is composed of bits that might correspond to a control signal(LM for example).\nFigure 7 A block diagram of an example micro-programmed control unit. 32 X 24 Control ROM(Read-only memory)  32 24-bit long Micro-Instructions can be stored in the ROM memory A Word is 24 bits in the context of the micro-programmable Control unit Micro-instructions in this example are composed of two fields  16-bit control signal field Each bit corresponds to a control signal 8-bit next-address field address(in ROM) of next micro-instruction to be executed. which permits additional Boolean logic shown in Figure 8.   Micro-instructions(Words) from the Control ROM are fed into the micro-instruction register.  24-bit Micro-instruction Register  analogous to the external computer\u0026rsquo;s Instruction Register. 16 signal lines are the same as the lines coming from the control matrix in Figure 2 and are connected to the signal pins shown in Figure 1. triggered by a falling clock edge See this article about signal edges  micro-counter register  Analogous to the external computer\u0026rsquo;s Program counter Register recieves input from the Multiplexer triggered by a rising clock edge. signal edges  Multiplexer (data selector) Chooses between 3 values to send to the micro-counter register\n Output of Address ROM Output from Current Address Incrementer Address stored in next-address field of the current micro-instruction (CRJA) The conditional logic is shown in the description of Figure 8  16 X 5 Address ROM Fed by outer computer\u0026rsquo;s Instruction Register. The contents of the Instruction Register can be found in Table 3\n maps opcode of external computer\u0026rsquo;s instruction to starting address of corresponding micro-routine. The first Micro-Instruction of the routine Address zero of Address ROM contains address of fetch routine in the Control ROM Other addresses in ROM correspond to the opcodes(external computer) in Table 1 addresses are of micro-routines in Control ROM  Note about signal edges The micro-counter is triggered by a rising clock edge (along with all operations in data path in Figure 1). The Micro-instruction register is triggered by a falling clock edge. In a series of steps:\n micro-counter is triggered (positive edge), presenting the new Micro-instruction address to the control ROM Previous micro-instruction is converted to signals which are sent to external system control ROM presents micro-instruction Word to micro-instruction register Micro-instruction register is triggered (negative edge) causing it to receive the micro-instruction Word  Table 3 Mapping of Op-codes to the contents of the Address ROM in the Control Unit Example using the ADD instruction ADD instruction has the 3 opcode which maps to the 09 micro-routine start address. The address is then goes to the multiplexer -\u0026gt; micro-counter -\u0026gt; Control ROM -\u0026gt; Micro-instruction register\nFigure 8 Next address field of the micro-instruction register.  CD is condition bit when CD is 1 (MAP is zero) the multiplexer\u0026rsquo;s select lines produce a 00 or a 10 (binary) based on the Negative Flag of the ACC register (of external computer). 00 00 selects incrementer address, 10 selects CRJA address MAP causes next microinstruction to be obtained from address ROM When the MAP bit is 1 (Multiplexer\u0026rsquo;s select line produce a 01(binary) (selects the address ROM)) HLT stops clock, terminating execution of activities in the entire computer CRJA (5 bits) is the control ROM jump address field (next-address field). When (CD, MAP is 0) then multiplexer\u0026rsquo;s select lines produce a 10(binary), selecting the CRJA field as the address to the next micro-instruction (in Control ROM).  With no branches, (CD=0, MAP=0, CRJA = address of next instruction in routine) Last micro-instruction in the fetch routine should have map=1 to take an instruction from the address ROM The last micro-instruction in a routine should have 00000(binary) as its CRJA field, CD=0, MAP=0 branching back to the fetch micro-routine.    Table 4 A micro-program (set of all micro-routines) that implements described instruction set from When loaded into Control ROM\n Microroutine Name (Mnemonic) The functional shortened name of each operation specified by the op-codes (Load Accumulator = LDA = 1) Used in assembly programming.\nColumns Micro-Instruction Address to Address of Next Micro-Instruction When taken together, forms the raw contents (changing hexadecimal to binary) of the Micro Control ROM\n   Address-ROM Address (Op-code)\ninput of instruction from external computer that is equivalent to first address of Micro-instruction in micro-routine.\n     Micro-Instruction Address\nMicro-Instruction addresses (2 bits) listed sequentially as part of a micro-routine\n LDA micro-routine = Opcode 1  03 04 05         Control Signal Field\nSame order of signals (bits) as the in Table 2 bits are equivalent to signals which are equivalent to pins.\n     CD\nConditional bit, when 1 causes multiplexer to depend on value of NF if MAP bit is 0\n     MAP\nWhen MAP bit is 1, multiplexer sends address of the first Micro-Instruction for the next micro-routine\n     Halt\nWhen HLT bit is 1, the Micro-Instruction Register will trigger the HALT signal, stopping the clock.\n     Address of Next micro-instruction\nOnly matters if HLT and MAP are not 1 (arbitrary values). The Micro-Instruction Address of the next Micro-Instruction in a non-branching program. Refer to description of Figure 8 for more explanation.\n  Comment This column provides a simple explanation for what happens in the micro-programmable control unit after each Micro-Instruction.\nFetch micro-routine  Control ROM addresses 0, 1, 2 (3 Micro-Instructions)  Micro-Instruction 0  Activates control Signal bits EP, LM Moves data from the Program Counter Register to the Memory Access Register MAR now contains address in RAM of the next instruction CD and MAP bits are zero, next Micro-Instruction corresponds to the address in the CRJA field (Micro-Instruction 1)   Micro-Instruction 1  Activates control Signal bit R Reads the data at adress MAR into Memory Data Register from RAM CD and MAP bits are zero, use CRJA field again (Micro-Instruction 2)   Micro-Instruction 2  Activates control Signal bits ED, LI, IP ED, LI moves the Word from MDR to the Instruction Register IP increments the PC Register New instruction is in IR PC now points to the next instruction MAP is now 1, so the next micro-instruction is from Address ROM (specified by op-code)   Signal pin description is in Figure 1    JN micro-routine  Control ROM addresses 0F, 10, 11  Micro-Instruction 0F  Does nothing except set the CD bit to 1 Execution of the next micro-instruction now depends on value of Negative Flag If NF is 0, increments Micro Program counter 0F + 1 = 10, Micro-Instruction 10 If NF is 1, uses CRJA field (Micro-Instruction 11)   Micro-Instruction 10  Does nothing (no set signal bits) MAP, CD = 0, uses CRJA field (Micro-Instruction 00 (Fetch routine)) PC is not altered, so next instruction is executed normally   Micro-Instruction 11  Activates Control Signal Bits EI, LP Moves contents of IR(least significan eight bits) to PC Next Instruction at location corresponding to value of the least significant 8 bits of IR Control of the computer is transferred to the \u0026ldquo;Jump address\u0026rdquo;      Hard-Wired vs. Micro-programmed Computers  Large majority of computers today are micro-programmed Micro-programmed computers are much more flexible you don\u0026rsquo;t have to make a new computer to change instruction set, only alter Control ROM Changing the firmware is the same as changing the contents of the Control ROM Hard-Wired architecture can not be easily(almost impossible) changed New architecture has do be designed at the hardware level (can\u0026rsquo;t add new instructions easily) Hard-Wired computers are faster (micro-program doesn\u0026rsquo;t have to react to input) and can be easier to manufacture (cheaper).  Old figures "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "I am going to be using this website as a way to consolidate my notes and material for all of my CS classes. DISCLAIMER: I am not responsible for trouble caused by incorrect information here, use at your own risk\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/how-to-write-a-micro-instruction/",
	"title": "How to write a micro-instruction",
	"tags": [],
	"description": "",
	"content": "A micro-instruction is just a binary (often converted to hex) number that represents a list of control signals, and points to the next micro-instruction. Micro-instructions can be changed by changing the A set of micro-instructions forms a micro-routine (otherwise known as an instruction).\nControl signals To understand the micro-instruction, you must understand what the control signals do. I will be referencing the Hard vs Micro paper. It has LA (load accumulator), and EA (enable accumulator) signals. They are essentially just wires connected to the control unit (referenced by the 16 coming out of CONTROL) Each register also generally has a clock signal (CLK), but that is connected directly to the clock circuit and is not required for the micro-instructions. The naming system for the signals is somewhat arbitrary but usually follows the pattern (L, E) (register). This is subverted somewhat by the ALU, but that will most likely be directly defined. The other main difference is the inclusion of I, which just means increment.\nLoad (L) A load signal allows the contents of a register to be changed by the state of the bus (a collection of wires). This signal must be paired with an E signal to do anything.\nEnable (E) An enable signal changes the state of the bus to match the contents of the register. Say a register contains the value 1111, when the enable signal is used for that register, the contents of the bus will now be 1111. This signal must be paired with a L signal to do anything.\nMicro-instructions A micro-instruction is a binary number composed of several fields stored at a specific address in the ROM (read only memory) of the control unit.\nControl field This is just a binary representation of the signals. In the hard vs micro document, the letters are signals are abbreviated into the following order:\n ILELAWLELELEASEL Expanded: IP LP EP LM R W LD ED LI EI LA EA A S E  A micro instruction will generally contain a pair of signals (L E) that transfers the contents of one register into another. The contents of the enabled register moves into the loaded register through the bus. A 1 means that the signal is on while a 0 means the signal is off.\nNext Address field This field has 4 different sub fields. it is used to determine the next micro instruction.\nCD CD (short for condition) is used for conditional logic. This bit makes the next micro-instruction depend on the value of the negative flag. This is generally off, unless you want conditional logic, see JN micro-routine\nMAP the map bit uses the next micro-instruction in the control ROM, see Fetch-routine . This is generally off.\nCRJA field This field consists of an address in the control ROM\nLDA micro-routine here is a step by step on how to write this instruction with micro-instructions.\n Find out where the data is coming from. Map out the sub-steps:  IR -\u0026gt; MAR RAM -\u0026gt; MDR MDR -\u0026gt; ACC   determine the signals for each substep:  EI, LM R ED, LA   Link the micro-instructions with the next address field.  address of next micro-instruction (04) in this case address of next micro-instruction (05) in this case link back to fetch (00) in this case   Write the codes!  (0001000001000000)(0)(0)(0)(0100) (0000010000000000)(0)(0)(0)(0101) (0000000010010000)(0)(0)(0)(0000)   Optional convert to hex  82004 20005 4800    "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/",
	"title": "Intro To Comp Systems",
	"tags": [],
	"description": "",
	"content": "Intro to comp systems as taught by Nitin.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/logic/",
	"title": "Logic",
	"tags": [],
	"description": "",
	"content": "Boolean operators    Negation NOT Unary \\(\\neg\\)     Conjunction AND Binary \\(\\wedge\\)   Disjunction OR Binary \\(\\vee\\)   Exclusive OR XOR Binary \\(\\bigoplus\\)   Implication IMPLIES Binary \\(\\rightarrow\\)   Bi-conditional IFF Binary \\(\\leftrightarrow\\)    Negation    p \\(\\neg p\\)     T F   F T    Conjunction    p q \\(p \\wedge q\\)     F F F   F T F   T F F   T T T    Disjunction    p q \\(p \\vee q\\)     F F F   F T T   T F T   T T T    Exclusive Or    p q \\(p \\bigoplus q\\)     F F F   F T T   T F T   T T F    Implication    p q \\(p \\rightarrow q\\)     F F T   F T T   T F F   T T T    Bi-conditional    p q \\(p \\leftrightarrow q\\)     F F T   F T F   T F F   T T T    "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/mock-exam-questions/",
	"title": "Mock Exam Questions",
	"tags": [],
	"description": "",
	"content": "Question 1 and 2 seem to reference this article: 8086-8088\nQ1 If a physical branch target address is 5A230 when CS = 5200, what will it be if the CS = 7800 ?\n We need to find the offset, where offset + CS(shifted 4 bits) = physical branch target address. CS(C segment register) Offset = Physical branch target address - CS Offset = 5A230 - 52000 = 8230 (hex)  The offset is then used to find the physical branch target adress.\n 78000 + 8230 = 80230 = physical branch target address  Q2 Given that the EA of a data is 2359 and DS = 490B, what is the PA of data?\n EA (effective Address), DS (D segment register), PA(Physical Address).  Physical address is given by EA + DS(shifted 4 bits).\n DS = 490B0 (hex) EA = 2359 (hex) PA = 490B0 + 2359 = 4B409  Q3 Assuming, W, X, Y and Z as memory addresses. Write a program using any machine sequence that will carry out the following: Z ← W + (Z-X).\n.globl main .text main: lw $t1, Z #load z into temporary register 1 lw $t2, X #load x into temporary register 2 sub $s1, $t1, $t2 #s1 \u0026lt;- t1-t2 lw $t1, W #load w into temporary register 1 add $s1, $t1, $s1 #s1 \u0026lt;- t1 sw $s1, Z # stores s1 into Z li $v0, 1 #prints z lw $a0, Z syscall li $v0, 10 #exits syscall .data Z: .word 12 #arbitrary value X: .word 10 #arbitrary value W: .word 5 #arbitrary value Downloadable solution prints 7\nQ4 Assume that the code below is run on a machine with a 2 GHz clock that requires the following number of cycles for each instruction: add, addi, sll, sra take 4cc each, lw takes 5cc, bne, beq take 3cc each. How many seconds will it take to execute this code. The values of registers are $4=0x20, $5= 0x20, $6= 0x30, $7= 0x10.\n.globl main .text main: sra $6, $6, 2 # changes original value of 0x30 / 2^2 = 0xC sll $7, $7, 2 # original value = 0x10, 0x10 * 2^2 = 0x40 add $8, $0, $0 # sets register 8 to 0 L2: add $12, $4, $8 #marks L2, $12 \u0026lt;- 0x20 + $8, $8 is iterator lw $12, 0($12) # $12 = memory at address of ($12) on stack add $9, $0, $0 # $9 = 0; L1: add $11, $5, $9 #marks L1, $11 = 0x20 + $9, $9 is iterator lw $11, 0($11) #$11 = memory at address of ($11) on stack addi $9, $9, 4 #$9 = $9 + 4 bne $9, $7, L1 # goes to L1 if $9 != $7 0x0 + 0x4 * 0x10 = 0x40 = $9 loop executes 0x10 times addi $8, $8, 4 # $8 = $8 + 4 beq $8, $6, L2 # goes to L2 if $8 == $6 exits before $8 can equal $6 Instruction definitions  sra (shift right arithmetic) (sra destination, origin, shift(in bits)) rounds down sll (shift left logical) (sll destination, origin, shift) bne (bne r1, r2, branch address) goes to branch address if r1 != r2 beq (beq, r1, r2, branch address) goest to branch address if r1 == r2  Calculate number of clock cycles Before loops  sra 4cc sll 4cc add 4cc total = 12cc  L2  add 4cc lw 5cc add 4cc L1 clocks addi 4cc beq 3cc total = (20cc + L1 clocks)*1 loop  L3  add 4cc lw 5cc addi 4ccc bne 3cc total = (16cc * 16 loops) = 256 clocks  Total clocks 12cc + 20cc + 256cc = 288cc\nCalculate time 288cc/(2*10^9cc/s) = 1.44 * 10^-7 seconds\nQ5 X[i] = A[B[i]] + C[i+4]\n starting address of A in $1 starting address of B in $2 starting address of C in $3 starting address of X in $4 i value in register $5  Q5 Solution download\n.globl main .text main: sll $s4, $5, 2 # multiplies i * 4 to conform to address form add $t2, $2, $s4 # gets address of B[i] offsets address of B by i lw $t3, ($t2) # sets t3 to value at address t3 = B[i] sll $t1, $t3, 2 # sets t1 to t3 * 4 to conform to address form add $t2, $1, $t1 # offsets addres value in $1 by $t1 A[B[i]] lw $s1, ($t2) # sets t3 to value at address $t2 t3 = A[B[i]] addi $t1, $5, 4 # offsets i by 4 = i+4 sll $t1, $5, 2 # multiplies i*4 to conform to address form add $t2, $3, $t1 # offsets C address by i+4 lw $s2, ($t2) # s2 = C[i+4] add $t1, $s1, $s2 # adds A[B[i]] + C[i+4] add $s3, $4, $s4 # offsets address of X by i sw $t1, ($s3) # stores $t1 to address of ($s3) Q6 The memory units that follow are specified by the number of words times the number of bits per word. How many address lines and input/output data lines are needed in each case? (a) 8K X 16 (b) 2G X 8 (c) 16M X 32 (d) 256K X 64\nPart A  Number of words = 8K Number of bits per word = 16 log base 2 of words = address lines 2^3 * 2^10 = 2^13 = 13 address lines I/O lines = address lines + bits per word 13 + 16 = 29 I/O lines  Part B  2^1 * 2^30 = 2G Address lines = 31 I/O lines = 31 + 8 = 39 39 I/O lines  Part C  2^4 * 2^20 = 16M Address lines = 24 I/O lines = 24 + 32 = 56 56 I/O lines  Part D  2^8 * 2^10 = 256K Address lines = 18 I/O lines = 18 + 64 82 I/O lines  Q7 Find the number of bytes that can be stored in the memories: (a) 8K X 16 (b) 2G X 8 (c) 16M X 32 (d) 256K X 64\nPart A  8 bits per byte 8K words 16 bits per word number of bits = 8K*16 = 2^13 * 2^4 = 2^17 number of bytes = 2^17/2^3 = 2^14 = 16K bytes  Part B  Number of bits = 2G*8 = 2^31*2^3 Number of bytes = 2^31*2^3/2^3 = 2^31 = 2G bytes  Part C  Number of bytes = 16M * 32 / 2^3 = 2^24 * 32 /2^3 = 2^26 = 64M bytes  Part D  Number of bytes = 256K*64/2^3 = 2^18 * 64 /2^3 = 2^24 /2^3 = 2^21 = 2M bytes  TODO Q8 How many 128 x 8 memory chips are needed to provide a memory capacity of 4096 x 16?\n 4096*16 / 128*8 = 64  Q9 Given a 32 x 8 ROM chip with an enable input, show the external connections necessary to construct a 128 x 8 ROM with four chips and a decoder. Useful Video\n 2^5 = 32 5 address lines for each ROM 128/32 = 4 chips 4 outputs on decoder (connected to enable inputs) 2^2 = 4 2 address lines for the decoder 5 + 2 = 7 total lines for complete address   Q10 Assume we have 8GB of word addressable memory with a word size of 64 bits and each refill line of memory stores 16 words. We have a direct-mapped cache with 1024 refill lines. Answer the following questions. More examples\n What is the address format for the cache If we changed the cache to a 2-way set associative cache, how does the format change? If we changed the cache to a fully associative cache, how does the format change?  Solution Part 1  8GB = 8*2^30 bytes 64 bit word = 8 byte word 8GB / 8 byte word = 1GW (gigaword) 1G = 2^30 30 bits for line addresses 16 words per line = 2^4 4 bits for word number on line address format (for MM) = [30-4 bit line address][4 bit word address] 1024 refill lines = 2^10 10 bits for line number 26-10 bits for tag [tag][line number][word number] 16-10-4 = cache memory format  Part 2  2 way set associate cache divide refill lines by 2 1024/2 = 512 = 2^9 26-9 = tag 17-9-4 = 2 way set cache memory format  Part 3  fully associative cache 1 refill line = 2^0 26-0 = tag 26-0-4 = fully associate cache  Hard Drive Sample question This is a writeup of question 1 from here.\nConsider a disk pack with the following specifications 16 surfaces, 128 tracks per surface, 256 sectors per track, 512 bytes per sector\nCapacity of Disk Space  Use the formula Capacity of Disk Space. 16 surfaces * 128 tracks/surface * 256 sectors/track * 512 bytes per sector = \\(2^{28}\\) bytes = 256 MB  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/modular-arithmetic/",
	"title": "Modular arithmetic",
	"tags": [],
	"description": "",
	"content": " \\(x \\equiv y (\\text{mod} : n) \\leftrightarrow (x-y) : \\text {mod} : n = 0\\)  Addition Tables  Z mod 4    + 0 1 2 3     0 \\((0 + 0) \\mod 4 = 0\\) 1 2 3   1 \\((1 + 0) \\mod 4 = 1\\) 2 3 0   2 \\((2 + 0) \\mod 4 = 1\\) 3 0 1   3 \\((3 + 0) \\mod 4 = 3\\) 0 1 2      Multiplication tables  Z mod 4    x 0 1 2 3     0 \\((0 \\cdot 0) \\mod 4 = 0\\) 0 0 0   1 \\((1 \\cdot 0) \\mod 4 = 0\\) 1 2 3   2 \\((2 \\cdot 0) \\mod 4 = 0\\) 2 0 2   3 \\((3 \\cdot 0) \\mod 4 = 0\\) 3 2 1      "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/normal-forms/",
	"title": "Normal forms",
	"tags": [],
	"description": "",
	"content": "Disjunctive Normal Form (DNF)    p q r \\(f\\) Clause Conjunction     F F F T \\(\\neg p \\wedge \\neg q \\vee \\neg r\\)   F F T F    F T F T \\(\\neg p \\wedge \\neg q \\wedge r\\)   F T T T \\(\\neg p \\wedge q \\wedge r\\)   T F F F    T F T F    T T F T \\(p \\wedge q \\wedge \\neg r\\)   T T T T \\(p \\wedge q \\wedge r\\)     Take all of the true statements in the table and write a clause for them Concatenate all of the true clauses together with a disjunction statement \\(\\vee\\) \\(\\neg f \\Leftrightarrow (\\neg p \\wedge \\neg q \\wedge \\neg r) \\vee (\\neg p \\wedge q \\wedge \\neg r) \\vee ( \\neg p \\wedge q \\wedge r) \\vee (p \\wedge q \\wedge r) \\vee (p \\wedge q \\wedge \\neg r) \\vee (p \\wedge q \\wedge r)\\)  Conjunctive Normal Form (CNF)  Negate the DNF form \\(\\neg (\\neg f) \\Leftrightarrow f\\) Use demorgans law to distribute  Expression Trees A binary tree representation of the logical expression\n "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/phys2001/",
	"title": "PHYS2001",
	"tags": [],
	"description": "",
	"content": "PHYS2001 as taught by Kathleen Koenig\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/",
	"title": "Programming Languages",
	"tags": [],
	"description": "",
	"content": "Programming Languages as taught by William Hawkins III\n8/23/2021 Definitions:  programming language: A system of communicating computational ideas between people and computing machines. high-level programming language: A programming language that is independent of any particular machine architecture. syntax: The rules for constructing structurally valid (note: valid, not correct) computer programs in a particular language. names: A means of identifying \u0026ldquo;entities\u0026rdquo; in a programming language. types: A type denotes the kinds of values that a program can manipulate. (A more specific definition will come later in the course). semantics: The effect of each statement\u0026rsquo;s execution on the program\u0026rsquo;s operation.  Concepts: There are four fundamental components of every programming language: syntax, names, types, and semantics.\nPractice:   For your favorite programming language, attempt to explain the most meaningful parts of its syntax. Think about what are valid identifiers in the language, how statements are separated, whether blocks of code are put inside braces, etc.\n  Next, ask yourself how your favorite language handles types. Are variables in the language given types explicitly or implicitly? If the language is compiled, are types assigned to variables before the code is compiled or as the program executes? These are issues that we will discuss in detail in Module 2.\n  Finally, think about how statements in your favorite programming language affect the program\u0026rsquo;s execution. Does the language have loops? if-then statements? function composition? goto?\n  8/25/2021 The value of studying PLs  Every new language that you learn gives you new ways to think about and solve problems.  There is a parallel here with natural languages. Certain written/spoken languages have words for concepts that others do not. Linguists have said that people can only conceive ideas for which there are words. In certain programming languages there may be constructs (\u0026ldquo;words\u0026rdquo;) that give you a power to solve problems and write algorithms in new, interesting ways.   You can choose the right tool for the job.  When all you have is a hammer, everything looks like a nail.   Makes you an increasingly flexible programmer.  The more you know about the concepts of programming languages (and the PLs that you know) the easier it is to learn new languages.   Using PLs better Studying PLs will teach you about how languages are implemented. This \u0026ldquo;awareness\u0026rdquo; can give you insight into the \u0026ldquo;right way\u0026rdquo; to do something in a particular language. For instance, if you know that recursion and looping are equally performant and computationally powerful, you can choose to use the one that improves the readability of your code. However, if you know that iteration is faster (and that\u0026rsquo;s important for your application) then you will choose that method for invoking statements repeatedly.  Programming domains  We write programs to solve real-world problems. The problems that we are attempting to solve lend themselves to programming languages with certain characteristics. Some of those real-world problems are related to helping others solve real-world problems (systems programs):  e.g., operating systems, utilities, compilers, interpreters, etc. There are a number of good languages for writing these applications: C, C++, Rust, Python, Go, etc.   But, most of programs are designed/written to solve actually real-world problems:  scientific calculations: these applications need to be fast (parallel?) and mathematically precise (work with numbers of many kinds). Scientific applications were the earliest programming domain and inspired the first high-level programming language, Fortran. artificial intelligence: AI applications manipulate symbols (in particular, lists of symbols) as opposed to numbers. This application requirement gave rise to a special type of language designed especially for manipulating lists, Lisp (List Processor). world wide web: WWW applications must embed code in data (HTML). Because of how WWW applications advance so quickly, it is important that languages for writing these applications support rapid iteration. Common languages for writing web applications are PERL, Python, JavaScript, Ruby, Go, etc. business: business applications need to produce reports, process character-based data, describe and store numbers with specific precision (aka, decimals). COBOL has traditionally been the language of business applications, although new business applications are being written in other languages these days (Java, the .Net languages). machine learning: machine learning applications require sophisticated math and algorithms and most developers do not want to rewrite these when good alternatives are available. For this reason, a language with a good ecosystem of existing libraries makes an ideal candidate for writing ML programs (Python). game development: So-called AAA games must be fast enough to generate lifelike graphics and immersive scenes in near-real time. For this reason, games are often written in a language that is expressive but generates code that is optimized, C++.    This list is non-exhaustive, obviously!\nThe John von Neumann Model of Computing  This computing model has had more influence on the development of PLs than we can imagine. There are two hardware components in this Model (the processor [CPU] and the memory) and they are connected by a pipe.  The CPU pipes data and instructions (see below) to/from the memory (fetch). The CPU reads that data to determine the action to take (decode). The CPU performs that operation (execute). Because there is only one path between the CPU and the memory, the speed of the pipe is a bottleneck on the processor\u0026rsquo;s efficiency.   The Model is interesting because of the way that it stores instructions and data together in the same memory. It is different than the Harvard Architecture where programs and data are stored in different memory. In the Model, every bit of data is accessible according to its address. Sequential instructions are placed nearby in memory.  For instance, in     for (int i = 0; i \u0026lt; 100; i++) { statement1; statement2; statement3; } statement1, statement2 and statement3 are all stored one after the other in memory.\n Modern implementations of the Model make fetching nearby data fast. Therefore, implementing repeated instructions with loops is faster than implementing repeated loops with recursion. Or is it? This is a particular case where learning about PL will help you as a programmer!  8/27/2021 Programming Paradigms  A paradigm is a pattern or model. A programming paradigm is a pattern of problem-solving thought that underlies a particular genre of programs and languages.  According to their syntax, names and types, and semantics, it is possible to classify languages into one of four categories (imperative, object-oriented, functional and logic). That said, modern researchers in PL are no longer as convinced that these are meaningful categories because new languages are generally a collection of functionality and features and contain bits and pieces from each paradigm.   The paradigms:  Imperative: Imperative languages are based on the centrality of assignment statements to change program state, selection statements to control program flow, loops to repeat statements and procedures for process abstraction (a term we will learn later).  These languages are most closely associated with the von Neumann architecture, especially assignment statements that approximate the piping operation at the hardware level. Examples of imperative languages include C, Fortran, Cobol, Perl.   Object-oriented: Object-oriented languages are based upon a combination of data abstraction, data hiding, inheritance and message passing.  Objects respond to messages by modifying their internal data \u0026ndash; in other words, they become active. The power of inheritance is that an object can reuse an implementation without having to rewrite the code. These languages, too, are closely associated with the von Neumann architecture and (usually) inherit selection statements, assignment statements and loops from imperative programming languages. Examples of object-oriented languages include Smalltalk, Ruby, C++, Java, Python, JavaScript.   Functional: Functional programming languages are based on the concept that functions are first-class objects in the language \u0026ndash; in other words, functions are just another type like integers, strings, etc.  In a functional PL, functions can be passed to other functions as parameters and returned from functions. The loops and selection statements of imperative programming languages are replaced with composition, conditionals, and recursion in functional PLs. A subset of functional PLs are known as pure functional PLs because functions those languages have no side-effects (a side-effect occurs in a function when that function performs a modification that can be seen outside the function \u0026ndash; e.g., changing a value of a parameter, changing a global variable, etc). Examples of functional languages include Lisp, Scheme, Haskell, ML, JavaScript, Python.   Logic: Simply put, logic programming languages are based on describing what to compute and not how to compute it.  Prolog (and its variants) are really the only logic programming language in widespread use.      Language Evaluation Criteria (New Material Alert) There are four (common) criteria for evaluating a programming language:\n Readability: A metric for describing how easy/hard it is to comprehend the meaning of a computer program written in a particular language.   Overall simplicity: The number of basic concepts that a PL has.\n Feature multiplicity: Having more than one way to accomplish the same thing. Operator overloading: Operators perform different computation depending upon the context (i.e., the type of the operands) Simplicity can be taken too far. Consider machine language.    Orthogonality: How easy/hard it is for the constructs of a language to be combined to build higher-level control and data structures.\n Alternate definition: The mutual independence of primitive operations. Orthogonal example: any type of entity in a language can be passed as a parameter to a function. Non-orthogonal example: only certain entities in a language can be used as a return value from a function (e.g., in C/C++ you cannot return an array). This term comes from the mathematical concept of orthogonal vectors where orthogonal means independent. The more orthogonal a language, the fewer exceptional cases there are in the language\u0026rsquo;s semantics. The more orthogonal a language, the slower the language: The compiler/interpreter must be able to compute based on every single possible combination of language constructs. If those combinations are restricted, the compiler can make optimizations and assumptions that will speed up program execution.    Data types: Data types make it easier to understand the meaning of variables.\n e.g., the difference between int userHappy = 0; and bool userHappy = True;    Syntax design\n A PL\u0026rsquo;s reserved words should make things clear. For instance, it is easier to match the beginnings and endings of loops in a language that uses names rather than { }s. The PL\u0026rsquo;s syntax should evoke the operation that it is performing.  For instance, a + should perform some type of addition operation (mathematical, concatenation, etc)       Writeability  Includes all the aspects of Readability, and Expressiveness: An expressive language has relatively convenient rather than cumbersome way of specifying computations.   Reliability: How likely is it that a program written in a certain PL is correct and runs without errors.  Type checking: a language with type checking is more reliable than one without type checking; type checking is testing for operations that compute on variables with incorrect types at compile time or runtime.  Type checking is better done at runtime. A strongly typed programming language is one that is always able to detect type errors either at compile time or runtime.   Exception handling (the ability of a program to intercept runtime errors and take corrective action) and aliasing (when two or more distinct names in a program point to the same resource) affect the PL\u0026rsquo;s reliability.        In truth, there are so many things that affect the reliability of a PL.               The easier a PL is to read and write, the more reliable the code is going to be.   Cost: The cost of writing a program in a certain PL is a function of  The cost to train programmers to use that language The cost of writing the program in that language The time/speed of execution of the program once it is written The cost of poor reliability The cost of maintenance \u0026ndash; most of the time spent on a program is in maintaining it and not developing it!    8/30/2021 Today we learned a more complete definition of imperative programming languages and studied the defining characteristics of variables. Unfortunately we did not get as far as I wanted during the class which means that there is some new material in this edition of the Daily PL!\nImperative Programming Languages Any language that is an abstraction of the von Neumann Architecture can be considered an imperative programming language.\nThere are 5 calling cards of imperative programming languages:\n state, assignment statements, and expressions: Imperative programs have state. Assignment statements are used to modify the program state with computed values from expressions  state: The contents of the computer\u0026rsquo;s memory as a program executes. expression: The fundamental means of specifying a computation in a programming language. As a computation, they produce a value. assignment statement: A statement with the semantic effect of destroying a previous value contained in memory and replacing it with a new value. The primary purpose of the assignment statement is to have a side effect of changing values in memory. As Sebesta says, \u0026ldquo;The essence of the imperative programming languages is the dominant role of the assignment statement.\u0026rdquo;   variables: The abstraction of the memory cell. loops: Iterative form of repetition (for, while, do \u0026hellip; while, foreach, etc) selection statements: Conditional statements (if/then, switch, when) procedural abstraction: A way to specify a process without providing details of how the process is performed. The primary means of procedural abstraction is through definition of subprograms (functions, procedures, methods).  Variables There are 6 attributes of variables. Remember, though, that a variable is an abstraction of a memory cell.\n type: Collection of a variable\u0026rsquo;s valid data values and the collection of valid operations on those values. name: String of characters used to identify the variable in the program\u0026rsquo;s source code. scope: The range of statements in a program in which a variable is visible. Using the yet-to-be-defined concept of binding, there is an alternative definition: The range of statements where the name\u0026rsquo;s binding to the variable is active. lifetime: The period of time during program execution when a variable is associated with computer memory. address: The place in memory where a variable\u0026rsquo;s contents (value) are stored. This is sometimes called the variable\u0026rsquo;s l-value because only a variable associated with an address can be placed on the left side of an assignment operator. value: The contents of the variable. The value is sometimes call the variable\u0026rsquo;s r-value because a variable with a value can be used on the right side of an assignment operator.  Looking forward to Binding (New Material Alert) A binding is an association between an attribute and an entity in a programming language. For example, you can bind an operation to a symbol: the + symbol can be bound to the addition operation.\nBinding can happen at various times:\n Language design (when the language\u0026rsquo;s syntax and semantics are defined or standardized) Language implementation (when the language\u0026rsquo;s compiler or interpreter is implemented) Compilation Loading (when a program [either compiled or interpreted] is loaded into memory) Execution  A static binding occurs before runtime and does not change throughout program execution. A dynamic binding occurs at runtime and/or changes during program execution.\nNotice that the six \u0026ldquo;things\u0026rdquo; we talked about that characterize variables are actually attributes!! In other words, those attributes have to be bound to variables at some point. When these bindings occur is important for users of a programming language to understand. We will discuss this on Wednesday! blob:https://1492301-4.kaf.kaltura.com/903896d9-2341-4dd3-9709-ca344de08719\n9/1/2021 Welcome to the Daily PL for September 1st, 2021! As we turn the page from August to September, we started the month discussing variable lifetime and scope. Lifetime is related to the storage binding and scope is related to the name binding. Before we learned that new material, however, we went over an example of the different bindings and their times in an assignment statement.\nBinding Example Consider a Python statement like this:\nvrb = arb + 5 Recall that a binding is an association between an attribute and an entity. What are some of the possible bindings (and their times) in the statement above?\n The symbol + (entity) must be bound to an operation (attribute). In a language like Python, that binding can only be done at runtime. In order to determine whether the operation is a mathematical addition, a string concatenation or some other behavior, the interpreter needs to know the type of arb which is only possible at runtime. The numerical literal 5 (entity) must be bound to some in-memory representation (attribute). For Python, it appears that the interpreter chooses the format for representing numbers in memory (https://docs.python.org/3/library/sys.html#sys.int%5Finfo (Links to an external site.), https://docs.python.org/3/library/sys.html#sys.float%5Finfo (Links to an external site.)) which means that this binding is done at the time of language implementation. The value (attribute) of the variables vrb and arb (entities) are bound at runtime. Remember that the value of a variable is just another binding.  This is not an exhaustive list of the bindings that are active for this statement. In particular, the variables vrb and arb must be bound to some address, lifetime and scope. Discussing those bindings requires more information about the statement\u0026rsquo;s place in the source code.\nVariables' Storage Bindings The storage binding is related to the variable\u0026rsquo;s lifetime (the time during which a variable is bound to memory). There are four common lifetimes:\n  static: Variable is bound to storage before execution and remains bound to the same storage throughout program execution.\n Variables with static storage binding cannot share memory with other variables (they need their storage throughout execution). Variables with static storage binding can be accessed directly (in other words, their access does not require redirection through a pointer) because the address of their storage is constant throughout execution. Direct addressing means that accesses are faster. Storage for variables with static binding does not need to be repeatedly allocated and deallocated throughout execution \u0026ndash; this will make program execution faster. In C++, variables with static storage binding are declared using the static keyword inside functions and classes. Variables with static storage binding are sometimes referred to as history sensitive because they retain their value throughout execution.    stack dynamic: Variable is bound to storage when it\u0026rsquo;s declaration statements are elaborated (the time when a declaration statement is executed).\n Variables with stack dynamic storage bindings make recursion possible because their storage is allocated anew every time that their declaration is elaborated. To fully understand this point it is necessary to understand the way that function invocation is handled using a runtime stack. We will cover this topic next week. Stay tuned! Variables with stack dynamic storage bindings cannot be directly accessed. Accesses must be made through an intermediary which makes them slower. Again, this will make more sense when we discuss the typical mechanism for function invocation. The storage for variables with stack dynamic storage bindings are constantly allocated and deallocated which adds to runtime overhead.  Variables with stack dynamic storage bindings are not history sensitive.\n  Explicit heap dynamic: Variable is bound to storage by explicit instruction from the programmer. E.g., new / malloc in C/C++.\n The binding to storage is done at runtime when these explicit instructions are executed. The storage sizes can be customized for the use. The storage is hard to manage and requires careful attention from the programmer. The storage for variables with explicit heap dynamic storage bindings are constantly allocated and deallocated which adds to runtime overhead.    Implicit heap dynamic: Variable is bound to storage when it is assigned a value at runtime.\n All storage bindings for variables in Python are handled in this way. https://docs.python.org/3/c-api/memory.html (Links to an external site.) When a variable with implicit heap dynamic storage bindings is assigned a value, storage for that variable is dynamically allocated. Allocation and deallocation of storage for variables with implicit heap dynamic storage bindings is handled automatically by the language compiler/interpreter. (More on this when we discuss memory management techniques in Module 3).    Variables' Name Bindings See the Pl for the Video.\nThis new material is presented above as Episode 1 of PL After Dark. Below you will find a written recap!\nScope is the range of statements in which a variable is visible (either referencable or assignable). Using the vocabulary of bindings, scope can also be defined as the collection of statements which can access a name binding. In other words, scope determines the binding of a name to a variable.\nIt is easy to get fooled into thinking that a variable\u0026rsquo;s name is intrinsic to the variable. However, a variable\u0026rsquo;s name is just another binding like address, storage, value, etc. There are two scopes that most languages employ:\n local: A variable is locally scoped to a unit or block of a program if it is declared there. In Python, a variable that is the subject of an assignment is local to the immediate enclosing function definition. For instance, in   def add(a, b): total = a + b return total total is a local variable.\n global: A variable is globally scoped when it is not in any local scope (terribly unhelpful, isn\u0026rsquo;t it?) Using global variables breaks the principles of encapsulation and data hiding.  For a variable that is used that is not local, the compiler/interpreter must determine to which variable the name refers. Determining the name/variable binding can be done statically or dynamically:\nStatic Scoping This is sometimes also known as lexical scoping. Static scoping is the type of scope that can be determined using only the program\u0026rsquo;s source code. In a statically scoped programming language, determining the name/variable binding is done iteratively by searching through a block\u0026rsquo;s nesting static parents. A block is a section of code with its own scope (in Python that is a function or a class and in C/C++ that is statements enclosed in a pair of {}s). The static parent of a block is the block in which the current block was declared. The list of static parents of a block are the block\u0026rsquo;s static ancestors.\nDynamic Scoping Dynamic scoping is the type of scope that can be determined only during program execution. In a dynamically scoped programming language, determining the name/value binding is done iteratively by searching through a block\u0026rsquo;s nesting dynamic parents. The dynamic parent of a block is the block from which the current block was executed. Very few programming languages use dynamic scoping (BASH, PERL [optionally]) because it makes checking the types of variables difficult for the programmer (and impossible for the compiler/interpreter) and because it increases the \u0026ldquo;distance\u0026rdquo; between name/variable binding and use during program execution. However, dynamic binding makes it possible for functions to require fewer parameters because dynamically scoped non local variables can be used in their place.\n9/3/2021 Welcome to The Daily PL for 9/3/2021. We spent most of Friday reviewing material from Episode 1 of PL After Dark and going over scoping examples in C++ and Python. Before continuing, make sure that you have viewed Episode 1 of PL After Dark.\nScope We briefly discussed the difference between local and global scope.\nIt is easy to get fooled into thinking that a variable\u0026rsquo;s name is intrinsic to the variable. However, a variable\u0026rsquo;s name is just another binding like address, storage, value, etc.\nAs a programmer, when a variable is local determining the name/variable binding is straightforward. Determining the name/variable binding becomes more complicated (and more important) when source code uses a non-local name to reference a variable. In cases like this, determining the name/variable binding depends on whether the language is statically or dynamically scoped.\nStatic Scoping This is sometimes also known as lexical scoping. Static scoping is the type of scope that can be determined using only the program\u0026rsquo;s source code. In a statically scoped programming language, determining the name/variable binding is done iteratively by searching through a block\u0026rsquo;s nesting static parents. A block is a section of code with its own scope (in Python that is a function or a class and in C/C++ that is statements enclosed in a pair of {}s). The static parent of a block is the block in which the current block was declared. The list of static parents of a block are the block\u0026rsquo;s static ancestors.\nHere is pseudocode for the algorithm of determining the name/variable binding in a statically scoped programming language:\ndef resolve(name, current_scope) -\u0026gt; variable s = current_scope while (s != InvalidScope) if s.contains(name) return s.variable(name) s = s.static_parent_scope() return NameError For practice doing name/variable binding in a statically scoped language, play around with an example in Python: static_scope.py\nConsider this \u0026hellip; Python and C++ have different ways of creating scopes. In Python and C++ a new scope is created at the beginning of a function definition (and that scope contains the function\u0026rsquo;s parameters automatically). However, Python and C++ differ in the way that scopes are declared (or not!) for variables used in loops. Consider the following Python and C++ code (also available at loop_scope.cpp and loop_scope.py :\ndef f(): for i in range(1, 10): print(f\u0026#34;i (in loop body): {i}\u0026#34;) print(f\u0026#34;i (outside loop body): {i}\u0026#34;) void f() { for (int i = 0; i\u0026lt;10; i++) { std::cout \u0026lt;\u0026lt; \u0026#34;i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // The following statement will cause a compilation error  // because i is local to the code in the body of the for  // loop.  // std::cout \u0026lt;\u0026lt; \u0026#34;i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } In the C++ code, the for loop introduces a new scope and i is in that scope. In the Python code, the for loop does not introduce a new scope and i is in the scope of f. Try to run the following Python code also available here at loop_scope_error.py to see why this distinction is important:\ndef f(): print(f\u0026#34;i (outside loop body): {i}\u0026#34;) for i in range(1, 10): print(f\u0026#34;i (in loop body): {i}\u0026#34;) Dynamic Scoping Dynamic scoping is the type of scope that can be determined only during program execution. In a dynamically scoped programming language, determining the name/value binding is done iteratively by searching through a block\u0026rsquo;s nesting dynamic parents. The dynamic parent of a block is the block from which the current block was executed. Very few programming languages use dynamic scoping (BASH, Perl [optionally] are two examples) because it makes checking the types of variables difficult for the programmer (and impossible for the compiler/interpreter) and because it increases the \u0026ldquo;distance\u0026rdquo; between name/variable binding and use during program execution. However, dynamic binding makes it possible for functions to require fewer parameters because dynamically scoped non local variables can be used in their place.\ndef resolve(name, current_scope) -\u0026gt; variable s = current_scope while (s != InvalidScope) if s.contains(name) return s.variable(name) s = s.dynamic_parent_scope() return NameError For practice doing name/variable binding in a dynamically scoped language, play around with an example in Python: dynamic_scope.py . Note that because Python is intrinsically a statically scoped language, the example includes some hacking of the Python interpreter to emulate dynamic scoping. Compare the dynamic in the aforementioned Python code with the resolve function in the pseudocode and see if there are differences!\nReferencing Environment (New Material Alert) The referencing environment of a statement contains all the name/variable bindings visible at that statement. NOTE: The example in the book on page 224 is absolutely horrendous \u0026ndash; disregard it entirely. Consider the example online here: referencing_environment.py . Play around with that code and make sure that you understand why certain variables are in the referencing environment and others are not.\nIn case you think that this is theoretical and not useful to you as a real, practicing programmer, take a look at the official documentation of the Python execution model and see how the language relies on the concept of referencing environments: naming-and-binding .\nScope and Lifetime Are Not the Same (New Material Alert) It is common for programmers to think that the scope and the lifetime of a variable are the same. However, this is not always true. Consider the following code in C++ (also available at scope_new_lifetime.cpp)\n#include \u0026lt;iostream\u0026gt; void f(void) { static int variable = 4; } int main() { f(); return 0; } In this program, the scope of variable is limited to the function f. However, the lifetime of variable is the entire program. Just something to keep in mind when you are programming!\n9/8/2021 Welcome to The Daily PL for September 8, 2021. I\u0026rsquo;m not lying when I say that this is the best. edition. ever. There is new material included in this edition which will be covered in a forthcoming episode of PL After Dark. When that video is available, this post will be updated!\nRecap The Type Characteristics of a Language\nIn today\u0026rsquo;s lecture we talked about types (again!). In particular, we talked about the two independent axis of types for a programming language: whether a PL is statically or dynamically typed and whether it is strongly or weakly typed. In other words, the time of the binding of type/variable in a language is independent of that language\u0026rsquo;s ability to detect type errors.\n A statically typed language is one where the type/variable binding is done before the code is run and does not change throughout program execution. A dynamically typed language is one where the type/variable binding is done at runtime and/or may change throughout program execution.    A strongly typed language is one where type errors are always detected (either at before or during program execution) A weakly typed language is one that is, well, not strongly typed.   In order to have a completely a satisfying definition of strongly typed language, we defined type error as any error that occurs when an operation is attempted on a type for which it is not well defined. In Python, \u0026quot;3\u0026quot; + 5 results in a TypeError: can only concatenate str (not \u0026quot;int\u0026quot;) to str. In this example, the operation is + and the types are str and int.\nCertain strongly typed languages appear to be weakly typed because of coercions. A coercion occurs when the language implicitly converts a variable of one type to another. C++ allows the programmer to define operations that will convert the type of a variable from, say, type a to type b. If the compiler sees an expression using a variable of type b where only a variable of type a is valid, then it will invoke that conversion operation automatically. While this adds to the language\u0026rsquo;s flexibility, the conversion behavior may hide the fact that a type error exists and, ultimately, make code more difficult to debug. Note that coercions are done implicitly \u0026ndash; a change between types done at the explicit request of the programmer is know as a (type)cast.\nFinally, before digging in to actual types, we defined type system: A type system is the set of types supported by a language and the rules for their usage.\nAggregate Data Types Aggregate data types are data types composed of one or more basic, or primitive, data types. Do not ask me to write a specific definition for primitive data type \u0026ndash; it will only get us into a circular mess :-)\nArray An array is a homogeneous (i.e., all its elements must be of the same type) aggregate data type in which an individual element is accessed by its position (i.e., index) in the aggregate. There are myriad design decisions associated with a language\u0026rsquo;s implementation of arrays (the type of the index, whether their size must be fixed or whether it can be dynamic, etc.) One of those design decisions is the way that a language lays out a two dimensional array in memory. There are two options: row-major order and column-major order. For a second, forget the concept of rows and columns altogether and consider that you access two dimensional arrays by letters and numbers. See the following diagram:\nThe memory of actual computers is linear. Therefore, two dimensional arrays must be flattened. In \u0026ldquo;letter major\u0026rdquo; order, the slices of the array identified by letters are stored in memory one after the other. In \u0026ldquo;number major\u0026rdquo; order, the slices of the array identified by numbers are stored in memory one after another. Notice that, in \u0026ldquo;letter major\u0026rdquo; order, the numbers \u0026ldquo;change fastest\u0026rdquo; and that, in \u0026ldquo;number major\u0026rdquo; order, the letters \u0026ldquo;change fastest\u0026rdquo;.\nSubstitute \u0026ldquo;row\u0026rdquo; for \u0026ldquo;letter\u0026rdquo; and \u0026ldquo;column\u0026rdquo; for \u0026ldquo;number\u0026rdquo; and, voila, you understand!! The C programming language stores arrays in row-major order; Fortran stores arrays in column-major order.\nKeep in mind that this description is only one way (or many) to store two dimensional arrays. There are (Links to an external site.) others (Links to an external site.).\nAssociative Arrays, Records, Tuples, Lists, Unions, Algebraic Data Types, Pattern Matching, List Comprehensions, and Equivalence All that, and more, in Episode 2 of PL After Dark!\nNote: In this video, I said that Python\u0026rsquo;s Lists function as arrays and that Python does not have true arrays. Your book implies as much in the section on Lists. However, I went back to check, and it does appear that there is a standard module in Python that provides arrays, in certain cases. Take a look at the documentation here: python arrays . The commonly used NumPy package also provides an array type: numpy arrays . While the language, per se, does not define an array type, the presence of the modules (particularly the former) is important to note. Sorry for the confusion!\n9/10/2021 In today\u0026rsquo;s edition of the Daily PL we will recap our discussion from today that covered expressions, order of evaluation, short-circuit evaluation and referential transparency.\nExpressions An expression is the means of specifying computations in a programming language. Informally, it is anything that yields a value. For example,\n 5 is an expression (value 5) 5 + 2 is an expression (value 7) Assuming fun is a function that returns a value, fun() is an expression (value is the return value) Assuming f is a variable, f is an expression (the value is the value of the variable)  Certain languages allow more exotic statements to be expressions. For example, in C/C++, the = operator yields a value (the value of the expression on the right operand). It is this choice by the language designer that allows a C/C++ programmer to write\nint a, b, c, d; a = b = c = d = 5; to initialize all four variables to 5.\nWhen we discuss functional programming languages, we will see how many more things are expressions that programmers typically think are simply statements.\nOrder of Evaluation Programmers learn the associativity and precedence of operations in their languages. That knowledge enables them to mentally calculate the value of statements like 5 + 4 * 3 / 2.\nWhat programmers often forget to learn about their language, is the order of evaluation of operands. Take several of those constants from the previous expression and replace them with variables and function calls:\n5 + a() * c / b() The questions abound:\n Is a() executed before the value of variable c is retrieved? Is b() executed before c()? Is b() executed at all?  In a language with functional side effects, the answer to these questions matter. Why? Consider that a could have a side effect that changes c. If the value of c is retrieved before the execution of a() then the expression will evaluate to a certain value and if the value of c is retrieved after execution of a() then the expression will evaluate to a different value.\nCertain languages define the order of evaluation of operands (Python, Java) and others do not (C/C++). There are reasons why defining the order is a good thing:\n The programmer can depend on that order and benefit from the consistency The program\u0026rsquo;s readability is improved. The program\u0026rsquo;s reliability is improved.  But there is at least one really good reason for not defining that order: optimizations. If the compiler/interpreter can move around the order of evaluation of those operands, it may be able to find a way to generate faster code!\nShort-circuit Evaluation Languages with short-circuit evaluation take these potential optimizations one step further. For a boolean expression, the compiler will stop evaluating the expression as soon as the result is fixed. For instance, in a() \u0026amp;\u0026amp; b(), if a() is false, then the entire statement will always be false, no matter the value of b(). In this case, the compiler/interpreter will simply not execute b(). On the other hand, in a() || b() if a() is true, then the entire statement will always be true, no matter the value of b(). In this case, the compiler/interpreter will simply not execute b().\nA programmer\u0026rsquo;s reliance on this type of behavior in a programming language is very common. For instance, this is a common idiom in C/C++:\nint *variable = nullptr; ... if (variable != nullptr \u0026amp;\u0026amp; *variable \u0026gt; 5) { ... } In this code, the programmer is checking to see whether there is memory allocated to variable before they attempt to read that memory. This is defensive programming thanks to short-circuit evaluation.\nReferential Transparency Most of these issues would not be a problem if programmer\u0026rsquo;s wrote functions that did not have side effects (remember that those are called pure functions). There are languages that will not allow side effects and those languages support referential transparency: A function has referential transparency if its value (its output) depends only on the value of its parameter(s). In other words, if given the same inputs, a referentially transparent function always gives the same output.\nPut It All Together Try you hand at the practice quiz Expressions, precedence, associativity and coercions to check your understanding of the material we covered in class on Friday and the material from your assigned reading! For the why, check out relational.cpp .\n9/13/2021 In today\u0026rsquo;s edition of the Daily PL we will recap our discussion from today that covered subprograms, polymorphism and coroutines!\nSubprograms A subprogram is a type of abstraction. It is process abstraction where the how of a process is hidden from the user who concerns themselves only with the what. A subprogram provides process abstraction by naming a collection of statements that define parameterized computations.​ Again, the collection of statements determines how the process is implemented. Subprogram parameters give the user the ability to control the way that the process executes. There are three types of subprograms:\n Procedure: A subprogram that does not return a value. Function: A subprogram that does return a value. Method: A subprogram that operates with an implicit association to an object; a method may or may not return a value.  Pay close attention to the book\u0026rsquo;s explanation and definitions of terms like parameter, parameter profile, argument, protocol, definition, and declaration.\nSubprograms are characterized by three facts:\n A subprogram has only one entry point Only one subprogram is active at any time Program execution returns to the caller upon completion  Polymorphism Polymorphism allows subprograms to take different types of parameters on different invocations. There are two types of polymorphism:\n ad-hoc polymorphism: A type of polymorphism where the semantics of the function may change depending on the parameter types. parametric polymorphism: A type of polymorphism where subprograms take an implicit/explicit type parameter used to define the types of their subprogram\u0026rsquo;s parameters; no matter the value of the type parameter, in parametric polymorphism the subprogram\u0026rsquo;s semantics are always the same.  Ad-hoc polymorphism is sometimes call function overloading (C++). Subprograms that participate in ad-hoc polymorphism share the same name but must have different protocols. If the subprograms' protocols and names were the same, how would the compiler/interpreter choose which one to invoke? Although a subprogram\u0026rsquo;s protocol includes its return type, not all languages allow ad-hoc polymorphism to depend on the return type (e.g., C++). See the various definitions of add in the C++ code here: subprograms.cpp . Note how they all have different protocols. Further, note that not all the versions of the function add perform an actual addition! That\u0026rsquo;s the point of ad-hoc polymorphism \u0026ndash; the programmer can change the meaning of a function.\nFunctions that are parametrically polymorphic are sometimes called function templates (C++) or generics (Java, soon to be in Go, Rust). A parametrically polymorphic function is like the blueprint for a house with a variable number of floors. A home buyer may want a home with three stories \u0026ndash; the architect takes their variably floored house blueprint and \u0026ldquo;stamps out\u0026rdquo; a version with three floors. Some \u0026ldquo;new\u0026rdquo; languages call this process monomorphization (Links to an external site.). See the definition of minimum in the C++ code here: subprograms.cpp . Note how there is only one definition of the function. The associated type parameter is T. The compiler will \u0026ldquo;stamp out\u0026rdquo; copies of minimum for different types when it is invoked. For example, if the programmer writes\nauto m = minimum(5, 4); then the compiler will generate\nint minimum(int a, int b) { return a \u0026lt; b ? a : b; } behind the scenes.\nCoroutines Just when you thought that you were getting the hang of subprograms, a new kid steps on the block: coroutines. Sebesta defines coroutines as a subprogram that cooperates with a caller. The first time that a programmer uses a coroutine, they call it at which point program execution is transferred to the statements of the coroutine. The coroutine executes until it yields control. The coroutine may yield control back to its caller or to another coroutine. When the coroutine yields control, it does not cease to exist \u0026ndash; it simply goes dormant. When the coroutine is again invoked \u0026ndash; resumed \u0026ndash; the coroutine begins executing where it previously yielded. In other words, coroutines have\n multiple entry points full control over execution until they yield the property that only one is active at a time (although many may be dormant)  Coroutines could be used to write a card game. Each player is a coroutine that knows about the player to their left (that is, a coroutine). The PlayerA coroutine performs their actions (perhaps drawing a card from the deck, etc) and checks to see if they won. If they did not win, then the PlayerA coroutine yields to the PlayerB coroutine who performs the same set of actions. This process continues until a player no longer has someone to their left. At that point, everything unwinds back to the point where PlayerA was last resumed \u0026ndash; the signal that a round is complete. The process continues by resuming PlayerA to start another round of the game. Because each player is a coroutine, it never ceased to exist and, therefore, retains information about previous draws from the deck. When a player finally wins, the process completes. To see this in code, check out cardgame.py .\n9/20/2021 This is an issue of the Daily PL that you are going to want to make sure that you keep safe \u0026ndash; definitely worth framing and passing on to your children! You will want to make sure you remember where you were when you first learned about \u0026hellip;\nFormal Program Semantics Although we have not yet learned about it (we will, don\u0026rsquo;t worry!), there is a robust set of theory around the way that PL designers describe the syntax of their language. You can use regular expressions, context-free grammars, parsers (recursive-descent, etc) and other techniques for defining what is a valid program.\nOn the other hand, there is less of a consensus about how a program language designer formally describes the semantics of programs written in their language. The codification of the semantics of a program written in a particular is known as formal program semantics. In other words, formal program semantics are a precise mathematical description of the semantics of an executing program.​ Sebesta uses the term dynamic semantics which is defines as the \u0026ldquo;meaning[] of the expressions, statements and program units of a programming language.\u0026rdquo;\nThe goal of defining formal program semantics is to understand and reason about the behavior of programs. There are many, many reasons why PL designers want a formal semantics of their language. However, there are two really important reasons: With formal semantics it is possible to prove that\n two programs calculate the same result (in other words, that two programs are equivalent), and a program calculates the correct result.  The alternative to formal program semantics are standards promulgated by committees that use natural language to define the meaning of program elements. Here is an example of a page from the standard for the C programming language:\n If you are interested, you can find the C++ language standard , the Java language standard , the C language standard , the Go language standard and the Python language standard all online.\nTesting vs Proving There is absolutely a benefit to testing software. No doubt about it. However, testing that a piece of software behaves a certain way does not prove that it operates a certain way.\n\u0026ldquo;Program testing can be used to show the presence of bugs, but never to show their absence!\u0026quot; - Edsger Dijkstra\nThere is an entire field of computer science known as formal methods whose goal is to understand how to write software that is provably correct. There are systems available for writing programs about which things can be proven. There is PVS, Coq ,Isabelle , and TLA+ , to name a few. PVS is used by NASA to write its mission-critical software and even it makes an appearance in the movie The Martian .\nThree Types of Formal Semantics There are three common types of formal semantics. It is important that you know the names of these systems, but we will only focus on one in this course!\n Operational Semantics: The meaning of a program is defined by how the program executes on an idealized virtual machine. Denotational Semantics: Program units \u0026ldquo;denote\u0026rdquo; mathematical functions and those functions transform the mathematically defined state of the program. Axiomatic Semantics: The meaning of the program is based on proof rules for each programming unit with an emphasis on proving the correctness of a program.  We will focus on operational semantics only!\nOperational Semantics Program State We have referred to the state of the program throughout this course. We have talked about how statements in imperative languages can have side effects that affect the value of the state and we have talked about how the assignment statement\u0026rsquo;s raison d\u0026rsquo;etre is to change a program\u0026rsquo;s state. For operational semantics, we have to very precisely define a program\u0026rsquo;s state.\nAt all times, a program has a state. A state is just a function whose domain is the set of defined program variables and whose range is V * T where V is the set of all valid variable values (e.g., 5, 6.0, True, \u0026ldquo;Hello\u0026rdquo;, etc) and T is the set of all valid variable types (e.g., Integer, Floating Point, Boolean, String, etc). ​In other words, you can ask a state about a particular variable and, if it is defined, the function will return the variable\u0026rsquo;s current value and its type.\nIt is important to note that PL researchers have math envy. They are not mathematicians but they like to use Greek symbols. So, here we go:\n\\begin{equation*} \\sigma(x) = (v, \\tau) \\end{equation*}\nThe state function is denoted with the σ . τ always represents some arbitrary variable type. Generally, v represents a value. So, you can read the definition above as \u0026ldquo;Variable x has value v and type τ in state σ.\u0026rdquo;\nProgram Configuration Between execution steps (a term that we will define shortly), a program is always in a particular configuration:\n\\begin{equation*} \u0026lt;e, \\sigma\u0026gt; \\end{equation*}\nThis means that the program in state σ\nis about to evaluate expression e.\nProgram Steps A program step is an atomic (indivisible) change from one program configuration to another. Operational semantics defines steps using rules. The general form of a rule is\n\\begin{equation*} \\frac{premises}{conclusion} \\end{equation*}\nThe conclusion is generally written like \u0026lt;e, σ\u0026gt; ⟶ (v, τ, σ). This statement means that, when the premises hold, the rule evaluates to a value (v), type (τ) and (possibly modified) state (σ') after a single step of execution of a program in configuration \u0026lt;e, σ\u0026gt;. Note that rules do not yield configurations. All this will make sense when we see an example.\nExample 1: Defining the semantics of variable access. In STIMPL, the expression to access a variable, say i, is written like Variable(\u0026ldquo;i\u0026rdquo;). Our operational semantic rule for evaluating such an access should \u0026ldquo;read\u0026rdquo; something like: When the program is about to execute an expression to access variable i in a state σ, the value of the expression will be the triple of i\u0026rsquo;s value, i\u0026rsquo;s type and the unchanged state σ.\u0026rdquo; In other words, the evaluation of the next step of a program that is about to access a value is the value and type of the variable being accessed and the program\u0026rsquo;s state is unchanged.\nLet\u0026rsquo;s write that formally!\n\\begin{equation*} \\(\\frac{\\sigma(x) \\rightarrow (v, \\tau)} {\u0026lt;\\text{Variable}(x), \\sigma \u0026gt; \\rightarrow (v, \\tau, \\sigma)}\\) \\end{equation*}\nState Update How do we write down that the state is being changed? Why would we want to change the state? Let\u0026rsquo;s answer the second question first: we want to change the state when, for example, there is an assignment statement. If σ (\u0026ldquo;i\u0026rdquo;) = (4, Integer) and then the program evaluated an expression like Assign(Variable(\u0026ldquo;i\u0026rdquo;), IntLiteral(2)), we don\u0026rsquo;t want the σ\nfunction to return (4, Integer) any more! We want it to return (2, Integer). We can define that mathematically like:\n\\begin{equation*} \\sigma[(v,\\tau)/x](y)= \\begin{cases} \u0026amp; \\sigma(y) \\quad y \\ne x \\\\ \u0026amp;(v,\\tau) \\quad y=x \\end{cases} \\end{equation*}\nThis means that if you are querying the updated state for the variable that was just reassigned (x), then return its new value and type (m and τ ). Otherwise, just return the value that you would get from accessing the existing σ\n.\nExample 2: Defining the semantics of variable assignment (for a variable that already exists). In STIMPL, the expression to overwrite the value of an existing variable, say i, with, say, an integer literal 5 is written like Assign(Variable(\u0026quot;i\u0026quot;), IntLiteral(5)). Our operational semantic rule for evaluating such an assignment should \u0026ldquo;read\u0026rdquo; something like: When the program is about to execute an expression to assign variable i to the integer literal 5 in a state σ and the type of the variable i in state σ is Integer, the value of the expression will be the triple of 5, Integer and the changed state σ' which is exactly the same as state σ\nexcept where (5, Integer) replaced i\u0026rsquo;s earlier contents.\u0026quot; That\u0026rsquo;s such a mouthful! But, I think we got it. Let\u0026rsquo;s replace some of those literals with symbols for abstraction purposes and then write it down!\n\\begin{equation*} \\frac{\u0026lt;e, \\sigma\u0026gt; \\longrightarrow (v, \\tau, \\sigma'), \\sigma(x) \\longrightarrow (*,, \\tau)} {\u0026lt;\\text{Assign(Variable)}(x, e), \\sigma \u0026gt; \\longrightarrow (v, \\tau, \\sigma' [(v, \\tau)/x])} \\end{equation*}\nLet\u0026rsquo;s look at it step-by-step:\n\\begin{equation*} \u0026lt;Assign(Variable(x),e),\\sigma\u0026gt; \\end{equation*}\nis the configuration and means that we are about to execute an expression that will assign value of expression e to variable x. But what is the value of expression e? The premise\n\\begin{equation} \u0026lt;e,\\sigma\u0026gt;⟶(v,\\tau, \\sigma′) \\end{equation}\ntells us that the value and type of e when evaluated in state σ is v, and τ. Moreover, the premise tells us that the state may have changed during evaluation of expression e and that subsequent evaluation should use a new state, σ\n\u0026lsquo;. Our mouthful above had another important caveat: the type of the value to be assigned to variable x must match the type of the value already stored in variable x. The second premise\n\\begin{equation*} \\sigma′(x)\\longrightarrow(*, \\tau) \\end{equation*}\ntells us that the types match \u0026ndash; see how the τs are the same in the two premises? (We use the * to indicate that we don\u0026rsquo;t care what that value is!)\nNow we can just put together everything we have and say that the expression assigning the value of expression e to variable x evaluates to\n\\begin{equation*} (v,\\tau,\\sigma′[(v,\\tau)/x]) \\end{equation*}\nThat\u0026rsquo;s Great, But Show Me Code! Well, Will, that\u0026rsquo;s fine and good and all that stuff. But, how do I use this when I am implementing STIMPL? I\u0026rsquo;ll show you! Remember the operational semantics for variable access:\n\\begin{equation*} \\(\\frac{\\sigma(x) \\rightarrow (v, \\tau)} {\u0026lt;\\text{Variable}(x), \\sigma \u0026gt; \\rightarrow (v, \\tau, \\sigma)}\\) \\end{equation*}\nCompare that with the code for it\u0026rsquo;s implementation in the STIMPL skeleton that you are provided for Assignment 1:\ndef evaluate(expression, state): ... case Variable(variable_name=variable_name): value = state.get_value(variable_name) if value == None: raise InterpSyntaxError(f\u0026#34;Cannot read from {variable_name}before assignment.\u0026#34;) return (*value, state) At this point in the code we are in a function named evaluate whose first parameter is the next expression to evaluate and whose second parameter is a state. Does that sound familiar? That\u0026rsquo;s because it\u0026rsquo;s the same as a configuration! We use pattern matching to select the code to execute. The pattern is based on the structure of expression and we match in the code above when expression is a variable access. Refer to Pattern Matching in Python for the exact form of the syntax. The state variable is an instance of the State object that provides a method called get_value (see Assignment 1: Implementing STIMPL for more information about that function) that returns a tuple of (v, τ) In other words, get_value works the same as σ. So,\nvalue = state.get_value(variable_name) is a means of implementing the premise of the operational semantics.\nreturn (*value, state) yields the final result! Pretty cool, right?\nLet\u0026rsquo;s do the same analysis for assignment:\n\\(\\frac{\u0026lt;e,\\sigma\u0026gt;\\longrightarrow(v,\\tau,\\sigma′),\\sigma′(x)\\longrightarrow(*,\\tau)}{\u0026lt;Assign(Variable(x),e),σ\u0026gt;\\longrightarrow(v,\\tau,σ′[(v,\\tau)/x])}\\)\nAnd here\u0026rsquo;s the implementation:\ndef evaluate(expression, state): ... case Assign(variable=variable, value=value): value_result, value_type, new_state = evaluate(value, state) variable_from_state = new_state.get_value(variable.variable_name) _, variable_type = variable_from_state if variable_from_state else (None, None) if value_type != variable_type and variable_type != None: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Assignment: Cannot assign {value_type}to {variable_type}\u0026#34;\u0026#34;\u0026#34;) new_state = new_state.set_value(variable.variable_name, value_result, value_type) return (value_result, value_type, new_state) First, look at\nvalue_result, value_type, new_state = evaluate(value, state) which is how we are able to find the values needed to satisfy the left-hand premise. value_result is v, value_type is τ and new_state is σ\u0026rsquo;.\nvariable_from_state = new_state.get_value(variable.variable_name) is how we are able to find the values needed to satisfy the right-hand premise. Notice that we are using new_state (σ') to get variable.variable_name (x). There is some trickiness in_, variable_type = variable_from_state if variable_from_state else (None, None) to set things up in case we are doing the first assignment to the variable (which sets its type), so ignore that for now! Remember that in our premises we guaranteed that the type of the variable in state σ' matches the type of the expression:\nif value_type != variable_type and variable_type != None: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Assignment: Cannot assign {value_type}to {variable_type}\u0026#34;\u0026#34;\u0026#34;) performs that check!\nnew_state = new_state.set_value(variable.variable_name, value_result, value_type) generates a new, new state (σ′[(v,τ)/x]) and\nreturn (value_result, value_type, new_state) yields the final result!\n9/22/2021 Like other popular newspapers that do in-depth analysis of popular topics (Links to an external site.), this edition of the Daily PL is part 2/2 of an investigative report on \u0026hellip;\nFormal Program Semantics In our previous class, we discussed the operational semantics of variable access and variable assignment. In this class we explored the operational semantics of the addition operator and the if/then statement.\nA Quick Review of Concepts At all times, a program has a state. A state is just a function whose domain is the set of defined program variables and whose range is V * T where V is the set of all valid variable values (e.g., 5, 6.0, True, \u0026ldquo;Hello\u0026rdquo;, etc) and T is the set of all valid variable types (e.g., Integer, Floating Point, Boolean, String, etc). ​In other words, you can ask a state about a particular variable and, if it is defined, the function will return the variable\u0026rsquo;s current value and its type.\nHere is the formal definition of the state function:\n\\begin{equation*} \\(\\sigma(x) = (v, \\tau)​\\) \\end{equation*}\nThe state function is denoted with the σ . τ always represents some arbitrary variable type. Generally, v represents a value. So, you can read the definition above as \u0026ldquo;Variable x has value v and type τ in state σ.\u0026rdquo;\nBetween execution steps, a program is always in a particular configuration:\n\\begin{equation*} \u0026lt;e, \\sigma\u0026gt; \\end{equation*}\nThis notation means that the program in state σ is about to evaluate expression e.\nA program step is an atomic (indivisible) change from one program configuration to another. Operational semantics defines steps using rules. The general form of a rule is\n\\begin{equation*} \\frac{premises}{conclusion} \\end{equation*}\nThe conclusion is generally written like \u0026lt;e, σ\u0026gt; ⟶ (v, τ, σ) which means that when the premises hold, the expression e evaluated in state σ evaluates to a value (v), type (τ) and (possibly modified) state (σ') after a single step of execution.\nDefining the Semantics of the Addition Expression In STIMPL, the expression to \u0026ldquo;add\u0026rdquo; two values n1 and n2 is written like Add(n1, n2). By the rules of the STIMPL language, for an addition to be possible, n1 and n2 must\n have the same type and have Integer, Floating Point or String type.  Because every unit in STIMPL has a value, we will define the operational semantics using two arbitrary expressions, e1 and e2. The program configuration to which we are giving semantics is\n\\begin{equation*} \u0026lt;Add(e_1),e_2),\\sigma\u0026gt; \\end{equation*}\nBecause our addition operator applies when its operands are three different types, we technically need three different rules for its evaluation. Let\u0026rsquo;s start with the operational semantics for add when its operands are of type Integer:\n\\begin{equation*} \\frac{\u0026lt;e_1,\\sigma\u0026gt;⟶(v_1,Integer,\\sigma),\u0026lt;e_2,\\sigma\u0026gt;⟶(v_2,Integer,\\sigma \\prime)}{\u0026lt;Add(e1,e2),σ\u0026gt;⟶(v1+v2,Integer,\\sigma\\prime)} \\end{equation*}\nLet\u0026rsquo;s look at the premises. First, there is\n\\begin{equation*} \u0026lt;e_1,\\sigma\u0026gt;⟶(v1,Integer,\\sigma \\prime) \\end{equation*}\nwhich means that, when evaluated in state σ, expression e1 has the value v1 and type Integer and may modify the state (to σ'). Notice that we are not using τ for the resulting type of the evaluation? Why? Because using τ indicates that this rule applies when the evaluation of e1 in state σ evaluates to any type (which we \u0026ldquo;assign\u0026rdquo; to τ in case we want to use it again in a later premise). Instead, we are explicitly writing Integer which indicates that this rule only defines the operational semantics for Add(e1, e2) in state σ when the expression e1 evaluates to a value of type Integer in state σ\n.\nAs for the second premise\n\\begin{equation*} \u0026lt;e_2,\\sigma \\prime\u0026gt;⟶(v_2,Integer,\\sigma\\prime \\prime) \\end{equation*}\nwe see something very similar. Again, our premise prescribes that, when evaluated in state σ' (note the ' there), e2\u0026rsquo;s type is an Integer. It is for this reason that we can be satisfied that this rule only applies when the types of the Add\u0026rsquo;s operands match and are integers! We \u0026ldquo;thread through\u0026rdquo; the (possibly) modified σ' when evaluating e2 to enforce the STIMPL language\u0026rsquo;s definition that operands are evaluated strictly left-to-right.\nAs for the conclusion,\n\\begin{equation*} (v_1+v_2,Integer,\\sigma \\prime \\prime) \\end{equation*}\nshows the value of this expression. We will assume here that + works as expected for two integers. Because the operands are integers, we can definitively write that the type of the addition will be an integer, too. We use σ'' as the resulting state because it\u0026rsquo;s possible that evaluation of the expressions of both e1 and e2 caused side effects.\nThe rule that we defined covers only the operational semantics for addition of two integers. The other cases (for floating-point and string types) are almost copy/paste.\nNow, how does that translate to an actual implementation?\ndef evaluate(expression, state): match expression: ... case Add(left=left, right=right): result = 0 left_result, left_type, new_state = evaluate(left, state) right_result, right_type, new_state = evaluate(right, new_state) if left_type != right_type: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Add: Cannot add {left_type}to {right_type}\u0026#34;\u0026#34;\u0026#34;) match left_type: case Integer() | String() | FloatingPoint(): result = left_result + right_result case _: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Cannot add {left_type}s\u0026#34;\u0026#34;\u0026#34;) return (result, left_type, new_state) In this snippet, the local variables left and right are the equivalent of e1 and e2, respectively, in the operational semantics. After initializing a variable to store the result, the evaluation of the premises is accomplished. new_state matches σ'' after being assigned and reassigned in those two evaluations. Next, the code checks to make sure that the types of the operands matches. Finally, if the types of the operands is an integer, then the result is just a traditional addition (+ in Python).\nYou can see the implementation for the other types mixed in this code as well. Convince yourself that the code above handles all the different cases where an Add is valid in STIMPL.\nDefining the Semantics of the If/Then/Else Expression In STIMPL, we write an If/Then/Else expression like If(c, t, f) where c is any boolean-typed expression, t is the expression to evaluate if the value of c is true and f is the expression to evaluate if the value of c is false. The value/type/updated state of the entire expression is the value/type/updated state that results from evaluating t when c is true and the value/type/updated state that results from evaluating f when c is false. This means that we are required to write two different rules to completely define the operational semantics of the If/Then/Else expression: one for the case where c is true and the other for the case when c is false. Sounds like the template that we used for the Add expression, doesn\u0026rsquo;t it? Because the two cases are almost the same, we will only go through writing the rule for when the condition is true:\n\\begin{equation*} \\frac{\u0026lt;c,\\sigma\u0026gt;\\longrightarrow(True,Boolean,\\sigma \\prime),\u0026lt;t,\\sigma \\prime\u0026gt;\\longrightarrow(v,\\tau,\\sigma\\prime \\prime)}{\u0026lt;If(c,t,f),σ\u0026gt;⟶(v,\\tau, \\sigma \\prime \\prime)} \\end{equation*}\nAs in the premises for the operational semantics of the Add operator, the first premise in the operational semantics above uses literals to guarantee that the rule only applies in certain cases:\n\\begin{equation*} \u0026lt;c,\\sigma \\prime\u0026gt;\\longrightarrow(True,Boolean,\\sigma\\prime \\prime) \\end{equation*}\nmeans that the rule only applies when c, evaluated in state σ, has a value of True and a boolean type. We use the second premise\n\\begin{equation*} \u0026lt;t,\\sigma\\prime\u0026gt;⟶(v,\\tau,\\sigma \\prime \\prime) \\end{equation*}\nto \u0026ldquo;get\u0026rdquo; some values that we will use in the conclusion. v and τ are the value and the type, respectively, of t when it is evaluated in state σ'. Note that we evaluate t in state σ' because the evaluation of the condition statement may have modified state σ and we want to thread that through. Evaluation of t in state σ' may modify σ', generating σ''. The combination of these premises are combined to define that the entire expression evaluates to\n\\begin{equation*} (v,\\tau,\\sigma\\prime \\prime) \\end{equation*}\nAgain, the pattern is the same for writing the operational semantics when the condition is false.\nLet\u0026rsquo;s look at how this translates into actual working code:\ndef evaluate(expression, state): match expression: ... case If(condition=condition, true=true, false=false): condition_value, condition_type, new_state = evaluate(condition, state) if not isinstance(condition_type, Boolean): raise InterpTypeError(\u0026#34;Cannot branch on non-boolean value!\u0026#34;) result_value = None result_type = None if condition_value: result_value, result_type, new_state = evaluate(true, new_state) else: result_value, result_type, new_state = evaluate(false, new_state) return (result_value, result_type, new_state) The local variables condition, true and false match c, t and f, respectively from the rule in the operational semantics. The first step in the implementation is to determine the value/type/updated state when c is evaluated in state σ. Immediately after doing that, the code checks to make sure that the condition statement has boolean type. Remember how our rule only applies when this is the case? Next, depending on whether the condition evaluated to true or false, the appropriate next expression is evaluated in the σ' state (new_state). It is the result of that evaluation that is the ultimate value of the expression and what is returned.\n9/24/2021 As we conclude the penultimate week of September, we are turning the page from imperative programming and beginning our work on object-oriented programming!\nThe Definitions of Object-Oriented Programming We started off by attempting to describe object-oriented programming using two different definitions:\n A language with support for abstraction of abstract data types (ADTs).​ (from Sebesta) A language with support for objects, containers of data (attributes, properties, fields, etc.) and code (methods).​ (from Wikipedia (Links to an external site.))  As graduates of CS1021C and CS1080C, the second definition is probably not surprising. The first definition, however, leaves something to be desired. Using Definition (1) means that we have to a) know the definition of abstraction and abstract data types and b) know what it means to apply abstraction to ADTs.\nAbstraction (Reprise) There are two fundamental types of abstractions in programming: process and data. We have talked about the former but the latter is new. When we talked previously about process abstractions, we did attempt to define the term abstraction but it was not satisfying.\nSebesta formally defines abstraction as the view or representation of an entity that includes only the most significant attributes. This definition seems to align with our notion of abstraction especially the way we use the term in phrases like \u0026ldquo;abstract away the details.\u0026rdquo; It didn\u0026rsquo;t feel like a good definition to me until I thought of it this way:\nConsider that you and I are both humans. As humans, we are both carbon based and have to breath to survive. But, we may not have the same color hair. I can say that I have red hair and you have blue hair to point out the significant attributes that distinguish us. I need not say that we are both carbon based and have to breath to survive because we are both human and we have abstracted those facts into our common humanity.\nWe returned to this point at the end of class when we described how inheritance is the mechanism of object-oriented programming that provides abstraction over ADTs. Abstract Data Types (ADTs)\nNext, we talked about the second form of abstraction available to programmers: data abstraction. As functions, procedures and methods are the syntactic and semantic means of abstracting processes in programming languages, ADTs are the syntactic and semantic means of abstracting data in programming languages. ADTs combine (encapsulate) data (usually called the ADT\u0026rsquo;s attributes, properties, etc) and operations that operate on that data (usually called the ADT\u0026rsquo;s methods) into a single entity.\nWe discussed that hiding is a significant advantage of ADTs. ADTs hide the data being represented and allow that data\u0026rsquo;s manipulation only through pre-defined methods, the ADT\u0026rsquo;s interface. The interface typically gives the ADT\u0026rsquo;s user the ability to manipulate/access the data internal to the type and perform other semantically meaningful operations (e.g., sorting a list).\nWe brainstormed some common ADTs:\n Stack Queue List Array Dictionary Graph Tree  These are are so-called user-defined ADTs because they are defined by the user of a programming language and composed of primitive data types.\nNext, we tackled the question of whether primitives are a type of ADT. A primitive type like floating point numbers would seem to meet the definition of an abstract data type:\n It\u0026rsquo;s underlying representation is hidden from the user (the programmer does not care whether FPs are represented according to IEEE754 or some other specification) There are operations that manipulate the data (addition, subtraction, multiplation, division).  The Requirements of an Object-Oriented Programming Language ADTs are just one of the three requirements that your textbook\u0026rsquo;s author believes are required for a language to be considered object oriented. Sebesta believes that, in addition to ADTs, an object-oriented programming language requires support for inheritance and dynamic method binding.\nInheritance It is inheritance where OOPs provide abstraction for ADTs. Inheritance allows programmers to abstract ADTs into common classes that share common characteristics. Consider three ADTs that we identified: trees, linked lists and graphs. These three ADTs all have nodes (of some kind or another) which means that we could abstract them into a common class: node-based things. A graph would inherit from the node-based things ADT so that its implementer could concentrate on what makes it distinct \u0026ndash; its edges, etc.\nDon\u0026rsquo;t worry if that is too theoretical. It does not negate the fact that, through inheritance, we are able to implement hierarchies that can be \u0026ldquo;read\u0026rdquo; using \u0026ldquo;is a\u0026rdquo; the way that inheritance is usually defined. With inheritance, cats inherit from mammals and \u0026ldquo;a cat is a mammal\u0026rdquo;.\nSubclasses inherit from ancestor classes. In Java, ancestor classes are called superclasses and subclasses are called, well, subclasses. In C++, ancestor classes are called base classes and subclasses are called derived classes. Subclasses inherit both data and methods.\nDynamic Method Binding In an OOP, a variable that is typed as Class A can be assigned anything that is actually a Class A or subclass thereof. We have not officially covered this yet, but in OOP a subclass can redefine a method defined in its ancestor.\nAssume that every mammal can make a noise. That means that every dog can make a noise just like every cat can make a noise. Those noises do not need to be the same, though. So, a cat \u0026ldquo;overrides\u0026rdquo; the mammal\u0026rsquo;s default noise and implements their own (meow). A dog does likewise (bark). A programmer can define a variable that holds a mammal and that variable can contain either a dog or a cat. When the programmer invokes the method that causes the mammal to make noise, then the appropriate method must be called depending on the actual type in the variable at the time. If the mammal held a dog, it would bark. If the mammal held a cat, it would meow.\nThis resolution of methods at runtime is known as dynamic method binding.\nOOP Example with Inheritance and Dynamic Method Binding abstract class Mammal { protected int legs = 0; Mammal() { legs = 0; } abstract void makeNoise(); } class Dog extends Mammal { Dog() { super(); legs = 4; } void makeNoise() { System.out.println(\u0026#34;bark\u0026#34;); } } class Cat extends Mammal { Cat() { super(); legs = 4; } void makeNoise() { System.out.println(\u0026#34;meow\u0026#34;); } } public class MammalDemo { static void makeARuckus(Mammal m) { m.makeNoise(); } public static void main(String args[]) { Dog fido = new Dog(); Cat checkers = new Cat(); makeARuckus(fido); makeARuckus(checkers); } } This code creates a hierarchy with Mammal at the top as the superclass of both the Dog and the Cat. In other words, Dog and Cat inherit from Mammal. The abstract keyword before class Mammal indicates that Mammal is a class that cannot be directly instantiated. We will come back to that later. The Mammal class declares that there is a method that each of its subclasses must implement \u0026ndash; the makeNoise function. If a subclass of Mammal fails to implement that function, it will not compile. The good news is that Cat and Dog do both implement that function and define behavior in accordance with their personality!\nThe function makeARuckus has a parameter whose type is a Mammal. As we said above, in OOP that means that I can assign to that variable a Mammal or anything that inherits from Mammal. When we call makeARuckus with an argument whose type is Dog, the function relies of dynamic method binding to make sure that the proper makeNoise function is called \u0026ndash; the one that barks \u0026ndash; even though makeARuckus does not know whether m is a generic Mammal, a Dog or a Cat. It is because of dynamic method binding that the code above generates\nbark meow as output.\n9/27/2021 It\u0026rsquo;s the last week of September but the first full week of OOP. Let\u0026rsquo;s do this!\nOverriding in OOP Recall the concept of inheritance that we discussed in the last class. Besides its utility as a formalism that describes the way a language supports abstraction of ADTs (and, therefore, makes it a plausibly OO language), inheritance provides a practical benefit in software engineering. Namely, it allows developers to build hierarchies of types.\nHierarchies are composed of pairs of classes \u0026ndash; one is the superclass and the other is the subclass. A superclass could conceivably be itself a subclass. A subclass could itself be a superclass. In terms of a family tree, we could say that the subclass is a descendant of the superclass (Note: remember that the terms superclass and subclass are not always the ones used by the languages themselves; C++ refers to them as base and derived classes, respectively).\nA subclass inherits both the data and methods from its superclass(es). However, as Sebesta says, \u0026ldquo;\u0026hellip; the features and capabilities of the [superclass] are not quite right for the new use.\u0026rdquo; Overriding methods allows the programmer to keep most of the functionality of the baseclass and customize the parts that are \u0026ldquo;not quite right.\u0026rdquo;\nAn overridden method is defined in a subclass and replaces the method with the same name (and usually protocol) in the parent.\nThe official documentation and tutorials for Java describe overriding in the language this way:\u0026ldquo;An instance method in a subclass with the same signature (name, plus the number and the type of its parameters) and return type as an instance method in the superclass overrides the superclass\u0026rsquo;s method.\u0026quot; The exact rules for overriding methods in Java are online at the language specification .\nLet\u0026rsquo;s make it concrete with an example:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } In this example, Car is the superclass of Tesla and Chevrolet. The Car class defines a method named ignite. That method will ignite the engine of the car \u0026ndash; an action whose mechanics differ based on the car\u0026rsquo;s type. In other words, this is a perfect candidate for overriding. Both Tesla and Chevrolet implement a method with the same name, return value and parameters, thereby meeting Java\u0026rsquo;s requirements for overriding. In Java, the @Override is known as an annotation. Annotations are \u0026ldquo;a form of metadata [that] provide data about a program that is not part of the program itself.\u0026quot; Annotations in Java are attached to particular syntactic units. In this case, the @Override annotation is attached to a method and it tells the compiler that the method is overriding a method from its superclass. If the compiler does not find a method in the superclass(es) that is capable of being overridden by the method, an error is generated. This is a good check for the programmer. (Note: C++ offers similar functionality through the override specifier (Links to an external site.).)\nLet\u0026rsquo;s say that the programmer actually implemented the Tesla class like this:\nclass Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite(int testing) { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } The ignite method implemented in Tesla does not override the ignite method from Car because it has a different set of parameters. The @Override annotation tells the compiler that the programmer thought they were overriding something. An error is generated and the programmer can make the appropriate fix. Without the @Override annotation, the code will compile but produce incorrect output when executed.\nAssume that the following program exists:\npublic class CarDemo { public static void main(String args[]) { Car c = new Car(); Car t = new Tesla(); Car v = new Chevrolet(); c.ignite(); t.ignite(); v.ignite(); } } This code instantiates three different cars \u0026ndash; the first is a generic Car, the second is a Tesla and the third is a Chevrolet. Look carefully and note that the type of each of the three is actually stored in a variable whose type is Car and not a more-specific type (ie, Tesla or Chevy). This is not a problem because of dynamic dispatch. At runtime, the JVM will find the proper ignite function and invoke it according to the variable\u0026rsquo;s actual type and not its static type. Because ignite is overridden by Chevy and Tesla, the output of the program above is:\nIgniting a generic car\u0026#39;s engine! Igniting a Tesla\u0026#39;s engine! Igniting a Chevrolet\u0026#39;s engine! Most OOP languages provide the programmer the option to invoke the method they are overriding from the superclass. Java is no different. If an overriding method implementation wants to invoke the functionality of the method that it is overriding, it can do so using the super keyword.\nclass Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } With these changes, the program now outputs:\nIgniting a generic car\u0026#39;s engine! Igniting a generic car\u0026#39;s engine! Igniting a Tesla\u0026#39;s engine! Igniting a generic car\u0026#39;s engine! Igniting a Chevrolet\u0026#39;s engine! New material alert: What if the programmer does not want a subclass to be able to customize the behavior of a certain method? For example, no matter how you subclass Dog, it\u0026rsquo;s noise method is always going to bark \u0026ndash; no inheriting class should change that. Java provides the final keyword to guarantee that the implementation of a method cannot be overridden by a subclass. Let\u0026rsquo;s change the code for the classes from above to look like this:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } void start() { System.out.println(\u0026#34;Starting a car ...\u0026#34;); if (this.ignite()) { System.out.println(\u0026#34;Ignited the engine!\u0026#34;); } else { System.out.println(\u0026#34;Did NOT ignite the engine!\u0026#34;); } } final boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } Notice that ignite in the Car class has a final before the return type. This makes ignite a final method : \u0026ldquo;A method can be declared final to prevent subclasses from overriding or hiding it\u0026rdquo;. (C++ has something similar \u0026ndash; the final specifier .) Attempting to compile the code above produces this output:\nCarDemo.java:30: error: ignite() in Tesla cannot override ignite() in Car boolean ignite() { ^ overridden method is final CarDemo.java:43: error: ignite() in Chevrolet cannot override ignite() in Car boolean ignite() { ^ overridden method is final 2 errors Subclass vs Subtype In OOP there is fascinating distinction between subclasses and subtypes. All those classes that inherit from other classes are considered subclasses. However, they are not all subtypes. For a type/class S to be a subtype of type/class T, the following must hold\nAssume that ϕ(t) is some provable property that is true of t, an object of type T. Then ϕ(s)\nmust be true as well for s, an object of type S.\nThis formal definition can be phrased simply in terms of behaviors: If it is possible to pass objects of type T as arguments to a function that expects objects of type S without any change in the behavior, then S is a subtype of T. In other words, a subtype behaves exactly like the \u0026ldquo;supertype\u0026rdquo;.\nBarbara Liskov who pioneered the definition and study of subtypes put it this way (Links to an external site.): \u0026ldquo;If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2, then S is a subtype of T.\u0026rdquo;\nOpen Recursion Open recursion in an OO PL is a fancy term for the combination of a) functionality that gives the programmer the ability to refer to the current object from within a method (usually through a variable named this or self) and b) dynamic dispatch. . Thanks to open recursion, some method A of class C can call some method B of the same class. But wait, there\u0026rsquo;s more! (Links to an external site.) Continuing our example, in open recursion, if method B is overriden in class D (a subclass of C), then the overriden version of the method is invoked when called from method A on an object of type D even though method A is only implemented by class C. Wild! It is far easier to see this work in real life than talk about it abstractly. So, consider our cars again:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } void start() { System.out.println(\u0026#34;Starting a car ...\u0026#34;); if (this.ignite()) { System.out.println(\u0026#34;Ignited the engine!\u0026#34;); } else { System.out.println(\u0026#34;Did NOT ignite the engine!\u0026#34;); } } boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } The start method is only implemented in the Car class. At the time that it is compiled, the Car class has no awareness of any subclasses (ie, Tesla and Chevrolet). Let\u0026rsquo;s run this code and see what happens:\npublic class CarDemo { public static void main(String args[]) { Car c = new Car(); Car t = new Tesla(); Car v = new Chevrolet(); c.start(); t.start(); v.start(); } } Here\u0026rsquo;s the output:\nStarting a car ... Igniting a generic car\u0026#39;s engine! Ignited the engine! Starting a car ... Igniting a Tesla\u0026#39;s engine! Ignited the engine! Starting a car ... Igniting a Chevrolet\u0026#39;s engine! Did NOT ignite the engine! Wow! Even though the implementation of start is entirely within the Car class and the Car class knows nothing about the Tesla or Chevrolet subclasses, when the start method is invoked on object\u0026rsquo;s of those types, the call to this\u0026rsquo;s ignite method triggers the execution of code specific to the type of car!\nHow cool is that?\n10/1/2021 Original is here.\nWe made it into October!! Spooky, spooky!\nCorrections Like in real newspapers (Links to an external site.), we are going to start including Corrections in each edition! We want to make sure that our reporters adhere to the highest standards:\nThe JVM will insert an implicit call to the to-be-instantiated class' default constructor (i.e., the one with no parameters) if the the to-be-constructed (sub)class does not do so explicitly. We\u0026rsquo;ll make this clear with an example:\nclass Parent { Parent() { System.out.println(\u0026#34;I am in the Parent constructor.\u0026#34;); } Parent(int parameter) { System.out.println(\u0026#34;This version of the constructor is not called.\u0026#34;); } } class Child extends Parent { Child() { /* * No explicit call to super -- one is automatically * injected to the parent constructor with no parameters. */ System.out.println(\u0026#34;I am in the Child constructor.\u0026#34;); } } public class DefaultConstructor { public static void main(String args[]) { Child c = new Child(); } } When this program is executed, it will print\nI am in the Parent constructor. I am in the Child constructor. The main function is instantiating an object of the type Child. We can visually inspect that there is no explicit call the super() from within the Child class' constructor. Therefore, the JVM will insert an implicit call to super() which actually invokes Parent().\nHowever, if we make the following change:\nclass Parent { Parent() { System.out.println(\u0026#34;I am in the Parent constructor.\u0026#34;); } Parent(int parameter) { System.out.println(\u0026#34;This version of the constructor is not called.\u0026#34;); } } class Child extends Parent { Child() { /* * No explicit call to super -- one is automatically * injected to the parent constructor with no parameters. */ super(1); System.out.println(\u0026#34;I am in the Child constructor.\u0026#34;); } } public class DefaultConstructor { public static void main(String args[]) { Child c = new Child(); } } Something different happens. We see that there is a call to Child\u0026rsquo;s superclass' constructor (the one that takes a single int-typed parameter). That means that the JVM will not insert an implicit call to super() and we will get the following output:\nThis version of the constructor is not called. I am in the Child constructor.\nThe C++ standard sanctions a main function without a return statement. The standard says: \u0026ldquo;if control reaches the end of main without encountering a return statement, the effect is that of executing return 0;.\u0026rdquo;\nA Different Way to OOP So far we have talked about OOP in the context of Java. Java, and languages like it, are called Class-based OOP languages. In a Class-based OOP, classes and objects exist in different worlds. Classes are used to define/declare\n the attributes and methods of an encapsulation, and the relationships between them.  From these classes, objects are instantiated that contain those attributes and methods and respect the defined/declared hierarchy. We can see this in the example given above: The classes Parent and Child define (no) attributes and (no) methods and define the relationship between them. In main(), a Child is instantiated and stored in the variable c. c is an object of type Child that contains all the data associated with a Child and a Parent and can perform all the actions of a Child and a Parent.\nNothing about Class-based OOP should be different than what you\u0026rsquo;ve learned in the past as you\u0026rsquo;ve worked with C++. There are several problems with Class-based OOP.\n The supported attributes and method of each class must be determined before the application is developed (once the code is compiled and the system is running, an object cannot add, remove or modify its own methods or attributes); The inheritance hierarchy between classes must be determined before the application is developed (once the code is compiled, changing the relationship between classes will require that the application be recompiled!).  In other words, Class-based OOP does not allow the structure of the Classes (nor their relationships) to easily evolve with the implementation of a system.\nThere is another way, though. It\u0026rsquo;s called Prototypal OOP. The most commonly known languages that use Prototypal OOP are JavaScript and Ruby! In Prototypal (which is a very hard word to spell!) OOP there is no distinction between Class and object \u0026ndash; everything is an object! In a Prototypal OOP there is a base object that has no methods or data attributes and every object is able to modify itself (its attributes and methods). To build a new object, the programmer simply copies from an existing object, the new object\u0026rsquo;s so-called prototype, and customizes the copied object appropriately.\nFor example, assume that there is an object called Car that has one attribute (the number of wheels) and one method (start). That object can serve as the prototype car. To \u0026ldquo;instantiate\u0026rdquo; a new Car, the programmer simply copies the existing prototypical car object Car and gives it a name, say, c. The programmer can change the value of c\u0026rsquo;s number of wheels and invoke its method, start. Let\u0026rsquo;s say that the same programmer wants to create something akin to a subclass of Car. The programmer would create a new, completely fresh object (one that has no methods or attributes), name it, say, Tesla, and link the new prototype Tesla object to the existing prototype car Car object through the prototype Tesla object\u0026rsquo;s prototype link (the sequence of links that connects prototype objects to one another is called a prototype chain). If a Tesla has attributes (range, etc) or methods (self_drive) that the prototype car does not, then the programmer would install those methods on the prototype Tesla Tesla. Finally, the programmer would \u0026ldquo;declare\u0026rdquo; that the Tesla object is a prototype Tesla.\n The blue arrows in the diagram above are prototype links. The orange lines indicate where a copy is made.\nHow does inheritance work in such a model? Well, it\u0026rsquo;s actually pretty straightforward: When a method is invoked or an attribute is read/assigned, the runtime will search the prototype chain for the first prototypical object that has such a method or attribute. Mic drop. In the diagram above, let\u0026rsquo;s follow how this would play out when the programmer calls start() on the Model 3 Instance. The Model 3 Instance does not contain a method named start. So, up we go! The Tesla Prototype Object does not contain that me either. All the way up! The Car Prototype Object, does, however, so that method is executed!\nWhat would it look like to override a function? Again, relatively straightforward. If a Tesla performs different behavior than a normal Car when it starts, the programmer creating the Tesla Prototype Object would just add a method to that object with the name start. Then, when the prototype chain is traversed by the runtime looking for the method, it will stop at the start method defined in the Tesla Prototype Object instead of continuing on to the start method in the Car Prototype Object. (The same is true of attributes!)\nThere is (at least) one really powerful feature of this model. Keep in mind that the prototype objects are real things that can be manipulated at runtime (unlike classes which do not really exist after compilation) and prototype objects are linked together to achieve a type of inheritance. With reference to the diagram above, say the programmer changes the definition of the start method on the Car Prototype Object. With only that change, any object whose prototype chain includes the Car Prototype Object will immediately have that new functionality (where it is not otherwise overridden, obviously) \u0026ndash; all without stopping the system!! How cool is that?\nHow scary is that? Can you imagine working on a system where certain methods you \u0026ldquo;inherit\u0026rdquo; change at runtime?\n OOP or Interfaces? Newer languages (e.g., Go, Rust, (new versions of) Java) are experimenting with new features that support one of the \u0026ldquo;killer apps\u0026rdquo; of OOP: The ability to define a function that takes a parameter of type A but that works just the same as long as it is called with an argument whose type is a subtype of A. The function doesn\u0026rsquo;t have care whether it is called with an argument whose type is A or some subtype of A because the language\u0026rsquo;s OOP semantics guarantee that anything the programmer can do with an object of type A, the programmer can do with and object of subtype of A.\nUnfortunately, using OOP to accomplish such a feat may be like killing a fly with a bazooka (or a laptop, like Alex killed that wasp today).\nInstead, modern languages are using a slimmer mechanism known as an interface or a trait. An interface just defines a list of methods that an implementer of that interface must support. Let\u0026rsquo;s see some real Go code that does this \u0026ndash; it\u0026rsquo;ll clear things up:\ntype Readable interface { Read() } This snippet defines an interface with one function (Read) that takes no parameters and returns no value. That interface is named Readable. Simple.\ntype Book struct { title string } This snippet defines a data structure called a Book \u0026ndash; such structs are the closest that Go has to classes.\nfunc (book Book) Read() { fmt.Printf(\u0026#34;Reading the book %v\\n\u0026#34;, book.title) } This snippet simply says that if variable b is of type Book then the programmer can call b.Read(). Now, for the payoff:\nfunc WhatAreYouReading(r Readable) { r.Read() } This function only accepts arguments that implement (i.e., meet the criteria specified in the definition of) the Readable interface. In other words, with this definition, the code in the body of the function can safely assume that it can can call Read on r. And, for the encore:\nbook := Book{title: \u0026#34;Infinite Jest\u0026#34;} WhatAreYouReading(book) This code works exactly like you\u0026rsquo;d expect. book is a valid argument to WhatAreYouReading because it implements the Read method which, implicitly, means that it implements the Readable interface. But, what\u0026rsquo;s really cool is that the programmer never had to say explicitly that Book implements the Readable interface! The compiler checks automatically. This gives the programmer the ability to generate a list of only the methods absolutely necessary for its parameters to implement to achieve the necessary ends \u0026ndash; and nothing unnecessary. Further, it decouples the person implementing a function from the person using the function \u0026ndash; those two parties do not have to coordinate requirements beforehand. Finally, this functionality means that a structure can implement as few or as many interfaces as its designer wants.\nDip Our Toe Into the Pool of Pointers We only had a few minutes to start pointers, but we did make some headway. There will be more on this in the next lecture!\nIt is important to remember that pointers are like any other type \u0026ndash; they have a range of valid values and a set of valid operations that you can perform on those values. What are the range of valid values for a pointer? All valid memory addresses. And what are the valid operations? Addition, subtraction, dereference and assignment.\n In the diagram, the gray area is the memory of the computer. The blue box is a pointer. It points to the gold area of memory. It is important to remember that pointers and their targets both exist in memory! In fact, in true Inception (Links to an external site.)style, a pointer can pointer to a pointer!\nAt the same time that pointers are types, they also have types. The type of a pointer includes the type of the target object. In other words, if the memory in the gold box held an object of type T, the the green box\u0026rsquo;s type would be \u0026ldquo;pointer to type T.\u0026rdquo; If the programmer dereferences the blue pointer, they will get access to the object in memory in the gold.\nIn an ideal scenario, it would always be the case that the type of the pointer and the type of the object at the target of the pointer are the same. However, that\u0026rsquo;s not always the case. Come to the next lecture to see what can go wrong when that simple fact fails to hold!\n10/4/2021 Original is here One day closer to Candy Corn!\nCorrections When we were discussing the nature of the type of pointers, we specified that the range of valid values for a pointer are all memory addresses. In some languages this may be true. However, some other languages specify that the range of valid values for a pointer are all memory addresses and a special null value that explicitly specifies a pointer does not point to a target.\nWe also discussed the operations that you can perform on a pointer-type variable. What we omitted was a discussion of an operation that will fetch the address of a variable in memory. For languages that use pointers to support indirect addressing (see below), such an operation is required. In C/C++, this operation is performed using the address of (\u0026amp;) operator.\nPointers We continued the discussion of pointers that we started on Friday! On Friday we discussed that pointers are just like any other type \u0026ndash; they have valid values and defined operations that the programmer can perform on those values.\nThe Pros of Pointers Though a very famous and influential computer scientist (Links to an external site.) once called his invention of null references a \u0026ldquo;billion dollar mistake\u0026rdquo; (he low balled it, I think!), the presence and power of pointers in a language is important for at least two reasons:\n Without pointers, the programmer could not utilize the power of indirection. Pointers give the programmer the power to address and manage heap-dynamic memory.  Indirection gives the programmer the power to link between different objects in memory \u0026ndash; something that makes writing certain data structures (like trees, graphs, linked lists, etc) easier. Management of heap-dynamic memory gives the programmer the ability to allocate, manipulate and deallocate memory at runtime. Without this power, the programmer would have to know before execution the amount of memory their program will require.\nThe Cons of Pointers Their use as a means of indirection and managing heap-dynamic memory are powerful, but misusing either can cause serious problems.\n   Possible Problems when Using Pointers for Indirection\nAs we said in the last lecture, as long as a pointer targets memory that contains the expected type of object, everything is a-okay. Problems arise, however, when the target of the pointer is an area in memory that does not contain an object of the expected type (including garbage) and/or the pointer targets an area of memory that is inaccessible to the program.\nThe former problem can arise when code in a program writes to areas of memory beyond their control (this behavior is usually an error, but is very common). It can also arise because of a use after free. As the name implies, a use-after-free error occurs when a program uses memory after it has been freed. There are two common scenarios that give rise to a use after free:\n Scenario 1:  One part of a program (part A) frees an area of memory that held a variable of type T that it no longer needs Another part of the program (part B) has a pointer to that very memory A third part of the program (part C) overwrites that \u0026ldquo;freed\u0026rdquo; area of memory with a variable of type S Part B accesses the memory assuming that it still holds a variable of Type T   Scenario 2:  One part of a program (part A) frees an area of memory that held a variable of type T that it no longer needs Part A never nullifies the pointer it used to point to that area of memory though the pointer is now invalid because the program has released the space A second part of the program (part C) overwrites that \u0026ldquo;freed\u0026rdquo; area of memory with a variable of type S Part A incorrectly accesses the memory using the invalid pointer assuming that it still holds a variable of Type T    Scenario 2 is depicted visually in the following scenario and intimates why use-after-free errors are considered security vulnerabilities:\n In the example shown visually above, the program\u0026rsquo;s use of the invalid pointer means that the user of the invalid pointer can now access an object that is at a higher privilege level (Restricted vs Regular) than the programmer intended. When the programmer calls a function through the invalid pointer they expect that a method on the Regular object will be called. Unfortunately, a method on the Restricted object will be called instead. Trouble!\nThe latter problem occurs when a pointer targets memory beyond the program\u0026rsquo;s control. This most often occurs when the program sets a variable\u0026rsquo;s address to 0 (NULL, null, nil) to indicate that it is invalid but later uses that pointer without checking its validity. For compiled languages this often results in the dreaded segmentation fault and for interpreted languages it often results in other anomalous behavior (like Java\u0026rsquo;s Null Pointer Exception (NPE)). Neither are good!\n     Possible Solutions\nWouldn\u0026rsquo;t it be nice if we had a way to make sure that the pointer being dereferenced is valid so we fall victim to some of the aforementioned problems? What would be the requirements of such a solution?\n Pointers to areas of memory that have been deallocated cannot be dereferenced. The type of the object at the target of a pointer always matches the programmer\u0026rsquo;s expectation.  Your author describes two potential ways of doing this. First, are tombstones. Tombstones are essentially an intermediary between a pointer and its target. When the programming language implements pointers and uses tombstones for protection, a new tombstone is allocated for each pointer the programmer generates. The programmer\u0026rsquo;s pointer targets the tombstone and the tombstone targets the pointer\u0026rsquo;s actual target. The tombstone also contains an extra bit of information: whether it is valid. When the programmer first instantiates a pointer to some target a the compiler/interpreter\n generates a tombstone whose target is a sets the valid bit of the tombstone to valid points the programmer\u0026rsquo;s pointer to the tombstone.  When the programmer dereferences their pointer, the compiler/runtime will check to make sure that the target tombstone\u0026rsquo;s valid flag is set to valid before doing the actual dereference of the ultimate target. When the programmer \u0026ldquo;destroys\u0026rdquo; the pointer (by releasing the memory at its target or by some other means), the compiler/runtime will set the target tombstone\u0026rsquo;s valid flag to invalid. As a result, if the programmer later attempts to dereference the pointer after it was destroyed, the compiler/runtime will see that the tombstone\u0026rsquo;s valid flag is invalid and generate an appropriate error.\nThis process is depicted visually in the following diagram.\nTombstones.png\nThis seems like a great solution! Unfortunately, there are downsides. In order for the tombstone to provide protection for the entirety of the program\u0026rsquo;s execution, once a tombstone has been allocated it cannot be reclaimed. It must remain in place forever because it is always possible that the programmer can incorrectly reuse an invalid pointer. As soon as the tombstone is deallocated, the protection that it provides is gone. The other problem is that the use of tombstones adds an additional layer of indirection to dereference a pointer and every indirection causes memory accesses. Though memory access times are small, they are not zero \u0026ndash; the cost of these additional memory accesses add up.\nWhat about a solution that does not require an additional level of indirection? There is a so-called lock-and-key technique. This protection method requires that the pointer hold an additional piece of information beyond the address of the target: the key. The memory at the target of the pointer is also required to hold a key. When the system allocates memory it sets the keys of the pointer and the target to be the same value. When the programmer dereferences a pointer, the two keys are compared and the operation is only allowed to continue if the keys are the same. The process is depicted visually below.\n With this technique, there is no additional memory access \u0026ndash; that\u0026rsquo;s good! However, there are still downsides. First, there is a speed cost. For every dereference there must be a check of the equality of the keys. Depending on the length of the key that can take a significant amount of time. Second, there is a space cost. Every pointer and block of allocated memory now must have enough space to store the key. For systems where memory allocations are done in big chunks, the relative size overhead of storing, say, and 8byte key is not significant. However, if the system allocates many small areas of memory, the relative size overhead is tremendous. Moreover, the more heavily the system relies on pointers the more space will be used to store keys rather than meaningful data.\nWell, let\u0026rsquo;s just make the keys smaller? Great idea. There\u0026rsquo;s only one problem: The smaller the keys the fewer unique key values. Fewer unique key values mean that it is more likely an invalid pointer randomly points to a chunk of memory with a matching key. In this scenario, the protection afforded by the scheme is vitiated. (I just wanted to type that word \u0026ndash; I\u0026rsquo;m not even sure I am using it correctly!)\n  10/6/2021 Original is here.\nI love Reese\u0026rsquo;s Pieces.\nCorrections None to speak of!!\nPointers for Dynamic Memory Management We finished up our discussion of pointers in today\u0026rsquo;s class. In the previous class, we talked about how pointers have two important roles in programming languages:\n indirection \u0026ndash; referring to other objects dynamic memory management \u0026ndash; \u0026ldquo;handles\u0026rdquo; for areas of memory that are dynamically allocated and deallocated by the system.  On Monday we focused on the role of pointers in indirection and how to solve some of the problems that can arise from using pointers in that capacity. In today\u0026rsquo;s class, we focused on the role of pointers in dynamic memory management.\nAs tools for dynamic memory management, the programmer can use pointers to target blocks (N.B.: I am using blocks as a generic term for memory and am not using it in the sense of a block [a.k.a. page] as defined in the context of operating systems) of dynamic memory that are allocated and deallocated by the operating system for use by an application. The programmer can use these pointers to manipulate what is stored in those blocks and, ultimately, release them back to the operating system when they are no longer needed.\nMemory in the system is a finite resource. If a program repeatedly asks for memory from the system without releasing previous allocations back to the system, there will come a time when the memory is exhausted. In order to be able to release existing allocations back to the operating system for reuse by other applications, the programmer must not lose track of those existing allocations. When there is a memory allocation from the operating system to the application that can no longer be reached by a pointer in the application, that memory allocation is leaked. Because the application no longer has a pointer to it, there is no way for the application to release it back to the system. Leaked memory belongs to the leaking application until it terminates.\nFor some programs this is fine. Some applications run for a short, defined period of time. However, there are other programs (especially servers) that are written specifically to operate for extended periods of time. If such applications leak memory, they run the risk of exhausting the system\u0026rsquo;s memory resources and failing (Links to an external site.).\nPreventing Memory Leaks System behavior will be constrained when those systems are written in languages that do not support using pointers for dynamic memory management. However, what we learned (above) is that it is not always easy to use pointers for dynamic memory management correctly. What are some of the tools that programming languages provide to help the programmer manage pointers in their role as managers of dynamic memory.\n   Reference Counting\nIn a reference-counted memory management system, each allocated block of memory given to the application by the system contains a reference count. That reference count, well, counts the number of references to the object. In other words, for every pointer to an operating-system allocated block of memory, the reference count on that block increases. Every time that a pointer\u0026rsquo;s target is changed, the programming language updates the reference counts of the old target (decrement) and the new target (increment), if there is a new target (the pointer could be changed to null, in which case there is no new target). When a block\u0026rsquo;s reference count reaches zero, the language knows that the block is no longer needed, and automatically returns it to the system! Pretty cool.\n The scenario depicted visually shows the reference counting process. At time (a), the programmer allocates a block of memory dynamically from the operating system and puts an application object in that block. Assume that the application object is a node in a linked list. The first node is the head of the list. Because the programmer has a pointer that targets that allocation, the block\u0026rsquo;s reference count at time (a) is 1. At time (b), the programmer allocates a second block of memory dynamically from the system and puts a second application object in that block \u0026ndash; another node in the linked list (the tail of the list). Because the head of the list is referencing the tail of the list, the reference count of the tail is 1. At time (c) the programmer deletes their pointer (or reassigns it to a different target) to the head of the linked list. The programming language decrements the reference count of the block of memory holding the head node and deallocates it because the reference count has dropped to 0. Transitively, the pointer from the head application object to the tail application object is deleted and the programming language decrements the reference count of its target, the block of memory holding the tail application object (time (d)). The reference count of the block of memory holding the tail application object is now 0 and so the programming language automatically deallocates the associated storage (time (e)). Voila \u0026ndash; an automatic way to handle dynamic memory management.\nThere\u0026rsquo;s only one problem. What if the programmer wants to implement a circularly linked list?\n Because the tail node points to the head node, and the head node points to the tail node, even after the programmer\u0026rsquo;s pointer to the head node is deleted or retargeted, the reference counts of the two nodes will never drop to 0. In other words, even with reference-counted automatic memory management, there could still be a memory leak! Although there are algorithms to break these cycles, it\u0026rsquo;s important to remember that reference counting is not a panacea. Python is a language that manages memory using reference counting.\n  Garbage Collection Garbage collection (GC) is another method of automatically managing dynamically allocated memory. In a GC\u0026rsquo;d system, when a programmer allocates memory to store an object and no space is available, the programming language will stop the execution of the program (a so-called GC pause) to calculate which previously allocated memory blocks are no longer in use and can be returned to the system. Having freed up space as a result of cleaning up unused garbage, the allocation requested by the programmer can be satisfied and the execution of the program can continue.\nThe most efficient way to engineer a GC\u0026rsquo;d system is if the programming language allocates memory to the programmer in fixed-size cells. In this scenario, every allocation request from the programmer is satisfied by a block of memory from one of several banks of fixed-size blocks that are stacked back-to-back. For example, a programming language may manage three different banks \u0026ndash; one that holds reserves of X-sized blocks, one that holds reserves of Y-sized blocks and one that holds reserves of Z-sized blocks. When the programmer asks for memory to hold an object that is of size a, the programming language will deliver a block that is just big enough to that object. Because the size of the requested allocation may not be exactly the same size as one of the available fixed-size blocks, space may be wasted.\nThe fixed sizing of blocks in a GC\u0026rsquo;d system makes it easy/fast to walk through every block of memory. We will see shortly that the GC algorithm requires such an operation every time that it stops the program to do a cleanup. Without a consistent size, traversing the memory blocks would require that each block hold a tag indicating its size \u0026ndash; a waste of space and the cause of an additional memory read \u0026ndash; so that the algorithm could dynamically calculate the starting address of the next block.\nWhen the programmer requests an allocation that cannot be satisfied, the programming language stops the execution of the program and does a garbage collection. The classic GC algorithm is called mark and sweep and has three steps:\nEvery block of memory is marked as free using a free bit attached to the block. Of course, this is only true of some of the blocks, but the GC is optimistic! All pointers active at the time the program is paused are traced to their targets. The free bits of those blocks are reset. The blocks that are marked free and released.\nThe process is shown visually below:\n At times (a), (b) and (c), the programmer is allocating and manipulating references to dynamically allocated memory. At time (c), the allocation request for variable z cannot be satisfied because there are no available blocks. A GC pause starts at time (d) and the mark-and-sweep algorithm commences by setting the free bit of every block. At time (e) the pointers are traced and the appropriate free bits are cleared. At time (f) the memory is released from the unused block and its free bit, too, is reset. At time (g) the allocation for variable z can be satisfied, the GC pause completes and the programming language restarts execution of the program.\nThis process seems great, just like reference counting seemed great. However, there is a significant problem: The programmer cannot predict when GC pauses will occur and the programmer cannot predict how long those pauses will take. A GC pause is completely initiated by the programming language and (usually) completely beyond the control of the programmer. Such random pauses of program execution could be extremely harmful to a system that is controlling a system that needs to keep up with interactions from the outside world. For instance, it would be totally unacceptable for an autopilot system to take an extremely long GC pause as it calculates the heading needed to land a plane. There are myriad other systems where pauses are inappropriate.\nThe mark-and-sweep algorithm described above is extremely naive and GC algorithms are the subject of intense research. Languages like go and Java manage memory with a GC and their algorithms are incredibly sophisticated. If you want to know more, please let me know!\n10/15/2021 The hunt for October!\nCorrections None to speak of!!\nIntroduction to Functional Programming We spent Friday beginning our module on Functional Programming (FP)! As we said at the beginning of the semester when we were learning about programming paradigms, FP is very different than imperative programming. In imperative programming, developers tell the computer how to do the operation. While functional programming is not logic programming (where developers just tell the computer what to compute and leave the how entirely to the language implementation), the writer of a program in a functional PL is much more concerned with specifying what to compute than how to compute it.\n Four Characteristics of Functional Programming There are four characteristics that epitomize FP:\n There is no state Functions are central  Functions can be parameters to other functions Functions can be return values from other others Program execution is function evaluation   Control flow is performed by recursion and conditional expressions Lists are a fundamental data type  In a functional programming language, there are no variables, per se. And because there are no variables, there is no state. That does not mean there are no names. Names are still important. It simply means that names refer to expressions themselves and not their values. The distinction will become more obvious as we continue to learn more about writing programs in functional languages.\nBecause there is no state, a functional programming language is not history sensitive. A language that is history sensitive means that results of operations in that language can be affected by operations that have come before it. For example, in an imperative programming language, a function may be history sensitive if it relies on the value of a global variable to calculate its return value. Why does that count as history sensitive? Because the value in the global variable could be affected by prior operations.\nA language that is not history sensitive has referential transparency. We learned the definition of referential transparency before, but now it might make a little more sense. In a language that has referential transparency, a the same function called with the same arguments generates the same result no matter what operations have preceded it.\nIn a functional programming language there are no loops (unless they are added as syntactic sugar) \u0026ndash; recursion is the way to accomplish repetition. Selective execution (as opposed to sequential execution) is accomplished using the conditional expression. A conditional expression is, well, an expression that evaluates to one of two values depending on the value of a condition. We have seen conditional expressions in STIMPL. That a conditional statement can have a value (thus making it a conditional expression) is relatively surprising for people who only have experience in imperative programming languages. Nevertheless, the conditional expressions is a very, very sharp sword in the sheath of the functional programmer.\nA functional program is a series of functions and the execution of a functional program is simply an evaluation of those functions. That sounds abstract at this point, but will become more clear when we see some real functional programs.\nLists are a fundamental data type in functional programming languages. Powerful syntactic tools for manipulating lists are built in to most functional PLs. Understanding how to wield these tools effectively is vital for writing code in functional PLs.\nThe Historical Setting of the Development of Functional PLs The first functional programming language was developed in the mid-1950s by John McCarthy . At the time, computing was most associated with mathematical calculations. McCarthy was instead focused on artificial intelligence which involved symbolic computing. Computer scientists thought that it was possible to represent cognitive processes as lists of symbols. A language that made it possible to process those lists would allow developers to build systems that work like our brains.\n McCarthy started with the goal of writing a system of meta notation that programmers could attach to Fortran. These meta notations would be reduced to actual Fortran programs. As they did their work, they found their way to a program representation built entirely of lists (and lists of lists, and lists of lists of lists, etc). Their thinking resulted in the development of Lisp, a list processing language. In Lisp, data are lists and programs are lists. They showed that list processing, the basis of the semantics of Lisp, is capable of universal computing. In other words, Lisp, and other list processing languages, is/are Turing complete.\nThe inability to execute a Lisp program efficiently on a physical computer based on the von Neumann model has given Lisp (and other functional programming languages) a reputation as slow and wasteful. (N.B.: This is not true today!) Until the late 1980s hardware vendors thought that it would be worthwhile to build physical machines with non-von Neumann architectures that made executing Lisp programs faster. Here is an image of a so-called Lisp Machine.\n LISP We will not study Lisp in this course. However, there are a few aspects of Lisp that you should know because they pervade the general field of computer science.\nFirst, you should know CAR, CDR and CONS \u0026ndash; pronounced car, could-er, and cahns, respectively. CAR is a function that takes a list as a parameter and returns the first element of the list. CDR is a function that takes a list as a parameter and returns the tail, everything but the head, of the list. CONS takes two parameters \u0026ndash; a single element and a list \u0026ndash; and returns a new list with the first argument appended to the front of the second argument.\nFor instance,\n(car (1 2 3)) is 1.\n(cdr (1 2 3)) is (2 3).\nSecond, you should know that, in Lisp, all data are lists and programs are lists.\n(a b c) is a list in Lisp. In Lisp, (a b c) could be interpreted as a list of atoms a, b and c or an invocation of function a with parameters b and c.\nLambda Calculus Lambda Calculus is the theoretical basis of functional programming languages in the same way that the Turing Machine is the theoretical basis of the imperative programming languages. The Lambda Calculus is nothing like \u0026ldquo;calculus\u0026rdquo; \u0026ndash; the word calculus is used here in its strict sense: a method or system of calculation . It is better to think of Lambda Calculus as a programming language rather than a branch of mathematics.\nLambda Calculus is a model of computation defined entirely by function application. The Lambda Calculus is as powerful as a Turning Machine which means that anything computable can be computed in the Lambda Calculus. For a language as simple as the Lambda Calculus, that\u0026rsquo;s remarkable!\nThe entirety of the Lambda Calculus is made up of three entities:\n Expression: a name, a function or an application Function: \\(\\lambda\\) .  Application:    Notice how the elements of the Lambda Calculus are defined in terms of themselves. In most cases it is possible to restrict names in the Lambda Calculus to be any single letter of the alphabet \u0026ndash; a is a name, z is a name, etc. Strictly speaking, functions in the Lambda Calculus are anonymous \u0026ndash; in other words they have no name. The name after the\nin a function in the Lambda Calculus can be thought of as the parameter of the function. Here\u0026rsquo;s an example of a function in the Lambda Calculus:\n\\(\\lambda\\)x . x\nLambda Calculiticians (yes, I just made up that term) refer to this as the identity function. This function simply returns the value of its argument! But didn\u0026rsquo;t I say that functions in the Lambda Calculus don\u0026rsquo;t have names? Yes, I did. Within the language there is no way to name a function. That does not mean that we cannot assign semantic values to those functions. In fact, associating meaning with functions of a certain format is exactly how high-level computing is done with the Lambda Calculus.\n10/18/2021 Lambda lower now. How low can you go?\nCorrections None to speak of!!\n(Recalling) Lambda Calculus Remember that we said Lambda Calculus is the theoretical basis of functional programming languages in the same way that the Turing Machine is the theoretical basis of the imperative programming languages. Again, don\u0026rsquo;t freak out when you hear the phrase \u0026ldquo;calculus\u0026rdquo;. As we said in class, it is better to think of the Lambda Calculus as a programming language rather than a branch of mathematics.\nLambda Calculus is a model of computation defined entirely by function application. The Lambda Calculus is as powerful as a Turning Machine which means that anything computable can be computed in the Lambda Calculus. For a language as simple as the Lambda Calculus, that\u0026rsquo;s remarkable!\nRemember that the entirety of the Lambda Calculus is made up of a small number of entities:\n Expression: a name, a function or an application Function: \\(\\lambda\\) .  Application:    We made the point in class that, without loss of generality, we will assume that all names are single letters from the alphabet. In other words, if you see two consecutive letters, e.g., ab, those are two separate names.\nBound and Free Names and the Tao of Function Application Because the entirety of the Lambda Calculus is function application, it is important that we get it exactly right. Let\u0026rsquo;s recall the simplest example of function application: \\((\\lambda a. a)x = \\left \\lfloor x/a \\right \\rfloor a = x\\) The \\(\\lfloor x/a \\rfloor a\\) means \u0026ldquo;replace all instances of a with x in whatever comes after the \\(\\lfloor \\rfloor\\) \u0026ldquo;. This is so easy. What about this, though?\n\\((\\lambda a. \\lambda b. ba)b\\)\nThe first thing to realize is that the b in the expression that is the body of the nested lambda function is completely separate from the b to which the lambda function is being applied. Why is that? Because the b in the nested lambda function is the \u0026ldquo;parameter\u0026rdquo; to that function. So, what are we to do?\nFirst, let\u0026rsquo;s step back and consider the definitions of free and bound names. Loosely speaking, a name is bound as soon as it is used as a parameter to a lambda function. It continues to be bound in nested expressions but may be rebound! For example,\n\\(\\lambda x. x \\lambda x. x\\)\nThe \u0026ldquo;body\u0026rdquo; of the outer function is \\(x \\lambda x . x\\) and the leftmost x is the x from the outer function\u0026rsquo;s parameter. In other words,\n\\((\\lambda x.x \\lambda x.x) (a) = a \\lambda x.x\\)\nThe substitution of a for x continues no further because x is rebound at the start of the nested lambda function. You will be relieved to know that,\n\\(\\lambda x.x \\lambda x.x = \\lambda x.x \\lambda a.a\\)\nIn fact, renaming like that has a special name: alpha conversion!\nFree names are those that are not bound.\nWow, that got pretty complicated pretty quickly! This is one case where some formalism actually improves the clarity of things, I think. Here is the formal definition of what it means for a name to be bound:\n name is bound in \\(\\lambda name_1.expression\\) if name = name1 or name is bound in expression. name is bound in \\(E_1 E_2\\) if name is bound in either \\(E_1\\) or \\(E_2\\).  Here is the formal definition of what it means for a name to be free:\n name is free in name name is free in \\(\\lambda name_1.expression\\) when name \\(\\ne\\) name1 and name is free in expression name is free in \\(E_1E_2\\) if name is free in either E1 or E2  Note that a name can be free and bound at the same time.\nAll this hullabaloo means that we need to be slightly more sophisticated in our function application. We have to check two boxes before assuming that can treat function application as a simple textual search/replace:\nWhen applying \\(\\lambda x. E_1\\) to E2, we only replace the free instances of x in E1 with E2 and if E2 contains a free name that is bound in E1, we have to alpha convert that bound name in E1 to something that doesn\u0026rsquo;t conflict. There is a good example of this in Section 1.2 of the XXXX that I will recreate here:\n\\((\\lambda x. (\\lambda y . (x\\lambda x. xy)))y\\)\nFirst, note y (our E2 in this case) contains y (a free name) that is bound in \\((\\lambda y. (x \\lambda x.xy))\\)(our E1). In other words, before doing a straight substitution, we have to alpha convert the bound y in E1 to something that doesn\u0026rsquo;t conflict. Let\u0026rsquo;s choose t:\n\\((\\lambda x. (\\lambda t. (x \\lambda x.xt)))y\\)\nNow we can do our substitution! But, be careful: x appears free in \\((\\lambda y. (x \\lambda x.xy)\\) (again, our E1) one time \u0026ndash; its leftmost appearance! So, the substitution would yield:\n\\((\\lambda t. (y \\lambda x.xt)\\)\nVoila!\nCurrying Functions Currying is the process of turning a function that takes multiple parameters into a sequence of functions that each take a single parameter. Currying is only possible in languages that support high-order functions: functions that a) take functions as parameters, b) return functions or c) both. Python is such a language. Let\u0026rsquo;s look at how you would write a function that calculates the sum of three numbers in Python:\ndef sum3(a, b, c): return a + b + c That makes perfect sense!\nLet\u0026rsquo;s see if we can Curry that function. Because a Curried function can only take one parameter and we are Currying a function with three parameters, it stands to reason that we are going to have to generate three different functions. Let\u0026rsquo;s start with the first:\ndef sum1(a): # Something? What something are we going to do? Well, we are going to declare another function inside sum1, call it sum2, that takes a parameter and then use that as the return value of sum1! It looks something like this:\ndef sum1(a): def sum2(b): pass return sum2 That means, if we call sum1 with a single parameter, the result is another function, one that takes a single parameter! So, we\u0026rsquo;ve knocked off two of the three parameters, now we need one more. So, let\u0026rsquo;s write something like this:\ndef sum1(a): def sum2(b): def sum3(c): pass return sum3 return sum2 This means that if we call sum1 with a single parameter and call the result of that with a single parameter, the result is another function, one that also takes a single parameter! What do we want that innermost function to do? That\u0026rsquo;s right: the summation! So, here\u0026rsquo;s our final code:\ndef sum1(a): def sum2(b): def sum3(c): return a + b + c return sum3 return sum2 We\u0026rsquo;ve successfully Curried a three-parameter summation function! There\u0026rsquo;s just one issue left to address? How can we possibly use a and b in the innermost function? Will, I thought you told us that Python was statically scoped! In order for this to work correctly, wouldn\u0026rsquo;t Python have to have something magical and dynamic-scope-like? Well, yes! And, it does. It has closures.\nWhen you return sum2 from the body of sum1, Python closes around the variables that are needed by any code in the implementation of the returned function. Because a is needed in the implementation of sum2 (the function returned by sum1), Python creates a closure around that function which includes the value of a at the time sum2 was returned. It is important to remember that every time sum2 is defined pursuant to an invocation of sum1, a new version of sum2 is returned with a new closure. This closure-creation process repeats when we return sum3 pursuant to an invocation of sum2 (which itself was generated as a result of an invocation of sum1)! Whew.\nBecause we Curried the sum3 function as sum1, we have to call them slightly differently:\nsum3(1, 2, 3) sum1(1)(2)(3) As we learn more about functional programming in Haskell, you will see this pattern more and more and it will become second nature.\nThe \u0026ldquo;good\u0026rdquo; news, if you can call it that, is that functions in the Lambda Calculus always exist in their Curried form. Prove it to yourself by looking back at the way we formally defined the Lambda Calculus.\nBut, because it is laborious to write all those \\(\\lambda\\)s over and over, we will introduce a shorthand for functions in the Lambda Calculus that take more than one parameter:\n\\(\\lambda p_1p_2 \u0026hellip; p_n.expression\\)\nis a function with n parameters named p1 through pn (which are each one letter). Simply put,\n\\(\\lambda x . \\lambda y.xy = \\lambda xy.xy\\)\nfor example.\nDoing Something with Lambda Calculus Remember how we have stressed that you cannot name functions inside the Lambda Calculus but how I have stressed that does not mean we cannot give names to functions from outside the Lambda Calculus? Well, here\u0026rsquo;s where it starts to pay off! We are going learn how to do boolean operations using the Lambda Calculus. Let\u0026rsquo;s assume that anytime we see a lambda function that takes two parameters and reduces to the first, we call that T. When we see a lambda function that takes two parameters and reduces to the second, we call that F:\n\\(T \\equiv \\lambda xy.x\\)\n\\(F \\equiv \\lambda xy.y\\)\nTo reiterate, it is the form that matters. If we see\n\\(\\lambda ab.a\\)\nthat is T too! In what follows, I will type T and F to save myself from writing all those \\(\\lambda\\)s, but remember: T and F are just functions!!\nOkay, let\u0026rsquo;s do something boolean operations. We can define the and operation as\n\\(\\wedge = \\lambda xy.xy F\\)\nLet\u0026rsquo;s give it a whirl. First, let\u0026rsquo;s get on the same page: True and False is False.\n\\(\\wedge TF = (\\lambda xy.xyF)TF = TFF = (\\lambda xy.x) FF = F\\)\nAwesome! Let\u0026rsquo;s try another: True and True is True.\n\\(\\wedge TT = (\\lambda xy.xyF)TT = TTF = (\\lambda xy.x) TF = T\\)\nWe can define the or operation as\n\\(\\lor = \\lambda xy.xTy\\)\nTry your hand at working through a few examples and make sure you get the expected results!\n10/22/2021 Corrections Thanks to Donald\u0026rsquo;s persistence, I researched the mechanism by which Haskell and other pure functional languages\n handle associations between names and expressions, and pass around infinite lists (without having to generate the entire list first \u0026ndash; an obvious impossibility)  thunks are covered below!\nFunction Invocation in Functional Programming Languages In imperative programming languages, it may matter to the correctness of a program the order in which parameters to a function are evaluated. (Note: For the purposes of this discussion we will assume that all operators [+, -, /, etc] are implemented as functions that take the operands as arguments in order to simplify the discussion. In other words, when describe the order of function evaluation we are also talking about the order of operand evaluation.) While the choice of the order in which we evaluate the operands is the language designer\u0026rsquo;s prerogative, the choice has consequences. Why? Because of side effects! For example:\n#include \u0026lt;stdio.h\u0026gt; int operation(int parameter) { static int divisor = 1; return parameter / (divisor++); } int main() { int result = operation(5) + operation(2); printf(\u0026#34;result: %d\\n\u0026#34;, result); return 0; } prints\nresult: 6 whereas\n#include \u0026lt;stdio.h\u0026gt; int operation(int parameter) { static int divisor = 1; return parameter / (divisor++); } int main() { int result = operation(2) + operation(5); printf(\u0026#34;result: %d\\n\u0026#34;, result); return 0; } prints\nresult: 4 In the difference between the two programs we see vividly the role that the static variable plays in the state of the program and its ultimate output.\nBecause of the referential transparency in pure functional programming languages, the designer of such a language does not need to worry about the consequences of the decision about the order of evaluation of arguments to functions. However, that does not mean that the language designer of a pure functional programming language does not have choices to make in this area.\nA very important choice the designer has to make is the time when function arguments are evaluated. There are two options available:\n All function arguments are evaluated before the function is evaluated. Function arguments are evaluated only when their results are needed.  Let\u0026rsquo;s look at an example: Assume that there are two functions: dbl, a function that doubles its input, and average, a function that averages its three parameters:\ndbl x = (+) x x average a b c = (/) ((+) a ((+) b c)) 3 Both functions are written using prefix notation (i.e., (  \u0026hellip; ). We will call these functions like this:\ndbl (average 3 4 5) If the language designer chooses to evaluate function arguments only when their results are needed, the execution of this function call proceeds as follows:\ndbl (average 3 4 5) + (average 3 4 5) (average 3 4 5) + ((/) ((+) 3 ((+) 4 5)) 3) (average 3 4 5) + (4) (average 3 4 5) + (4) ((/) ((+) 3 ((+) 4 5)) 3) + (4) (4) 8 The outermost function is always reduced (expand) before the inner functions. Note: Primitive functions (+, and / in this example) cannot be expanded further so we move inward in evaluation if we encounter such a function for reduction.\nIf, however, the language designer chooses to evaluate function arguments before the function is evaluated, the execution of the function call proceeds as follows:\ndbl (average 3 4 5) dbl ((/) ((+) 3 ((+) 4 5)) 3) dbl 4 + 4 4 8 No matter the designer\u0026rsquo;s choice, the outcome of the evaluation is the same. However, there is something strikingly different about the two. Notice that in the first derivation, the calculation of the average of the three numbers happens twice. In the second derivation, it happens only once! That efficiency is not a fluke! Generally speaking, the method of function invocation where arguments are evaluated before the function is evaluated is faster.\nThese two techniques have technical names:\n applicative order: \u0026ldquo;all the arguments to … procedures are evaluated when the procedure is applied.\u0026rdquo; normal order: \u0026ldquo;delay evaluation of procedure arguments until the actual argument values are needed.\u0026rdquo;  These definitions come from\nAbelson, H., Sussman, G. J.,, with Julie Sussman (1996). Structure and Interpretation of Computer Programs. Cambridge: MIT Press/McGraw-Hill. ISBN: 0-262-01153-0\nIt is obvious, then, that any serious language designer would choose applicative order for their language. There\u0026rsquo;s no reason redeeming value for the inefficiency of normal order. The Implications of Applicative Order\nScheme is a Lisp dialect . I told you that we weren\u0026rsquo;t going to work much with Lisp, but I lied. Sort of. Scheme is an applicative-order language with the same list-is-everything syntax as all other Lisps (see The Daily PL - 10/15/2021). In Scheme, you would define an if function named myif like this:\n(define (myif c t f) (cond (c t) (else f))) c is a boolean and myif returns t when c is true and f when c is false. No surprises.\nWe can define a name a and set its value to 5:\n(define a 5) Now, let\u0026rsquo;s call myif:\n(myif (= a 0) 1 (/ 1 a)) If a is equal to 0, then the call returns 1. Perfect. If a is not zero, the call returns the reciprocal of a. Given the value of a, the result is 1/7.\nLet\u0026rsquo;s define the name b and set its value to 0:\n(define b 0) Now, let\u0026rsquo;s call myif:\n(myif (= b 0) 1 (/ 1 b)) If b is equal to 0, then the call returns 1. If b is not zero, the call returns the reciprocal of b. Given the value of b, the result is 1:\n/: division by zero context...: \u0026#34;/home/hawkinsw/code/uc/cs3003/scheme/applicative/applicative.rkt\u0026#34;: [running body] temp37_0 for-loop run-module-instance!125 perform-require!78 That looks exactly like 1. What happened?\nRemember we said that the language is applicative order. No matter what the value of b, both of the arguments are going to be evaluated before myif starts. Therefore, Scheme attempts to evaluate 1 / b which is 1 / 0 which is division by zero.\nThanks to situations like this, the Scheme programming language is forced into defining special semantics for certain functions, like the built-in if expression. As a result, function invocation is not orthogonal in Scheme \u0026ndash; the general rules of function evaluation in Scheme must make an exception for applying functions like the built-in if expression. Remember that the orthogonality decreases as exceptions in a language\u0026rsquo;s specification increase. Sidebar: Solving the problem in Scheme\nFeel free to skip this section if you are not interested in details of the Scheme programming language. That said, the concepts in this section are applicable to other languages.\nIn Scheme, you can specify that the evaluation of an expression be delayed until it is forced.\n(define d (delay (/ 1 7))) defines d to be the eventual result of the evaluation of the division of 1 by 7. If we ask Scheme to print out d, we see\n#\u0026lt;promise:d\u0026gt; To bring the future tense into the present tense, we force a delayed evaluation:\n(force d) If we ask Scheme to print the result of that expression, we see:\n1/7 Exactly what we expect! With this newfound knowledge, we can rewrite the myif function:\n(define (myif c t f) (cond (c (force t)) (else (force f)))) Now myif can accept ts and fs that are delayed and we can use myif safely:\n(define b 0) (myif (= b 0) (delay 1) (delay (/ 1 b))) #+end_src lisp and we see the reasonable result: #+begin_src 1 Kotlin, a modern language, has a concept similar to delay called lazy . Ocaml, an object-oriented functional programming language, contains the same concept . Swift has some sense of laziness , too!\nWell, We Are Back to Normal Order I guess that we are stuck with the inefficiency inherent in the normal order function application. Going back to the dbl/average example, we will just have to live with invoking average twice.\nOr will we?\nReal-world functional programming languages that are normal order use an interesting optimization to avoid these recalculations! When an expression is passed around and it is unevaluated, Haskell and languages like it represent it as a thunk (Links to an external site.). The thunk data structure is generated in such a way that it can calculate the value of its associated expression some time in the future when the value is needed. Additionally, the thunk then caches (or memoizes) the value so that future evaluations of the associated expression do not need to be repeated.\nAs a result, in the dbl/average example,\n a thunk is created for (average 3 4 5), that thunk is passed to dbl, where it is duplicated during the reduction of dbl, (average 3 4 5) is (eventually) calculated for the first time using the thunk, 4 is stored (cached, memoized) in the thunk, and the cached/memoized value is retrieved from the thunk instead of evaluating (average 3 4 5) for a second time.  A thunk, then, is the mechanism that allows the programmer to have the efficiency of applicative order function invocation with the semantics of the normal order function invocation!\n10/27/2021 The Tail That Wags the Dog There are no loops in functional programming languages. We\u0026rsquo;ve learned that, instead, functional programming languages are characterised by the fact that they use recursion for control flow. As we discussed earlier in the class (much earlier, in fact), when running code on a von Neumann machine, iterative algorithms typically perform faster than recursive algorithms because of the way that the former leverages the properties of the hardware (e.g., spatial and temporal locality of code ). In addition, recursive algorithms typically use much more memory than iterative algorithms.\nWhy? To answer that question, we will need to recall what we learned about stack frames. For every function invocation, an activation record (aka stack frame) is allocated on the run-time stack (aka call stack). The function\u0026rsquo;s parameters, local variables, and return value are all stored in its activation record. The activation record for a function invocation remains on the stack until it has completed execution. In other words, if a function f invokes a function g, then function f\u0026rsquo;s activation record remains on the stack until (at least) g has completed execution.\nConsider some algorithm A. Let\u0026rsquo;s assume that an iterative implementation of A requires n executions of a loop and l local integer variables. If that implementation is contained in a function, an invocation of that function would consume approximately l*sizeof(integer variable) + activation record overhead space on the runtime stack. Let\u0026rsquo;s further assume that a function implementing the recursive version of A also uses l local integer variables and also requires n executions of itself. Each invocation of the implementing function, then, needs l*sizeof(integer variable) + activation record overhead space on the runtime stack. Multiply that by n, the number of recursive calls, and you can see that, at it\u0026rsquo;s peak, executing the recursive version of algorithm A requires n * (l * sizeof(integer variable) + activation record overhead) space on the run-time stack. In other words, the recursive version requires n times more memory! That escalated quickly .\nThat\u0026rsquo;s all very abstract. Let\u0026rsquo;s look at the implications with a real-world example, which we will write in Haskell syntax:\nmyLen [] = 0 myLen (x:xs) = 1 + (myLen xs) myLen is a function that recursively calculates the length of a list. Let\u0026rsquo;s see what the stack would look like when we call myLen [1,2,3]:\n When the recursive implementation of myLen reaches the base case, there are four activation records on the run-time stack.\nAllocating space on the stack takes time. Therefore, the more activation records placed on the stack, the more time the program will take to execute. But, if we are willing to live with a slower program, then there\u0026rsquo;s nothing else to worry about.\nRight?\nWrong. Modern hardware is fast. Modern computers have lots of memory. Unfortunately, they don\u0026rsquo;t have an infinite amount of memory. A program only has a finite amount of stack space. Given a long enough list, myLen could cause so many activation records to be placed on the stack that the amount of stack space is exhausted and the program crashes. In other words, it\u0026rsquo;s not just that a recursive algorithm might execute slower, a recursive algorithm might fail to calculate the correct result entirely!\nTail Recursion - Hope The activation records of functions that recursively call themselves remain on the stack because, presumably, they need the result of the recursive invocation to complete their work. For example, in our myLen function, an invocation of myLen cannot completely calculate the length of the list given as a parameter until the recursive call completes.\nWhat if there was some way to rewrite a recursive function in a way that it did not need to wait on a future recursive invocation to completely calculate its result? If that could happen, then the stack frame of the current invocation of the function could be replaced by the stack frame of the recursive invocation. Why? Because the information contained in the current invocation of the function has no bearing on its overall result \u0026ndash; the only information needed to completely calculate the result of the function is the result of the future recursive invocation! The implementation of a recursive function that matches this specification is known as a tail-recursive function. The book says \u0026ldquo;A function is tail recursive if its recursive call is the last operation in the function.\u0026rdquo;\nWith a tail-recursive function, we get the expressiveness of a recursive definition of the algorithm along with the efficiency of an iterative solution! Ron Popeil, that\u0026rsquo;s a deal!\nRewriting The rub is that we need to figure out a way to rewrite those non-tail recursive functions into tail-recursive versions. I am not aware of any general purpose algorithms for such a conversion. However, there is one technique that is widely applicable: accumulators. It is sometimes possible to add a parameter to a non-tail recursive function and use that parameter to define a tail-recursive version. Seeing an accumulator in action is the easiest way to define the technique. Let\u0026rsquo;s rewrite myLen in a tail-recursive manner using an accumulator:\nmyLen list = myLenImpl 0 list myLenImpl acc [] = acc myLenImpl acc (x:xs) = myLenImpl (1 + acc) xs First, notice how we are turning myLen into a function that simply invokes a worker function whose job is to do the actual calculation! The first parameter to myLenImpl is used to hold a running tally of the length of the list so far. The first invocation of myLenImpl from the implementation of myLen, then, passes 0 as the argument to the accumulator because the running tally of the length so far is, well, 0. The implementation of myLenImpl adds 1 to that accumulator variable for every list item that is stripped from the front of the list. The result is that the result of an invocation of myLenImpl does not rely on the completion of a recursive execution. Therefore, myLenImpl qualifies as a tail-recursive function! Woah.\nLet\u0026rsquo;s look at the difference in the contents of the run-time stack when we use the tail-recursive version of myLen to calculate the length of the list [1,2,3]:\n A constant amount of stack space is being used \u0026ndash; amazing!\n10/29/2021 No one Puts Locals In a Corner One of the really cool things about functional programming languages is their first-class support for functions. In functional programming languages, the programmer can pass functions as parameters to other functions and return functions from functions. Let\u0026rsquo;s take a closer look at functions that \u0026ldquo;generate\u0026rdquo; other functions. JavaScript has the capability to return functions from functions so we\u0026rsquo;ll use that language to explore:\nfunction urlGenerator(prefix) { function return_function(url) { return prefix + \u0026#34;://\u0026#34; + url; } return return_function; } The urlGenerator function takes a single parameter \u0026ndash; prefix. The caller of urlGenerator passes the protocol prefix as the argument (probably either \u0026ldquo;http\u0026rdquo; or \u0026ldquo;https\u0026rdquo;). The return value of urlGenerator is itself a function that takes a single parameter, a url, and returns url prepended with the prefix specified by the call to urlGenerator. An example might help:\nconst httpsUrlGenerator = urlGenerator(\u0026#34;https\u0026#34;); const httpUrlGenerator = urlGenerator(\u0026#34;http\u0026#34;); console.log(httpsUrlGenerator(\u0026#34;google.com\u0026#34;)); console.log(httpUrlGenerator(\u0026#34;google.com\u0026#34;)); generates\n\u0026#34;https://google.com\u0026#34; \u0026#34;http://google.com\u0026#34; In other words, the definition of httpsUrlGenerator is (conceptually)\nfunction httpsUrlGenerator(url) { return \u0026#34;https\u0026#34; + \u0026#34;://\u0026#34; + url; } and the definition of httpUrlGenerator is (conceptually)\nfunction httpUrlGenerator(url) { return \u0026#34;http\u0026#34; + \u0026#34;://\u0026#34; + url; } But that\u0026rsquo;s only a conceptual definition! The real definition continues to contain prefix:\nreturn prefix + \u0026#34;://\u0026#34; + url; But, prefix is locally scoped to the urlGenerator function. So, how can httpUrlGenerator and httpsUrlGenerator continue to use its value after leaving the scope of urlGenerator?\nThe Walls Are Closing In JavaScript, and other languages like it, have the concept of closures. A closure is a context that travels with a function returned by another function. Inside the closure are values for the free variables (remember that definition?) of the returned function. In urlGenerator, the returned function (return_function) uses the free variable prefix. Therefore, the closure associated with return_function contains the value of prefix at the time the closure is created!\nIn the example above, there are two different copies of return_function generated \u0026ndash; one when urlGenerator is called with the argument \u0026ldquo;http\u0026rdquo; and one when urlGenerator is called with the parameter \u0026ldquo;https\u0026rdquo;. Visually, the situation is\n Connections with Other Material Think about the connection between closures and the format of the urlGenerator/return_function functions and other concepts we\u0026rsquo;ve explored previously like partial application and currying.\n11/1/2021 Your Total Is \u0026hellip; In today\u0026rsquo;s class, we started with writing a simple function to sum the numbers in a list and ended up with the definition of a fundamental operation of functional programming: the fold. Let\u0026rsquo;s start by writing the simple, recursive definition of a sum function in Haskell:\nsimpleSum [] = 0 simpleSum (first:rest) = first + (simpleSum rest) When invoked with the list [1,2,3,4], the result is 10:\n*Summit\u0026gt; simpleSum [1,2,3,4] 10 Exactly what we expected. Let\u0026rsquo;s think about our job security: The boss tells us that they want a function does \u0026ldquo;products\u0026rdquo; all the elements in the list. Okay, that\u0026rsquo;s easy:\nsimpleProduct [] = 1 simpleProduct (first:rest) = first * (simpleProduct rest) When invoked with the list [1,2,3,4], the result is 24:\n*Summit\u0026gt; simpleProduct [1,2,3,4] 24 Notice that there are only minor differences between the two functions: the value returned in the base case (0 or 1) and the operation being performed on head and the result of the recursive invocation.\nI hear some of your shouting at me already: This isn\u0026rsquo;t tail recursive; you told us that tail recursive functions are important. Fine! Let\u0026rsquo;s rewrite the two functions so that they are tail recursive. We will do so using an accumulator and a helper function:\ntrSimpleSum list = trSimpleSumImpl 0 list trSimpleSumImpl runningTotal [] = runningTotal trSimpleSumImpl runningTotal (x:xs) = trSimpleSumImpl (runningTotal + x) xs When invoked with the list [1,2,3,4], the result is 10:\n*Summit\u0026gt; trSimpleSum [1,2,3,4] 10 And, we\u0026rsquo;ll do the same for the function that calculates the product of all the elements in the list:\ntrSimpleProduct list = trSimpleProductImpl 1 list trSimpleProductImpl runningTotal [] = runningTotal trSimpleProductImpl runningTotal (x:xs) = trSimpleProductImpl (runningTotal * x) xs When invoked with the list [1,2,3,4], the result is 24:\n*Summit\u0026gt; trSimpleProduct [1,2,3,4] 24 One of These Things is Just Like The Other Notice the similarities between trSimpleSumImpl and trSimpleProductImpl. Besides the names, the only difference is really the operation that is performed on the runningTotal and the head element of the list. Because we\u0026rsquo;re using a functional programming language, what if we wanted to let the user specify that operation in terms of a function parameter? Such a function would need have to accept two arguments (the up-to-date running total and the head element) and return a new running total. For summing, we might write a sumOperation function:\nsumOperation runningTotal headElement = runningTotal + headElement Next, instead of defining trSimpleSumImpl and trSimpleProductImpl with fixed definitions of their operation, let\u0026rsquo;s define a trSimpleOpImpl that could use sumOperation:\ntrSimpleOpImpl runningTotal operation [] = runningTotal trSimpleOpImpl runningTotal operation (x:xs) = trSimpleOpImpl (operation runningTotal x) operation xs Fancy! Now, let\u0026rsquo;s use trSimpleOpImpl and sumOperation to recreate trSimpleSum from above:\ntrSimpleSum list = trSimpleOpImpl 0 sumOperation list Let\u0026rsquo;s check to make sure that we get the same results: When invoked with the list [1,2,3,4], the result is 10:\n*Summit\u0026gt; trSimpleSum [1,2,3,4] 10 To confirm our understanding of what\u0026rsquo;s going on here, let\u0026rsquo;s visualize the invocations of sumOperation necessary to complete the calculation of trSimpleSum:\nsumOperation 0 1 sumOperation 1 2 sumOperation 3 3 sumOperation 6 4 Let\u0026rsquo;s do a similar thing for trSimpleProduct:\nproductOperation runningTotal headElement = runningTotal * headElement trSimpleProduct list = trSimpleOpImpl 0 productOperation list Let\u0026rsquo;s check to make sure that we get the same results: When invoked with the list [1,2,3,4], the result is 24:\n*Summit\u0026gt; trSimpleProduct [1,2,3,4] 24 Think About the Types: We\u0026rsquo;ve stumbled on a pretty useful pattern! Let\u0026rsquo;s look at its components:\n A \u0026ldquo;driver\u0026rdquo; function (called trSimpleOpImpl) that takes three parameters: an initial value (of a particular type, T), an operation function (see below) and a list of inputs, each of which is of type T. An operation function that takes two parameters \u0026ndash; a running total, of some type R; an element to \u0026ldquo;accumulate\u0026rdquo; on to the running total of type T \u0026ndash; and returns a new running total of type R. A list of inputs, each of which is of type T.  Here are the types of those functions in Haskell:\noperation function: R -\u0026gt; T -\u0026gt; R\nlist of inputs: [T]\ndriver function: T -\u0026gt; (R -\u0026gt; T -\u0026gt; R) -\u0026gt; R\nLet\u0026rsquo;s play around and see what we can write using this pattern. How about a concatenation of a list of strings in to a single string?\nconcatenateOperation concatenatedString newString = concatenatedString ++ newString concatenateStrings list = trSimpleOpImpl \u0026#34;\u0026#34; concatenateOperation list (the ++ just concatenates two strings together). When we run this on [\u0026ldquo;Hello\u0026rdquo;, \u0026ldquo;,\u0026rdquo;, \u0026ldquo;World\u0026rdquo;] the result is \u0026ldquo;Hello, World\u0026rdquo;:\n*Summit\u0026gt; concatenateStrings [\u0026#34;Hello\u0026#34;, \u0026#34;,\u0026#34;, \u0026#34;World\u0026#34;] \u0026#34;Hello,World\u0026#34; So far our Ts and Rs have been the same \u0026ndash; integers and strings. But, the signatures indicate that they could be different types! Let\u0026rsquo;s take advantage of that! Let\u0026rsquo;s use trSimplOpImpl to write a function that returns True if every element in the list is equal to the number 1 and False otherwise. Let\u0026rsquo;s call the operation function continuesToBeAllOnes and define it like this:\ncontinuesToBeAllOnes equalToOneSoFar maybeOne = equalToOneSoFar \u0026amp;\u0026amp; (maybeOne == 1) This function will return True if the list (to this point) has contained all ones (equalToOneSoFar) and the current element (maybeOne) is equal to one. In this case, the R is a boolean and the T is an integer. Let\u0026rsquo;s implement a function named isListAllOnes using continuesToBeAllOnes and trSimplOpImpl:\nisListAllOnes list = trSimpleOpImpl True continuesToBeAllOnes list Does it work? When invoked with the list [1,1,1,1], the result is True:\n*Summit\u0026gt; isListAllOnes [1,1,1,1] True When invoked with the list [1,2,1,1], the result is False:\n*Summit\u0026gt; isListAllOnes [1,2,1,1] False Naming those \u0026ldquo;operation\u0026rdquo; functions every time is getting annoying, don\u0026rsquo;t you think? I bet that we could be lazier!! Let\u0026rsquo;s rewrite isListAllOnes without specifically defining the continuesToBeAllOnes function:\nisListAllOnes list = trSimpleOpImpl True (\\equalToOneSoFar maybeOne -\u0026gt; equalToOneSoFar \u0026amp;\u0026amp; (maybeOne == 1)) list Now we are really getting functional!\nI am greedy. I want to write a function that returns True if any element of the list is a one:\nisAnyElementOne list = trSimpleOpImpl False (\\anyOneSoFar maybeOne -\u0026gt; anyOneSoFar || (maybeOne == 1)) list This is just way too much fun!\nFold the Laundry This type of function is so much fun that it is included in the standard implementation of Haskell! It\u0026rsquo;s called fold! And, in true Haskell fashion, there are two different versions to maximize flexibility and confusion: the fold-left and fold-right operation. The signatures for the functions are the same in both cases:\nfold[l,r] :: operation function -\u0026gt; initial value -\u0026gt; list -\u0026gt; result\nIn all fairness, these two versions are necessary. Why? Because certain operation functions are not associative! It doesn\u0026rsquo;t matter the order in which you add or multiply a series of numbers \u0026ndash; the result of (5 * (4 * (3 *2))) is the same as (((5 * 4) * 3) * 2). The problem is, that\u0026rsquo;s not the case of an operation like division!\nA fold-left operation (foldl) works by starting the operation (essentially) from the first element of the list and the fold-right operation (foldr) works by starting the operation (essentially) from the last element of the list. Furthermore, the choice of foldl vs foldr affects the order of the parameters to the operation function: in a foldl, the running value (which is known as the accumulator in mainstream documentation for fold functions) is the left parameter; in a foldr, the accumulator is the right parameter.\nThis will make more sense visually, I swear:\n*Summit\u0026gt; foldl (\\x y -\u0026gt; x /y ) 1 [3,2,1] 0.16666666666666666  *Summit\u0026gt; foldr (\\x y -\u0026gt; x / y ) 1 [3,2,1] 1.5  Let\u0026rsquo;s use our newfound fold power, to recreate our work from above:\nisAnyElementOne list = foldl (\\anyOneSoFar maybeOne -\u0026gt; anyOneSoFar || (maybeOne == 1)) False list isListAllOnes list = foldl (\\equalToOneSoFar maybeOne -\u0026gt; equalToOneSoFar \u0026amp;\u0026amp; (maybeOne == 1)) True list concatenateStrings list = foldl (\\concatenatedString newString -\u0026gt; concatenatedString ++ newString) \u0026#34;\u0026#34; list 11/3/2021 It\u0026rsquo;s the 3rd day of November and we are about to switch to non-daylight savings time. What better way to celebrate the worst day of the year than switching our attention to a new topic!\nI Do Declare In this module we are going to focus on logic (also known as declarative) programming languages. Users of a declarative programming language declare the outcome they wish to achieve and let the compiler do the work of achieving it. This is in marked contrast to users of an imperative programming language who have to tell the compiler not only the outcome they wish to achieve but also how to achieve it. A declarative programming language does not have program control flow constructs, per se. Instead, a declarative programming language gives the programmer the power to control execution by means of recursion (again?? I know, sorry!) and backtracking. Backtracking is a concept that we will return to later. A declarative program is all about defining facts (either as axioms or as ways to build new facts from axioms) and asking questions about those facts. From the programming language\u0026rsquo;s perspective, those facts have no inherent meaning. We, the programmers, have to impugn meaning on to the facts. Finally, declarative programming languages do have variables, but they are nothing the variables that we know and love in imperative programming languages.\nAs we have worked through the different programming paradigms, we have discussed the theoretical underpinning of each. For imperative programming languages, the theoretical model is the Turing Machine. For the functional programming languages, the theoretical model is the Lambda Calculus. The declarative programming paradigm has a theoretical underpinning, too: first-order predicate calculus. We will talk more about that in class soon!\nIn the Beginning Unlike imperative, object-oriented and functional programming languages, there is really only one extant declarative/logic programming language: Prolog. Prolog was developed by Alain Colmeraurer, Phillipe Roussel, and Robert Kowalski in order to do research in artificial intelligence and natural language processing\n. Its official birthday is 1972.\n Prolog programs are made up of exactly three components:\n Facts Rules Queries  The syntax of Prolog defines the rules for writing facts, rules and queries. Syntactic elements in Prolog belong to one of three categories:\n Atoms: The most fundamental unit of a Prolog program. They are simply symbols. Usually they are simply sequences of characters that begin with a lowercase letter. However, atoms can contain spaces (in which case they are enclosed in \u0026rsquo;s) and they can start with uppercase letters (in which case they are wrapped with \u0026rsquo;s). Variables: Like atoms, but variables always start with uppercase letters. Functors: Like atoms, but functors define relationships/facts.  If Prolog is a logic programming language, there must be some means for specifying logical operations. There is! In the context of specifying a rule, the and operation is written using a ,. In the context of specifying a rule, the or operation is written using a ;. Obviously!\nThe best way to learn Prolog is to start writing some programs. We\u0026rsquo;ll come back to the theory later!\nJust The Facts At its most basic, a Prolog program is a set of facts:\ntakes(jane, cs4999). takes(alicia, cs2020). takes(alice, cs4000). takes(mary, cs1021). takes(bob, cs1021). takes(kristi, cs4000). takes(sam, cs1021). takes(will, cs2080). takes(alicia, cs3050).  In relation to the first-order predicate logic that Prolog models, takes is a logical predicate. We\u0026rsquo;ll refer to them as facts or predicates, depending on what\u0026rsquo;s convenient. Formally, they predicates and facts are written as /. Two (or more) facts/predicates with the same functor but different arities are not the same. For instance, takes/1 and takes/2 are completely different.\nLet\u0026rsquo;s read one of these facts in English:\ntakes(jane, cs4999). could be read as \u0026ldquo;Jane takes CS4999.\u0026rdquo;. As programmers, we know what that means: the student named Jane is enrolled in the class CS4999. However, Prolog does not share our sense of meaning! Prolog simply thinks that we are defining one element of the takes relationship where jane is somehow related to cs4999. As a Prolog programmer, we could just has easily have written\ntennis_shoes(jane, cs4999). tennis_shoes(alicia, cs2020). tennis_shoes(alice, cs4000). tennis_shoes(mary, cs1021). tennis_shoes(bob, cs1021). tennis_shoes(kristi, cs4000). tennis_shoes(sam, cs1021). tennis_shoes(will, cs2080). tennis_shoes(alicia, cs3050). and gotten the same result! But, we programmers want to define something that is meaningful, so we choose to use atoms that reflect their semantic meaning. With nothing more than the facts that we have defined above, we can write queries. In order to interact with queries in real time, we can use the Prolog REPL. Once we have started the Prolog REPL, we will see a prompt like this:\n?- The world awaits \u0026hellip;\nTo load a Prolog file in to the REPL, we will use the consult predicate:\n?- consult(\u0026#39;intro.pl\u0026#39;). true. The Prolog facts, rules and queries in the intro.pl file are now available to us. Assume that intro.pl contains the takes facts from above. Let\u0026rsquo;s make some queries:\n?- takes(bob, cs1021). true. ?- takes(will, cs2080). true. ?- takes(ali, cs4999). false. These are simple yes/no queries and Prolog obliges us with terse answers. But, even with the simple facts shown above, Prolog can be used to make some powerful inferences. Prolog can tell us the names of all the people it knows who are taking a particular class:\n?- takes(Students, cs1021). Students = mary ; Students = bob ; Students = sam. Wow! Here Prolog is telling us that there are three different values of the Students variable that will make the query true: mary, bob and sam. In the lingo, Prolog is unifying Students with the values that will make our query true. Let\u0026rsquo;s go the other way around:\n?- takes(alicia, Classes). Classes = cs2020 ; Classes = cs3050. Here Prolog is telling us that there are two different classes that Alicia is taking. Nice.\nThat\u0026rsquo;s great, but pretty limited: it\u0026rsquo;s kind of terrible if we had to write out each fact explicitly! The good news is that we don\u0026rsquo;t have to do that! We can use a Prolog rule to define facts based on the existence of other facts. Let\u0026rsquo;s define a rule which will tell us the students who are CS majors. To be a CS major, you must be taking (exactly) two classes:\ncs_major(X) :- takes(X, Y), takes(X, Z), Y @\u0026lt; Z. That\u0026rsquo;s lots to take in at first glance. Start by looking at the general format of a rule:\n Okay, so now back to our particular rule that defines what it means to be a CS Major. (For the purposes of this discussion, assume that the @\u0026lt; operator is \u0026ldquo;not equal\u0026rdquo;). Building on what we know (e.g., , is and, :- is implication, etc), we can read the rule like: \u0026ldquo;X is a CS Major if X takes class Y and X takes class Z and class Y and Z are not the same class.\u0026rdquo; Pretty succinct definition.\nTo make the next few examples a little more meaningful, let\u0026rsquo;s update our list of facts before moving on:\ntakes(jane, cs4999). takes(alicia, cs2020). takes(alice, cs4000). takes(mary, cs1021). takes(bob, cs1021). takes(bob, cs8000). takes(kristi, cs4000). takes(sam, cs1021). takes(will, cs2080). takes(alicia, cs3050). With that, let\u0026rsquo;s find out if our rule works as intended!\n?- cs_major(alicia). true ; false. Wow! Pretty cool! Prolog used the rule that we wrote, combined it with the facts that it knows, and inferred that Alicia is a CS major! (For now, disregard the second False \u0026ndash; we\u0026rsquo;ll come back to that!). Like we could use Prolog to generate the list of classes that a particular student is taking, can we ask Prolog to generate a list of all the CS majors that it knows?\n?- cs_major(X). X = alicia ; X = bob ; false. Boom!\n11/5/2021 *The retreat to move forward.\nBacktracking In today\u0026rsquo;s lecture, we discussed the concepts of backtracking and choice points in order to learn how Prolog can determine the meaning of our programs.\nThere is a formal definition of choice points and backtracking from the Prolog glossary:\nbacktracking: Search process used by Prolog. If a predicate offers multiple clauses to solve a goal, they are tried one-by-one until one succeeds. If a subsequent part of the proof is not satisfied with the resulting variable binding, it may ask for an alternative solution, causing Prolog to reject the previously chosen clause and try the next one.\nThere are lots of undefined words in that definition! Let\u0026rsquo;s dig a little deeper.\nA predicate is like a boolean function. A predicate takes one or more arguments and yields true/false. As we said at the outset of our discussion about declarative programming languages, the reason that a predicate may yield true or false depends on the semantics imposed upon it from the outside. A predicate is a term borrowed from first-order logic, a topic that we will return to in later lectures.\nRemember our takes example from previous lectures? takes is a predicate! takes has two arguments and returns a boolean.\nIn Prolog, rules and facts are written to define predicates. A rule defines the conditions under which a predicate is true using a body \u0026ndash; a list of other predicates, logical conjunctives, implications, etc. See The Daily PL - 11/3/2021 for additional information about rules. A fact is a rule without a body and unconditionally defines that a certain relationship is true for a predicate.\nrelated(will, bill). related(ali, bill). related(bill, william). related(X, Y) :- related(X, Z), related(Z, Y).\nIn the example above, related is a predicate defined by facts and rules. The facts and rules above are the clauses of the predicate.\nchoicepoint: A choice point represents a choice in the search for a solution. Choice points are created if multiple clauses match a query or using disjunction (;/2). On backtracking, the execution state of the most recent choice point is restored and search continues with the next alternative (i.e., next clause or second branch of ;/2).\nThat\u0026rsquo;s a mouthful! I think that the best way to understand this is to look at backtracking in action and see where choice points exist.\nGive and Take Remember the takes predicate that we defined in the last class:\ntakes(jane, cs4999). takes(alicia, cs2020). takes(alice, cs4000). takes(mary, cs1021). takes(bob, cs1021). takes(bob, cs8000). takes(kristi, cs4000). takes(sam, cs1021). takes(will, cs2080). takes(alicia, cs3050). We subsequently defined a cs_major predicate:\ncs_major(X) :- takes(X, Y), takes(X, Z), Y @\u0026lt; Z. The cs_major predicate says that a student who takes two CS classes is a CS major. Let\u0026rsquo;s walk through how Prolog would respond to the following query:\ncs_major(X). To start, Prolog realizes that in order to satisfy our query, it has to at least satisfy the query\ntakes(X, Y). So, Prolog starts there. In order to satisfy that query, it searches its knowledge base (its list of facts/rules that define predicates) from top to bottom. X and Y are variables and the first appropriate fact that it finds is\ntakes(jane, cs4999). So, it unifies X with jane and Y with cs4999. Having satisfied that query, Prolog realizes that it must also satisfy the query:\ntakes(X, Z). However, Prolog has already provisionally unified X with jane. So, Prolog really attempts to satisfy the query:\ntakes(jane, Z). Great. For Prolog, attempting to satisfy this query is completely distinct from its attempt to satisfy the query takes(X, Y) which means that Prolog starts searching its knowledge base anew (again, from top to bottom!). The first appropriate fact that it finds that satisfies this query is\ntakes(jane, cs4999).\nSo, it unifies Z with cs4999. Having satisfied that query too, Prolog moves on to the third query:\nY @\u0026lt; Z. Unfortunately, because X and Y are both unified to cs4999, Prolog fails to satisfy that query. In other words, Prolog realizes that its provisional unification of X with jane, Y with cs4999 and Z with cs4999 is not a way to satisfy our original query (cs_major(X)).\nDoes Prolog just give up? Absolutely not! It\u0026rsquo;s persistent. It backtracks! To where?\nWell, according to the definition above, it backtracks to the most-recent choicepoint! In this case, the most recent choicepoint was its provisional unification of Z with cs4999. So, Prolog forgets about that attempt, and restarts the search of its knowledge base.\nWhere does it restart that search, though? This is important: It restarts its search where it left off. In other words, it starts at the first fact at takes(jane, cs4999). Because there are no other facts about classes that Jane takes, Prolog fails again, this time attempting to satisfy the query takes(jane, Z).\nI ask again, does Prolog just give up? No, it backtracks again! This time it backtracks to its most-recent choicepoint. Now, that most recently choicepoint was its provisional unification of X with jane. Prolog forgets that attempt, and restarts the search of its knowledge base! Again, because this is the continuation of a previous search, Prolog begins where it left off in its top-to-bottom search of its knowledge base. The next fact that it see is\ntakes(alicia, cs2020) So, Prolog provisionally unifies X with alicia and Y with cs2020. Having satisfied that query (for a second time!), Prolog realizes that it must also satisfy the query:\ntakes(X, Z). However, Prolog has provisionally unified X with alicia. So, Prolog really attempts to satisfy the query:\ntakes(alicia, Z). Great. For Prolog, attempting to satisfy this query is completely distinct from its attempt to satisfy the query takes(X, Y) and its previous attempt to satisfy the query takes(jane, Z). Therefore, Prolog starts searching its knowledge base anew (again, from top to bottom!). The first appropriate fact that it finds that satisfies this query is\ntakes(alicia, cs2020). So, it unifies Z with cs2020. Having satisfied that query too, Prolog moves on to the third query:\nY @\u0026lt; Z. Unfortunately, because Z and Y are both unified to cs2020, Prolog fails to satisfy that query. In other words, Prolog realizes that its provisional unification of X with alicia, Y with cs2020 and Z with cs2020 is not a way to satisfy our original query (cs_major(X)). Again, Prolog does not give up and it backtracks to its most recent choicepoint. The good news is that Prolog can satisfy the query\ntakes(alicia, Z) a second way by unifying Z with cs3050. Prolog proceeds to the final part of the rule\nX @\u0026lt; Y which can be satisfied this time because cs3050 and cs2020 are different!\nVictory!\nProlog was able to satisfy the original query when it unified X with alicia, Y with cs2020 and Z with cs3050.\nBelow is a visualization of the description given above:\n A Prolog user at the REPL (or a Prolog program using this rule) could ask for all the ways that this query is satisfied. And, if the user/program does, then Prolog will backtrack as if it did not find a satisfactory unification for Z, Y or X (in that order!).* 11/5/2021\n11/8/2021 In the true spirit of a picture being worth a thousand words , think of this Daily PL as a graphic novel.\nGoing Over Backtracking Again (see what I did there?) In today\u0026rsquo;s lecture, we went deeper into the discussion of backtracking and saw its utility. In particular, we discussed the following Prolog program for generating all the integers.\ngenerate_integer(0). generate_integer(X) :- generate_integer(Y), X is Y + 1. This is an incredibly succinct way to declare what it means to be an integer. This generator function is attributable to Programming in Prolog by Mellish and Clocksin (Links to an external site.). In other words, we know that it\u0026rsquo;s a reasonable definition.\nAs discussed in the description of Assignment 3, the best way to learn Prolog, I think, is to play with it! So, let\u0026rsquo;s see what this can do!\n?- generate_integer(5). true ; In other words, it can be used to determine whether a given number is an integer. Awesome.\n?- generate_integer(X). X = 0 ; X = 1 ; X = 2 ; X = 3 ; X = 4 ; X = 5 ; X = 6 Woah, look at that \u0026hellip; generate_integer/1 can do double duty and generate all the integers, too. Pretty cool!\nThe generation of the numbers using this definition is possible thanks to the power of backtracking. If you need a refresher on backtracking and/or choice points, I recommend previous Daily PLs.\nThe (im)possibility of using words to describe the backtracking involved in generate_integer/1, led me to create the following diagram that attempts to illustrate the process. I encourage you to look at the diagram, run generate_integer/1 in swipl with trace enabled and ask any questions that you have! Again, this is not a simple concept, but once you understand what is going on, Prolog will begin to make more sense!\n It may be necessary to use a high-resolution version of the diagram if you are curious about the details. Such a version is available in SVG format here.\nChasing Our Tails Yes, I can hear you! I know, generate_integer/1 is not tail recursive. We learned that tail recursion is a useful optimization in functional programming languages (and even imperative programming languages). Does that mean that it\u0026rsquo;s an important optimization in Prolog?\nTo find out, I timed how long it took Prolog to answer the query\n?- generate_integer(50000). The answer? On my desktop computer, it took 1 minute and 48 seconds.\nIf we want something for comparison, we\u0026rsquo;ll have to come up with a tail-recursive version of generate_integer/1. Let\u0026rsquo;s call it generate_integer_tr/1 (creative, I know), and define it like:\ngenerate_integer_tr(X) :- next_integer(0,X). next_integer(J, J). next_integer(J, L) :- K is J + 1, next_integer(K, L). The fact next_integer(J, J) is a \u0026ldquo;trick\u0026rdquo; to define a base case for our definition in the same way that generate_integer(0) was a base case in the definition of generate_integer/1. To get a sense for the need for next_integer(J, J) think about what happens when the first query of next_integer(0,X) is performed in order to satisfy the query generate_integer_tr(50000). In this case, the next_integer(J, J) fact matches (convince yourself why! Hint: there are no restrictions on J). As a result, J unifies with the 0, and the X unifies with the J. That\u0026rsquo;s great, but (X = ) 0 does not equal 50000. So, Prolog does what?\nIn unison: BACKTRACKS.\nThe choice point is Prolog\u0026rsquo;s selection of next_integer(J, J), so Prolog tries again at the next possible fact/rule: next_integer(J, L) :- K is J + 1, next_integer(K, L). J is unified with 0, K is unified with 1 (J + 1) and Prolog must now satisfy a new goal: next_integer(1, L). Because this query for next_integer/1 is completely different than the one it is currently attempting to satisfy, Prolog starts the search anew at the top of the list of facts. The first fact to match? next_integer(J, J). Therefore, J unifies with 1, L unifies with J (making L 1), and X (from the initial query) unifies with L. Unfortunately, the result again does not satisfy our query. But, Prolog persists and backtracks but only as far as the second attempt to satisfy next_integer (using next_integer(J,J)). In much the same way that generate_integer/1 worked, Prolog continues to progress, then backtrack, then progress, then backtrack \u0026hellip; while solving the generate_integer_tr(50000) query.\nThe difference between the two functions is that in next_integer/2, the recursive act is the last thing that is done. In other words, generate_integer_tr/1 is tail recursive.\nDoes this impact performance? Absolutely! On my desktop. Prolog can answer the query generate_integer_tr(50000). in 0.029 seconds. Yeow!\n11/10/2021 In today\u0026rsquo;s class we learned about a workhorse rule in logic programming: append/3. append/3 can be used to implement many other different rules, including a rule that will generate all the permutations of a list!\nPin the Tail on the List The goal (pun intended) of append is to determine whether two lists, X and Y, are the same as a third list, Z, when appended together. We could use the append query like this:\n?- append([1,2,3], [a,b,c], [1,2,3,a,b,c]). true. or\n?- append([1,2,3], [a,b], [1,2,3,a,b,c]). false. What we will find is that append/3 has a hidden superpower besides its ability to simply answer yes/no queries like the ones above.\nThe definition of append/3 will follow the pattern of other recursively defined rules that we have seen so far. Let\u0026rsquo;s start with the base case. The result of appending an empty list with some list Y, is just list Y. Let\u0026rsquo;s write that down:\nappend([], Y, Y). And now for the recursive case: appending list X to list Y yields some list Z where Z is the first element of the list X following by the result of appending the tail of X with Y. The natural language version of the definition is complicated but I think that the Prolog definition makes it more clear:\nappend([H|T], Y, [H|AppendedList]) :- append(T, Y, AppendedList). Let\u0026rsquo;s see how Prolog attempts to answer the query append([1,2], [a,b], [1,2,a,b]).\n And now let\u0026rsquo;s look at it\u0026rsquo;s operation for the query append([1,2], [a,b], [1,a,b]).\n It\u0026rsquo;s also natural to look at append/3 as a tool to \u0026ldquo;assign\u0026rdquo; a variable to be the result of appending two lists:\n?- append([1,2,3], [a,b,c], Z). Z = [1, 2, 3, a, b, c]. Here we are asking Prolog to assign variable Z to be a list that holds the appended contents of the lists in the first two arguments.\nLook at Append From a Different Angle We\u0026rsquo;ve seen append/3 in action so far in a procedural way \u0026ndash; answering whether two lists are equal to one another and \u0026ldquo;assigning\u0026rdquo; a variable to a list that holds the contents of another two lists appended to one another. But earlier I said that append/3 has some magical powers.\nIf we look at append/3 from a different angle, the declarative angle, we can see how it can be used to generate all the different combinations of two lists that, when appended together, yield a third list! For example,\n?- append(X, Y, [1,2,3]). X = [], Y = [1, 2, 3] ; X = [1], Y = [2, 3] ; X = [1, 2], Y = [3] ; X = [1, 2, 3], Y = [] ; Wow. That\u0026rsquo;s pretty cool! Prolog is telling us that it can figure out three different combinations of lists that, when appended together, will equal the list [1,2,3]. I mean, if that doesn\u0026rsquo;t make your blood boil, I don\u0026rsquo;t know what will.\nLet\u0026rsquo;s Ride the Thoroughbred The power of append/3 makes it useful in so many different ways. When I started learning Prolog, the resource I was using (Learn Prolog Now (Links to an external site.)) spent an inordinate amount of time discussing append/3 and it\u0026rsquo;s utility. It took me a long time to really understand the author\u0026rsquo;s point. A long time.\nPrefix and Suffix Let\u0026rsquo;s take a quick look at how to define a rule prefix/2. prefix/2 takes two arguments \u0026ndash; a possible prefix, PP, and a list, L \u0026ndash; and determines whether PP is a prefix of L. We\u0026rsquo;ve gotten so used to writing recursive definitions, it seems obvious that we would define prefix/2 using that pattern. In the base case, an empty list is a prefix of any list:\nprefix([], _). (Remember that _ is \u0026ldquo;I don\u0026rsquo;t care.\u0026quot;). With that out of the way, we can say that PP is a prefix of L if\nthe head element of PP is the same as the head element of L, and the tail of PP is a prefix of the tail of L:\nprefix([H|Xs], [H|Ys]) :- prefix(Xs, Ys). Fantastic. That\u0026rsquo;s a pretty easy definition and it works in all the ways that we would expect:\n?- prefix([1,2], [1,2,3]). true. ?- prefix([1,2], [2,3]). false. ?- prefix(X, [1,2,3]). X = [] ; X = [1] ; X = [1, 2] ; X = [1, 2, 3] ; But, what if there was a way that we could write the definition of prefix/2 more succinctly! Remember, programmers are lazy \u0026ndash; the fewer keystrokes the better!\nThink about this alternate definition of prefix: PP is a prefix of L, when there is a (possibly empty) list, W (short for \u0026ldquo;whatever\u0026rdquo;), such that PP appended with W is equal to L. Did you see the magic word? Appended! We have append/3, so let\u0026rsquo;s use it:\nprefix(PP, L) :- append(PP, _, L). (Note that we have replaced W from a natural-language definition with _ because we don\u0026rsquo;t care about it\u0026rsquo;s value!)\nWe could go through the same process of defining suffix/2 recursively, or we could cut to the chase and define it in terms of append/3. Let\u0026rsquo;s save ourselves some time: SS is a suffix of L, when there is a (possibly empty) list, W (short for \u0026ldquo;whatever\u0026rdquo;), such that W appended with SS is equal to L. Let\u0026rsquo;s codify that:\nsuffix(SS, L) :- append(_, SS, L). But, does it work?\n?- suffix_append([3,4], [1,2,3,4]). true. ?- suffix_append([3,5], [1,2,3,4]). false. ?- suffix_append(X, [1,2,3,4]). X = [1, 2, 3, 4] ; X = [2, 3, 4] ; X = [3, 4] ; X = [4] ; X = [] ; Permutations We\u0026rsquo;re all friends here, aren\u0026rsquo;t we? Good. I have no problem admitting that I am terrible at thinking about permutations of a set. I have tried and tried and tried to understand the section in Volume 4 of Knuth\u0026rsquo;s TAOCP (Links to an external site.) about generating permutations (Links to an external site.) but it\u0026rsquo;s just too hard for me. Instead, I just play around with them until I grok it. To help me play, I want Prolog\u0026rsquo;s help. I want Prolog to generate for me all the permutations of a given list. We will call this permute/2. Procedurally, permute/2 will say whether its second argument is a permutation of its first argument. Declaratively, permute/2 will generate a list of permutations of elements in the list in its first argument. Let\u0026rsquo;s work back from the end: We\u0026rsquo;ll see how it should work before actually defining it:\n?- permutation([1,2,3], [3,1,2]). true . ?- permutation([1,2,3], L). L = [1, 2, 3] ; L = [1, 3, 2] ; L = [2, 1, 3] ; L = [2, 3, 1] ; L = [3, 1, 2] ; L = [3, 2, 1] ; false. Cool. If I run permute/2 enough times I might start to understand them!\nNow that we know the results of invoking permute/2, how are we going to define it? Again, let\u0026rsquo;s take the easy way out and see the definition and then walk through it piece-by-piece in order to understand its meaning:\npermute([], []). permute(List, [X|Xs]) :- append(W, [X|U], List), append(W, U, ListWithoutX), permute(ListWithoutX, Xs). Well, the first rule is simple \u0026ndash; the permutation of an empty list is just the empty list!\nThe second rule, well, not so much! There are three conditions that must be satisfied for a list defined as [X|Xs] to be a permutation of List.\nThink of the first condition in a declarative sense: \u0026ldquo;Prolog, make me two lists that, when appended together, equal List. Name the first one W. And, while you\u0026rsquo;re at it, make sure that the first element in the second list is X and call the tail of the second list U. Thanks.\u0026rdquo;\nThink of the second condition in a procedural sense: \u0026ldquo;Prolog, append W and U to create a new list named ListWithoutX.\u0026rdquo; The name ListWithoutX is pretty evocative because, well, it is a list that contains every element of List but X.\nFinally, think of the third condition in a declarative sense: \u0026ldquo;I want Xs to be all the possible permutations of ListWithoutX \u0026ndash; Prolog, make it so!\u0026rdquo;\nLet\u0026rsquo;s try to put all this together into a succinct natural-language definition: A list whose first element is X and whose tail is Xs is a permutation of List if:\n X is one of the elements of List, and Xs is a permutation of the list List without element X.  Below is a visualization of the process Prolog takes to generate the first two permutations of [1,2,3]:\n 11/12/2021 If the append/3 predicate that we wrote on Wednesday is a horse that we can ride to accomplish many different tasks, then Prolog is like a wild stallion that tends to run in a direction of its choosing. We can use cuts to tame our mustang and make it go where we want it to go!\nThe Possibilities Are Endless Let\u0026rsquo;s start the discussion by writing a merge/3 predicate. The first two arguments are sorted lists. The final argument should unify to the in-order version of the first two arguments merged. Before starting to write some Prolog, let\u0026rsquo;s think about how we could do this.\nLet\u0026rsquo;s deal with the base cases first: When either of the first two lists are empty, the merged list is the non-empty list. We\u0026rsquo;ll write that like\nmerge(Xs, [], Xs). merge([], Ys, Ys). And now, for the recursive cases: We will call the first argument Left, the second argument Right, and the third argument Sorted. The first element of Left can be called HeadLeft and the rest of Left can be called TailLeft. The first element of Right can be called HeadRight and the rest of Right can be called TailRight. In order to merge, there are three cases to consider:\n HeadLeft is less than HeadRight HeadLeft is equal to HeadRight HeadRight is less than HeadLeft  For case (1), the head of the merged result is HeadLeft and the tail of the merged result is the result of merging TailLeft with Right. For case (2), the head of the merged result is HeadLeft and the tail of the merged result is the result of merging TailLeft with TailRight. For case (3), the head of the merged result is HeadRight and the tail of the merged result is the result of merging Left with TailRight.\nIt\u0026rsquo;s far easier to write this in Prolog than English:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X\u0026lt;Y, merge(Xs, [Y|Ys], Zs). merge([X|Xs], [Y|Ys], [X|Zs]) :- X=:=Y, merge(Xs, Ys, Zs). merge([X|Xs], [Y|Ys], [Y|Zs]) :- Y\u0026lt;X, merge([X|Xs], Ys, Zs). (Note: : is the \u0026ldquo;equal to\u0026rdquo; boolean operator in Prolog. See =:=/2 (Links to an external site.) for more information.\nFor merge/3, let\u0026rsquo;s write the base cases after our recursive cases. With that switcheroo, we have the following complete definition of merge/3:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X\u0026lt;Y, merge(Xs, [Y|Ys], Zs). merge([X|Xs], [Y|Ys], [X|Zs]) :- X=:=Y, merge(Xs, Ys, Zs). merge([X|Xs], [Y|Ys], [Y|Zs]) :- Y\u0026lt;X, merge([X|Xs], Ys, Zs). merge(Xs, [], Xs). merge([], Ys, Ys). Let\u0026rsquo;s follow Prolog as it attempts to use our predicate to answer the query\nmerge([1, 3, 5], [2,4,6], M). As we know, Prolog will search top to bottom when looking for ways to unify and the first rule that Prolog sees is applicable:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X\u0026lt;Y, merge(Xs, [Y|Ys], Zs). Why? Because Prolog sees X as 1 and Y as 2 and 1 \u0026lt; 2. Therefore, Prolog will complete its unification for this query by replacing it with another query:\nmerge([3,5], [2,4,6], Zs).  Once Prolog has completed that query, the response comes back:\nM = [1,2,3,4,5,6] Unfortunately, that\u0026rsquo;s not the whole story. While Prolog is off attempting to satisfy the subquery merge([3,5], [2,4,6], Zs)., it believes that there are still several other viable alternatives for satisfying our original query:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X=:=Y, merge(Xs, Ys, Zs). merge([X|Xs], [Y|Ys], [Y|Zs]) :- Y\u0026lt;X, merge([X|Xs], Ys, Zs). merge(Xs, [], Xs). merge([], Ys, Ys). The result is that Prolog will have to use memory to remember those alternatives. As the lists that we ask Prolog to merge get longer and longer, that memory will have an impact on the system\u0026rsquo;s performance. However, we know that those other alternatives will never match and keeping them around is a waste. How do we know that? Well, if X \u0026lt; Y then it cannot be equal to Y and it certainly cannot be greater than Y. Moreover, the lists in the first two arguments cannot be empty. Overall, each of the possible rules for merge/3 are mutually exclusive. You can only choose one.\nIf there were a way to tell Prolog that those other possibilities are impossible after it encounters a matching rule that would save Prolog from having to keep them around. The good news is that there is!\nWe can use the cut (Links to an external site.) operator to tell Prolog that once it has \u0026ldquo;descended\u0026rdquo; down a particular path, there is no sense backtracking beyond that point to look for alternate solutions. The technical definition of a cut is\n Discard all choice points created since entering the predicate in which the cut appears. In other words, commit to the clause in which the cut appears and discard choice points that have been created by goals to the left of the cut in the current clause.\n Let\u0026rsquo;s rewrite our merge/3 predicate to take advantage of cuts and save some overhead:\nmerge([X|Xs], [Y|Ys], [X|Zs]) :- X\u0026lt;Y, !, merge(Xs, [Y|Ys], Zs). merge([X|Xs], [Y|Ys], [X|Zs]) :- X=:=Y, !, merge(Xs, Ys, Zs). merge([X|Xs], [Y|Ys], [Y|Zs]) :- Y\u0026lt;X, !, merge([X|Xs], Ys, Zs). merge(Xs, [], Xs) :- !. merge([], Ys, Ys) :- !. Returning to the definition of cut, in the first rule we are telling Prolog (through our use of the cut) to disregard all choice points created to the left of the !. In particular, we are telling Prolog to forget about the choice it made that X \u0026lt; Y. The result is that Prolog is no longer under the impression that there are other rules that are applicable. Visually, the situation resembles\nMerge Cut.png\nDr. Cutyll and Mr. Unify Cuts are not always so beneficial. In fact, their use in Prolog is somewhat controversial. A cut necessarily limits Prolog\u0026rsquo;s ability to backtrack. If the Prolog programmer uses a cut in a rule that is meant to be used declaratively (in order to generate values) and procedurally, then the cut may change the results.\nThere are two types of cuts. A green cut is a cut that does not change the meaning of a predicate. The cut that we added in the merge/3 predicate above is a green cut. A red cut is, technically speaking, a cut that is not a green cut. I know that\u0026rsquo;s a satisfying definition. Sorry. The implication, however, is that a red cut is a cut that changes the meaning of predicate.\n11/15/2021 Red Alert At the end of lecture on Friday, we discussed the two different types of cuts \u0026ndash; red and green. A green cut is one that does not alter the behavior of a Prolog program. A red cut does alter the behavior of a Prolog program. The implication was that red cuts are bad and green cuts are good. But, is this always the case?\nTo frame our discussion, let\u0026rsquo;s look at a Prolog program that performs a Bubble Sort on a list of numbers. The predicate, bsort/2, is defined like this:\nbsort(Unsorted, Sorted):- append(Left, [A, B | Right], Unsorted), B\u0026lt;A, append(Left, [B, A | Right], MoreSorted), bsort(MoreSorted, Sorted). bsort(Sorted, Sorted). The first rule handles the recursive case (when there is at least one list item out of order) and the second rule handles the base case (when the list is sorted). According to this definition, how does Prolog handle the query\nbsort([2,1,3,5], M). Prolog (as we are well aware) searches its rule top to bottom. The first rule that it sees is the recursive case and Prolog immediately opts for it. Opting for this case is not without consequences \u0026ndash; a choice point is created. Why? Because Prolog made the choice to attempt to satisfy the query according to the first rule and not the (just as applicable) second rule! Let\u0026rsquo;s call this choice point A.\nNext, Prolog attempts to unify the subquery append(Left, [A, B | Right], Unsorted). The first (of many) unifications that append/3 generates is Left = [], A = 2, B = 1, Right = [3, 5]. By continuing with this particular unification, Prolog creates another choice point. For what reason? Well, Prolog knows that append/3 could generate other potential unifications and it will need to check those to see if they, too, satisfy the original query! Let\u0026rsquo;s call this choice point B. Next, Prolog checks whether A is less than B \u0026ndash; it is. Prolog continues in a like manner to satisfy the final two subgoals of the rule.\nWhen Prolog does satisfy those, it has generated a sorted version of [2,1,3,5]. It reports that result to the querier. Great! There\u0026rsquo;s only one problem. There are still choice points that Prolog wants to explore. In particular, there are choice points A and B. In this case, we can forget about choice point B because there are no other unifications of append/3 that meet the criteria for A\u0026lt;B in Unsorted. In other words, visually the situation looks like this:\n If the querier is using bsort/2 declaratively, this scenario is a problem: Prolog will backtrack to choice point A and then consider the base case rule for bsort/2. In other words, Prolog will also generate\n[2,1,3,5] as a unification! This is clearly not right. What\u0026rsquo;s more, the Prolog programmer could query for\nbsort([3,2,1,4,5], Sorted). in which choice point B will have consequences. Run this query for yourself (using trace), examine the results, and make sure you understand why the results are generated.\nSo, what are we to do? cut/0 (Links to an external site.) to the rescue! Logically speaking, once our bsort/3 rule finds a single element out of order and we adjust that element, the recursive call to itself will handle ordering any other unordered elements. In other words, we can forget any earlier choice points! This is exactly what the cut is intended for!\nLet\u0026rsquo;s rewrite bsort/3 using cuts and see if our problem is sorted (see what I did there?):\nbsort(Unsorted, Sorted):- append(Left, [A, B | Right], Unsorted), B\u0026lt;A, !, append(Left, [B, A | Right], MoreSorted), bsort(MoreSorted, Sorted). bsort(Sorted, Sorted). And now let\u0026rsquo;s see how our earlier troublesome queries perform:\n?- bsort([2,1,3,5], Sorted). Sorted = [1, 2, 3, 5]. ?- bsort([3,2,1,4,5], Sorted). Sorted = [1, 2, 3, 4, 5]. Amazing!\nHere\u0026rsquo;s the rub: The version of our Prolog program with the cut gave different results than the version without. Is this cut a green or a red cut? That\u0026rsquo;s right, it\u0026rsquo;s a red cut. I guess there are such things as good red cuts after all!\nThe Fundamentals As we have said in the past, there is a theoretical underpinning for each of the programming paradigms that we have studied this semester. Logic programming is no different. The theory behind logic programming is first-order predicate logic. Predicate logic is an extension of propositional logic. Propositional logic is based on the evaluation of propositions.\nA proposition is simply any statement that can be true or false. For example,\n Will is a programmer. Alicia is a programmer. Sam is a programmer.  Those statements can be true or false. In propositional logic we can build more complicated propositions from existing propositions using logical connectives like and, or, not, and implication (if \u0026hellip; then). Each of these has an associated truth table to determine the truth of two combined propositions.\nLook closely at the example propositions above and you will notice an underlying theme: they all do with someone (let\u0026rsquo;s call them x) being a programmer. In propositional logic, our ability to reason using that underlying theme is impossible. We can only work with the truth of the statement as a whole.\nIf we add to propositional logic variables, constants, and quantifiers then we get predicate logic and we are able to reason more subtly. Although you might argue that propositional logic has variables, everyone can agree that they are limited \u0026ndash; they can only have two values, true and false. In first-order predicate logic, variables can have domains other than just {T, F}. That\u0026rsquo;s already a huge step!\nQuantifiers \u0026ldquo;define\u0026rdquo; variables and \u0026ldquo;constrain\u0026rdquo; their possible values. There are two quantifiers in first-order predicate logic \u0026ndash; the universal and the existential. The universal is the \u0026ldquo;for all\u0026rdquo; (aka \u0026ldquo;every\u0026rdquo;) quantifier. We can use the universal quantifier to write statements (in logic) like \u0026ldquo;Every person is a Bearcats fan.\u0026rdquo; Symbolically, we write the universal quantifier with the \\(\\forall\\). We can write our obvious statement from above in logic like\n.org-center { margin-left: auto; margin-right: auto; text-align: center; }  \\(\\forall x(person(x) \\Longrightarrow fan(x, bearcats))\\)\n Now we are talking! As for the existential quantifier, it allows us to write statements (in logic) like \u0026ldquo;There is some person on earth that speaks Russian.\u0026rdquo; Symbolically, we write the existential quantifier with the \\(\\exists\\). We can write the statement about our Russian-speaking person as\n.org-center { margin-left: auto; margin-right: auto; text-align: center; }  \\(\\exists x(person(x) \\land speaks(x, russion))\\)\n How are quantifiers embedded in Prolog? Variables in Prolog queries are existentially qualified \u0026ndash; \u0026ldquo;Does there exist …?\u0026ldquo;​ Variables in Prolog rules are universally quantified \u0026ndash; \u0026ldquo;For all ….\u0026rdquo;\nIn first-order predicate logic, there are such things as Boolean-valued functions. This is familiar territory for us programmers. A Boolean-valued function is a function that has 0 or more parameters and returns true or false.\nWith these definitions, we can now define predicates: A predicate is a proposition in which some Boolean variables are replaced by Boolean-valued functions and quantified expressions. Let\u0026rsquo;s look at an example.\n.org-center { margin-left: auto; margin-right: auto; text-align: center; }  \\(p \\Longrightarrow q\\)\n is a proposition where p and q are boolean variables. Replace p with the Boolean-valued function \\(is\\_teacher(x)\\) and q with the quantified expression \\(\\exists y(student(x) \\land teaches(x, russion))\\) and we have the predicate\n.org-center { margin-left: auto; margin-right: auto; text-align: center; }  \\(is\\_teacher(x) \\Longrightarrow \\exists y(student(x) \\land teaches(x,y))\\)\n There is only one remaining question: Why is it called first-order predicate logic and not, say, higher-order predicate logic? \u0026ldquo;First-order\u0026rdquo; here indicates that the predicates in this logic cannot manipulate or reason about predicates themselves. Does this sound familiar? Imperative languages can define functions but they cannot reason about functions themselves while functional languages can!\n11/17/2021 So far in this course we have covered lots of material. We\u0026rsquo;ve learned lots of definitions, explored lots of concepts and programmed in languages that we\u0026rsquo;ve never seen before. In all that time, though, we never really got to the bottom of one thing: What exactly is a language? In the final module of this course, we are going to cover exactly that!\nBack to Basics Let\u0026rsquo;s think way back to August 23 and recall two very important definitions: syntax and semantics. On that day we defined semantics as the effect of each statement\u0026rsquo;s execution on the program\u0026rsquo;s operation. We defined syntax as the rules for constructing structurally valid (note: valid, not correct) computer programs in a particular language. There\u0026rsquo;s that word again \u0026ndash; language.\nBefore starting to define language, let me offer you two alternate definitions for syntax and semantics that draw out the distinction between the two:\n The syntax of a programming language specifies the form of its expressions, statements and program units. The semantics of a programming language specifies the meaning of its expressions, statements and program units.  It is interesting to see those two definitions side-by-side and realize that they are basically identical with the exception of a single word! One final note about the connection between syntax and semantics before moving forward: Remember how a well-designed programming language has a syntax that is evocative of meaning. In other words, a well-designed language might allow variables to contain a symbol like ? which would allow the programmer to indicate that it holds a Boolean.\nBefore we can specify a syntax for a programming language, we need to specify the language itself. In order to do that, we need to start by defining the language\u0026rsquo;s alphabet \u0026ndash; finite set of characters that can be used to write sentences in that language. We usually denote the alphabet of a language with the \\(\\sum\\). It is sometimes helpful to denote the set of all the possible sentences that can be written using the characters in the alphabet. We usually denote that with \\(\\sum\\). For example, say that \\(sum = \\{a, b\\}\\), then \\(\\sum = \\{a, b, aa, ab, ba, aaa, aab, aba, abb, \u0026hellip;\\}\\). Notice that even though \\(|\\sum|\\) is finite (that is, the number of elements in is finite), \\(|\\sum| = \\infty\\).\nThe alphabet of the C++ programming language is large, but it\u0026rsquo;s not infinite. However, the set of sentences that can be written using that alphabet is. But, as we all learn early in our programming career, just because you can write out a program using the valid symbols in the C++ alphabet does not mean the program is syntactically valid. The very job of the compiler is to distinguish between valid and invalid programs, right?\nLet\u0026rsquo;s call the language that we are defining \\(L\\) and say that it\u0026rsquo;s alphabet is \\(\\sum\\). \\(L\\) can be thought of as the set of all valid sentences in the language. Therefore, every sentence that is in \\(L\\) is in \\(\\sum\\) \u0026ndash; \\(L \\subseteq \\sum\\).\n Look closely at the relationship between \\(\\sum\\) and \\(L\\). While \\(L\\) never includes a sentence that is not included in \\(\\sum\\), they can be identical! Think of some languages where any combination of characters from its alphabet are valid sentences! Can you come up with one?\nThe Really Bad Programmer So, how can we determine whether a sentence made up of characters from the alphabet is in the language or not? We have to be able to answer this question \u0026ndash; one of the fundamental jobs of the compiler, after all, is to do exactly that. Why not just write down every single valid sentence in a giant chart and compare the program with that list? If the program is in the list, then it\u0026rsquo;s a valid program! That\u0026rsquo;s easy.\nNot so fast. Aren\u0026rsquo;t there an infinite number of valid C++ programs?\nint main() { if (true) { if (true) { if (true) { ... std::cout \u0026lt;\u0026lt; \u0026#34;Hello, World.\u0026#34;; } } } } Well, dang. There goes that idea.\nI guess we need a concise way to specify how to describe sentences that are in the language and those that are not. Before we start to think about ways to do that, let\u0026rsquo;s think back to Prolog. One of the really neat things about Prolog was the ability to specify something that had, at the same time, an ability to recognize and generate valid solutions to a set of constraints. Even in our simplest Prolog example, we could write down a set of facts and the language could determine whether a student took a particular class (giving us a yes/no answer) or generate a list of the people who were taking a particular class!\nIt would be great to have a tool that does the same thing for languages \u0026ndash; a common description that allows us to create something that recognizes and generates valid sentences in the language. We will do exactly that!\nLanguage Classes The famous linguist Noam Chomsky was the first to recognize how there is a hierarchy of languages. The hierarchy is founded upon the concept of how easy/hard it is to write a concise definition for the language\u0026rsquo;s valid sentences.\n Each level of Chomsky\u0026rsquo;s Hierarchy, as it is called, contains the levels that come before it. Which languages belong to which level of the hierarchy is something that you will cover more in CS4040. (Note: The languages that belong to the Regular level can be specified by regular expressions (Links to an external site.). Something I know some of you have written before!)\nFor our purposes, we are going to be concerned with those languages that belong to the Context-Free level of the hierarchy.\nContext-Free Grammars The tool that we can use to concisely specify a Context-Free language is called a Context-Free Grammar. Precisely, a Context-Free Grammar, , is a set of productions , a set of terminal symbols, , a set of non-terminal symbols, , one of which is named\nand is known as the start symbol.\nThat\u0026rsquo;s a ton to take in. The fact of the matter, however, is that the vocabulary is intuitive once you see and use a grammar. So, let\u0026rsquo;s do that. We will define a grammar for listing/recognizing all the positive integers:\n Now that we see an example of a grammar and its parts and pieces, let\u0026rsquo;s \u0026ldquo;read\u0026rdquo; it to help us understand what it means. Every valid integer in the language can be derived by writing down the start symbol and iteratively replacing every non-terminal according to a production until there are only terminals remaining! Again, that sounds complicated, but it\u0026rsquo;s really not. Let\u0026rsquo;s look at the derivation for the integer 142:\n We start with the start symbol Integer and use the first production to replace it with one of the two options in the production. We choose to use the second part of the production and replace Integer with Integer Digit. Next, we use a production to replace Integer with Integer Digit again. At this point we have Integer Digit Digit. Next, we use one of the productions to replace Integer with Digit and we are left with Digit Digit Digit. Our next move is to replace the left-most Digit non-terminal with a terminal \u0026ndash; the 1. We are left with 1 Digit Digit. Pressing forward, we replace the left-most Digit with 4 and we come to 14Digit. Almost there. Let\u0026rsquo;s replace Digit with 2 and we get 142. Because 142 contains only terminals, we have completed the derivation. Because we can derive 142 from the grammar, we can confidently say that 142 is in the language described by the grammar.\n11/19/2021 (Context-free) Grammars (CFG) are a mathematical description of a language-generation mechanism. Every grammar defines a language. The strings in that language are those that can be derived from the grammar\u0026rsquo;s start symbol. Put another way, a CFG describes a process by which all the sentences in a language can be enumerated, or generated.\nDefining a way to generate all the sentences in a grammar is one way to specify a language. Another way to define a language is to build a tool that separates sentences that are in the language from sentences that are not in the language. This tool is known as a recognizer. There is a relationship between recognizers and generators and we will explore that in future lectures.\nGrammatical Errors On Wednesday we worked with a grammar that purported to describe all the strings that looked like integers:\n We performed a derivation using its productions to prove (by construction) that 142 is in the language. But, let\u0026rsquo;s consider this derivation:\n We have just proven (again, by construction) that 012 is an integer. This seems funny. A number that starts with 0 (that is not just a 0) should not be deemed an integer. Let\u0026rsquo;s see how we can fix this problem.\nIn order to guarantee that we cannot derive a number that starts with a 0 and deem it an integer, we will need to add a few more productions. First, let\u0026rsquo;s reconsider the production for the start symbol Integer. At the very highest level, we know that 0 is an integer. So, our start symbol\u0026rsquo;s production should handle that case.\nWith the base case accounted for, we next consider that an integer cannot start with a zero, but it can start with any other digit between 1 and 9. It seems handy to have a production for a non-terminal that expands to the terminals 1 through 9. We will call that non-terminal Nzd for non-zero digits. After the initial non-zero digit, an integer can contain zero or more digits between 0 and 9 (we can call this the rest of the integer). For brevity, we probably want a production that will expand to all the digits between 0 and 9. Let\u0026rsquo;s call that non-terminal Zd for zero digits. We\u0026rsquo;ll define it either as a 0 or anything in Nzd. If we put all of this together, we get the following grammar:\n Let\u0026rsquo;s look at a few derivations to see if this gets the job done. Is 0 an integer?\n And a single-digit integer?\n How about an integer between 10 and 100, inclusive?\n We are really cooking. Let\u0026rsquo;s try one more. Is a number greater than 100 derivable?\n Think about this grammar and see if you can see any problems with it. Are there integers that cannot be derived? Are there sentences that can be derived that are not integers?\nTrees or Derivations Let\u0026rsquo;s take a look at a grammar that will generate sentences that are simple mathematical expressions using the + and * operators and the numbers 0 through 9:\n Until now we have used proof-by-construction through derivations to show that a particular string is in the language described by a particular grammar. What\u0026rsquo;s really cool is that any derivation can also be written in the form of a tree. The two representations contain the same information \u0026ndash; a proof that a particular sentence is in a language. Let\u0026rsquo;s look at the derivation of the expression 1 + 5 * 2:\n There\u0026rsquo;s only one problem with this derivation: Our choice of which production to apply to expand the start symbol was arbitrary. We could have just as easily used the second production and derived the expression 1 + 5 * 2:\n This is clearly not a good thing. We do not want our choice of productions to be arbitrary. Why? When the choice of the production to expand is arbitrary, we cannot \u0026ldquo;encode\u0026rdquo; in the grammar any semantic meaning. (We will learn how to encode that semantic meaning below). A grammar that produces two or more valid parse trees for a particular sentence is known as an ambiguous grammar.\nLet Me Be Clear The good news is that we can rewrite the ambiguous grammar for mathematical expressions from above and get an unambiguous grammar. Like the way that we rewrote the grammar for integers, rewriting this grammar will involve adding additional productions:\n Using this grammar we have the following parse tree for the expression 1 + 5 * 2:\n Before we think about how to encode semantic meaning in to a parse tree, let\u0026rsquo;s talk about the properties of a parse tree. The root node of the parse tree is always the start symbol of the grammar. The internal nodes of a parse tree are always non-terminals. The leaves of a parse tree are always terminals.\nMaking Meaning If we adopt a convention that reading a parse tree occurs through a depth-first in-order traversal, we can add meaning to these beasts and their associated grammars. First, we can see how associativity is encoded: By writing all the productions so that \u0026ldquo;recursion\u0026rdquo; happens on the left side of any terminals (a so-called left-recursive production), we will arrive at a grammar that is left-associative. The converse is true \u0026ndash; the productions whose recursion happens on the right side of any terminals (a right-recursive production) specifies right associativity. Second, we can see how precedence is encoded: The further away a production is from the start symbol, the higher the precedence. In other words, the precedence is inversely related to the distance from the start symbol. All alternate options for the same production have equal precedence.\nLet\u0026rsquo;s look back at our grammar for Expr. A Term and an addition operation have the same precedence. A Factor and a multiplication operation have the same precedence. The production for an addition operation is left-recursive and, therefore, the grammar specifies that addition is left associative. The same is true for the multiplication operation.\n11/22/2021 Before reading this Daily PL, please re-read the one from Friday, 11/19/2021. The content has been updated since it was originally published to reflect certain errors with the \u0026ldquo;better\u0026rdquo; Integer grammar we developed in class. Thanks, especially, to Jeroen for making sure that I got the grammar correct!\nMaking Meaning In an earlier part of the class we talked about how to describe the dynamic semantics of a program. We worked closely with operational semantics but saw other examples, too (axiomatic and denotational). Although we said that what we are studying in this module is the syntax of a language and not its semantics, a language may have static semantics that the compiler can check during syntax analysis. Syntax analysis is done using a tool known as the parser.\nRemember that syntax and syntax analysis is only concerned with valid programs. If the program, considered as a string of letters of the language\u0026rsquo;s alphabet, can be derived from the language grammar\u0026rsquo;s start symbol, then the program is valid. We all agreed that was the limit of what a syntax analyzer could determine.\nOne form of output of a parser is a parse tree. It is possible to apply \u0026ldquo;decoration\u0026rdquo; to the parse tree in order to verify certain extra-syntactic properties of a program at the time that it is being parsed. These properties are known as a language\u0026rsquo;s static semantics. More formally, Sebesta defines static semantics as the rules for a valid program in a particular language that are difficult (or impossible!) to encode in a grammar. Checking static semantics early in the compilation process is incredibly helpful for programmers as they write their code and allows the stages of the compilation process after syntax analysis to make more sophisticated assumptions about a program\u0026rsquo;s validity.\nOne example of static semantics of a programming language has to do with its type system. Checking a program for type safety is possible at the time of syntax analysis using an attribute grammar. An attribute grammar is an extension of a CFG that adds (semantic) information to its terminals and nonterminals.​ This information is known as attributes. Attached to each production are attribute calculation functions. At the time the program is parsed, the attribute calculation functions are evaluated every time that the associated production is used in a derivation and the results of that invocation are stored in the nodes of the parse tree that represent terminals and nonterminals. Additionally, in an attribute grammar, each production can have a set of predicates. Predicates are simply Boolean-valued functions. When a parser attempts to use a production during parsing, it\u0026rsquo;s not just the attribute calculation functions that are invoked \u0026ndash; the production\u0026rsquo;s predicates are too. If any of those predicates returns false, then the derivation fails. An Assignment Grammar\nTo start class, we looked at a snippet of an industrial-strength grammar \u0026ndash; the one for the C programming language. The snippet we saw concerned the assignment expression: There are lots of details in the grammar for an assignment statement in C and not all of them pertain to our discussion. So, instead of using that grammar of assignment statements, we\u0026rsquo;ll use a simpler one:\n Assume that this is part of a larger grammar for a complete, statically typed programming language. In the subset of the grammar that we are going to work with, there are three terminals: A, B and C. These are variable names available for the programmer in our language. As a program in this language is parsed, the compiler builds a mapping between variables and their types (known as the symbol table), adding an entry for each new variable declaration it encounters. The compiler can \u0026ldquo;lookup\u0026rdquo; the type of a variable using its name thanks to the lookup function.\nOur hypothetical language contains only two numerical types: int and real. A variable can only be assigned the result of an expression if that expression has the same type as the variable. In order to determine the type of an expression, our language adheres to the following rules:\n Let\u0026rsquo;s assume that we are looking at the following program written in our hypothetical language:\nint A; real B; A = A + B; The declarations of the variables A and B are handled by parts of the grammar that we are not showing in our example. Again, when those declarations are parsed, an entry is made in the symbol table so that variable names can be mapped to types. Let\u0026rsquo;s derive A = A + B; using the snippet of the grammar shown above:\n Great! The program passes the syntactic analysis so it\u0026rsquo;s valid!\n Right?\nThis Grammar Goes To 11 Wrong. According to the language\u0026rsquo;s type rules, we can only assign to variables that have the same type as the expression being assigned. The rules say that A + B is a real (a.ii). A is declared as an int. So, even though the program parses, it is invalid!\nWe can solve this using attribute grammars and that\u0026rsquo;s exactly what we are going to do! For our Assign grammar, we will add the following attributes to each of the terminals and nonterminals:\nexpected_type: The type that is expected for the expression. actual_type: The actual type of the expression.\nThe values for the attributes are set according to functions. An attribute whose calculation function uses attribute values from only its children and peer nodes in the parse tree is known as a synthesized attribute. An attribute whose calculation function uses only attribute values from its parent nodes in the parse tree is known as an inherited attribute.\n Let\u0026rsquo;s define the attribute calculation functions for the expected_type and actual_type attributes of the Assign grammar:\n For this production, we can see that the expression\u0026rsquo;s expected_type attribute is defined according to the variable\u0026rsquo;s actual_type which means that it is an inherited attribute.\n For this production, we can see that the expression\u0026rsquo;s actual_type attribute is defined according to the variable\u0026rsquo;s actual_type which means that it is a synthesized attribute.\nAnd now for the most complicated (but not complex) attribute calculation function definition:\n Again, we can see that the expression\u0026rsquo;s actual_type attribute is defined according to its children nodes \u0026ndash; the actual_type of the two variables being added together \u0026ndash; which means that it is a synthesized attribute.\nIf you are thinking that the attribute calculation functions are recursive, you are exactly right! And, you can probably see a problem straight ahead. So far the attribute calculation functions have relied on attributes of peer, children and parent nodes in the parse tree to already have values. Where is our base case?\nGreat question. There\u0026rsquo;s a third type of attribute known as an intrinsic attribute. An intrinsic attribute is one whose value is calculated according to some information outside the parse tree. In our case, the actual_type attribute of a variable is calculated according to the mapping stored in the symbol table and accessible by the lookup function that we defined above.\n That\u0026rsquo;s all for the definition of the attribute calculation functions and is all well and good. Unfortunately, we still have not used our attributes to inform the parser when a derivation has gone off the rails by violating the type rules. We\u0026rsquo;ll define these guardrails predicate functions and show the complete attribute grammar at the same time:\n The equalities after the checkmarks are the predicates. We can read them as \u0026ldquo;If the actual type of the expression is the same as the expected type of the predicate, then the derivation (parse) can continue. Otherwise, it must fail because the assignment statement being parsed violates the type rules.\u0026rdquo; Put The Icing On The Cookie\nThe process of calculating the attributes of a production during parsing is known as decorating the parse tree. Let\u0026rsquo;s look at the parse tree from the assignment statement A = A + B; and see how it is decorated:\n 11/29/2021 Recognizing vs Generating We have talked at length about how, depending on your vantage point, you can look at predicates in Prolog as either declarative or procedural. Viewed from a declarative perspective, the predicates will generate all the values of a variable that will make the predicate true. Viewed from the other direction, predicates look more like traditional boolean-valued functions in a procedural (or OOP) programming language. The dichotomy between the declarative and procedural view has parallels in syntax and grammars. From one end of the PL football stadium, grammars are generators; from the other endzone they are recognizers.\nWe have seen the power of the derivation and the parse tree to generate strings that are in a language L defined by a grammar G. We can create a valid sentence in language L by repeated application of the productions in G to the sentential forms derived from G\u0026rsquo;s start symbol. Applying all the productions in all possible combinations will eventually enumerate all valid strings in the language L (don\u0026rsquo;t hold your breath for that process to finish!).\nThe only problem is that (with one modern exception ), our compilers don\u0026rsquo;t actually generate source code for us! It is the programmer \u0026ndash; us! \u0026ndash; who writes the source code and the compiler that checks whether it is a valid program. There are obviously myriad ways in which a programmer can write an invalid program in a particular programming language and the compiler can detect all of them. However, the easiest invalid programs for the compiler to detect are the ones that are not syntactically correct.\nTo reiterate, (most) programming languages specify their syntax using a (context-free) grammar (CFG) \u0026ndash; the theoretical language L that we\u0026rsquo;ve talked about as being defined by a grammar G can actually be a programming language! Therefore, the source code for a program is technically just a particular sequence of terminals from L\u0026rsquo;s alphabet. For that sequence of terminals to be a valid program, it must be the final sentential form in some derivation following the productions of G.\nIn other words, the compiler is not a generator but rather a recognizer.\nParsers Recall from Chapter 2 of Sebesta (or earlier programming courses), the stages of compilation. The stage/tool that recognizes whether a particular sequence of terminals from a language\u0026rsquo;s alphabet is a valid program or not (the recognizer) is called parsing/a parser. Besides recognizing whether a program is valid, parsers convert the source code for a program written in a particular programming language defined according to a specific grammar into a parse tree.\n What\u0026rsquo;s sneaky is that the parsing process is really two processes: lexical analysis and syntax analysis. Lexical analysis turns the bytes on the disk into a sequence of tokens (and associated lexemes). Syntax analysis turns the tokens into a parse tree.\nWe\u0026rsquo;ve seen several examples of languages defined by grammars and those grammars contain productions, terminals and nonterminals. We haven\u0026rsquo;t seen any tokens, though, have we? Well, tokens are an abstract way to represent groups of bytes in a file of source code in an abstract manner. The actual bytes that are in a group that are bundled together stay associated with the token and are known as lexemes. Tokenizing the input makes the syntax analysis process easier. Note: Read Sebesta\u0026rsquo;s discussion about the reason for separating lexical analysis from syntax analysis in Section 4.1 on (approximately pg. 143). Turtles All The Way Down\nRemember the Chomsky Hierarchy of languages? Context-free languages can be described by CFGs. Slightly less complex languages are known as regular languages. Regular languages can be recognized by a logical apparatus known as a finite automata (FA). If you have ever written a regular expression then you have actually written a stylized FA. Cool, right?? You will learn more about FAs in CS4040, but for now it\u0026rsquo;s important to know the broad outlines of the definition of an FA because of the central role they play in lexical analysis. An FA is\n A (finite) set of states, S; A (finite) alphabet, A; A transition function, \\(f : S, A \\rightarrow S\\), that takes two parameters (the current state [an element in S] and an element from the alphabet) and returns a new state; A start state (one of the states in S); and A set of accepting states (a subset of the states in S).  Why does this matter? Because we can describe how to group the bytes on disk into tokens (and lexemes) using regular languages. You probably have a good idea about what is going on, but let\u0026rsquo;s dig in to an example \u0026ndash; that\u0026rsquo;ll help!\nLexical Analysis of Mathematical Expressions For the remainder of this edition, let\u0026rsquo;s go back to the (unambiguous) grammar of mathematical expressions:\n Here\u0026rsquo;s a syntactically correct expression with labeled tokens and lexemes:\n What do you notice? Well, the first thing that I notice is that in most cases, the lexeme value is actually, completely, utterly useless. For instance, what other logical grouping of bytes other than the one that represents the ) will be converted in to the CLOSE_PAREN token?\nThere is, however, one token whose lexeme is very important: the Id token. Why? Because that token can be any number! The actual lexeme of that Id makes a big difference when it comes time to actually evaluate the mathematical expression that we are parsing (if that is, indeed, the goal of the operation!).\nCertitude and Finitude Let\u0026rsquo;s take a stab at defining the FA that we can use to convert a stream of bytes on disk in to a stream of tokens (and associated lexemes). Let yourself slip in to a world where you are nothing more than a robot with a simple memory: you can be in one of two states. Depending on the state that you are in, you are able to perform some specific actions depending on the next character that you see. The two states are BEGIN and NUMBER. When you are in the BEGIN state and you see a ), +, *, or ( then you simply emit the appropriate token and stay in the BEGIN state. If you are in the BEGIN state and you see a digit, you simply store that digit and then change your state to NUMBER. When you are in the NUMBER state, if you see a digit, you store that digit after all the digits you\u0026rsquo;ve previously seen and stay in the same state. However, when you are in the NUMBER state and you see a ), +, *, or (, you must perform three actions:\n Convert the string of digits that you are remembering into a number and emit a Id token with the converted value as the lexeme; Emit the token appropriate to the value you just saw; and Reset your state to BEGIN.  For as long as there are bytes in the input file, you continue to perform those operations robotically. The operations that I just described are the FA\u0026rsquo;s state transition function! See how each operation depends on the current state and an input? Those are the parameters to the state transition function! And see how we specified the state we will be in after considering the input? That\u0026rsquo;s the function\u0026rsquo;s output! Woah!\nWhat\u0026rsquo;s amazing about all those words that I just wrote is that they can be turned into a picture that is far easier to understand:\n Make the connection between the different aspects of the written and graphical description above and the formal components of an FA, especially the state transition function. In the image above, the state transition function is depicted with the gray arrows!\nThe Last Step\u0026hellip; There\u0026rsquo;s just one more step \u0026hellip; we want to convert this high-level description of a tokenizer into actual code. And, we can! What\u0026rsquo;s more, we can use a technique we learned earlier this semester in order to do it! Consider that the tokenizer will work hand-in-hand with the syntax analyzer. As the syntax analyzer goes about its business of creating a parse tree, it will occasionally turn to the lexical analyzer and say, \u0026ldquo;Give me the next token!\u0026rdquo;. In between answers, it would be helpful if the tokenizer could maintain its state.\nIf that doesn\u0026rsquo;t sound like a coroutine, then I don\u0026rsquo;t know what does! Because a coroutine is dormant and not dead between invocations, we can easily program the tokenizer as a coroutine so that it remembers its state (either BEGIN or NUMBER) and other information (like the current digits that it is has seen [if it is in the NUMBER state] and the progress it has made reading the input file). Kismet!\nTo see such an implementation in Python, check here (Links to an external site.).\n12/1/2021 Peanut Butter and Jelly To reiterate, the goal of a parser is to convert the source code for a program written in a particular programming language defined according to a specific grammar into a parse tree. Parsing occurs in two parts: lexical analysis and syntax analysis. The lexical analyzer (what we studied in the previous lecture) converts the program\u0026rsquo;s source code (in the form of bytes on disk) into a sequence of tokens. The syntax analyzer, the subject of this lecture, turns the sequence of tokens in to a parse tree. The lexical analyzer and the syntax analyzer go hand-in-hand: As the syntax analyzer goes about its business of creating a parse tree, it will periodically turn to the lexical analyzer and say, \u0026ldquo;Give me the next token!\u0026rdquo;.\nWe saw algorithms for building a lexical analyzer directly from the language\u0026rsquo;s CFG. It would be great if we had something similar for the syntax analyzer. In today\u0026rsquo;s lecture we are going to explore just one of the many techniques for converting a CFG into code that will build an actual parse tree. There are many such techniques, some more general and versatile than others. Sebesta talks about several of these. If you take a course in compiler construction you will see those general techniques defined in detail. In this class we only have time to cover one particular technique for translating a CFG into code that constructs parse trees and it only works for a subset of all grammars.\nWith those caveats in mind, let\u0026rsquo;s dig in to the details!\nDescent Into Madness A recursive-descent parser is a type of parser that can be written directly from the structure of a CFG \u0026ndash; as long as that CFG meets certain constraints. In a recursive descent parser built from a CFG G, there is a subprogram for every nonterminal in G. Most of these subprograms are recursive. A recursive-descent parser builds a parse tree from the top down, meaning that it begins with G\u0026rsquo;s start symbol and attempts to build a parse tree that represents a valid derivation whose final sentential form matches the program\u0026rsquo;s tokens. There are other parsers that work bottom up, meaning that they start by analyzing a sentential form made up entirely of the program\u0026rsquo;s tokens and attempt to build a parse tree that that \u0026ldquo;reduces\u0026rdquo; to the grammar\u0026rsquo;s start symbol. That the subprograms are recursive and that the parse tree is built top down, the recursive-descrent parser is aptly named.\nWe mentioned before that there are limitations on the types of languages that a recursive-descent parser can recognize. In particular, recursive-descent parsers can only recognize LL grammars. Those are grammars whose parse trees represent a leftmost derivation and can be built from a single left-to-right scan of the input tokens. To be precise, the first L represents the left-to-right scan of the input and the second L indicates that the parser generates a leftmost derivation. There is usually another restriction \u0026ndash; how many lookahead tokens are available. A lookahead token is the next token that the lexical analyzer will return as it scans through the input. Limiting the number of lookahead tokens reduces the memory requirements of the syntax analyzer but restricts the types of languages that those syntax analyzers can recognize. The number of lookahead tokens is written after LL and in parenthesis. LL(1) indicates 1 token of lookahead. We will see the practical impact of this restriction later in this edition.\nAll of these words probably seem very arbitrary, but I think that an example will make things clear!\nOld Faithful Let\u0026rsquo;s return to the grammar for mathematical expressions that we have been examining throughout this module:\n We will assume that there are appropriately named tokens for each of the terminals (e.g, the ) token is CLOSE_PAREN) and that any numbers are tokenized as ID with the lexeme set appropriately.\nAccording to the definition of a recursive-descent parser, we want to write a (possibly recursive) subprogram for each of the nonterminals in the grammar. The job of each of these subprograms is to parse the the upcoming tokens into a parse tree that matches the nonterminal. For example, the (possibly recursive) subprogram for Expr, Expr, parses the upcoming tokens into a parse tree for an expression and returns that parse tree. To facilitate recursive calls among these subprograms, each subprogram returns the root of the parse tree that it builds. The parser begins by invoking the subprogram for the grammar\u0026rsquo;s start symbol. The return value of that function call will be the root node of the parse tree for the entire input expression. Any recursive calls to other subprograms from within the subprogram for the start symbol will return parts of that overall parse tree.\nI am sure that you see why each of the subprograms usually contains some recursive function calls \u0026ndash; the nonterminals themselves are defined recursively.\nHow would we write such a (possibly recursive) subprogram to build a parse tree rooted at a Factor from a sequence of tokens?\nThere are two productions for a Factor so the first thing that the Factor subprogram does is determine whether it is parsing, for example, (5+2) \u0026ndash; a parenthesized expression \u0026ndash; or 918 \u0026ndash; a simple ID. In order to differentiate, the function simply consults the lookahead token. If that token is an open parenthesis then it knows that it is going to be evaluating the production Factor \\(\\rightarrow ( Expr )\\). On the other hand, if that token is an ID, then it knows that it is going to be evaluating the production Factor \\(\\rightarrow\\) id. Finally, if the current token is neither an open parenthesis nor an ID, then that\u0026rsquo;s an error!\nLet\u0026rsquo;s assume that the current token is an open parenthesis. Therefore, Factor knows that it should be parsing the production Factor \\(\\rightarrow ( Expr )\\). Remember how we said that in a recursive-descent parser, each nonterminal is represented by a (possibly recursive) subprogram? Well, that means that we can assume there is a subprogram for parsing an Expr (though we haven\u0026rsquo;t yet defined it!). Let\u0026rsquo;s call that mythical subprogram Expr. As a result, the Factor subprogram can invoke Expr which will return a parse tree rooted at that expression. Pretty cool! Before continuing to parse, Factor will check Expr\u0026rsquo;s return value \u0026ndash; if it is an error node, then parsing stops and Factor simply returns that error.\nOtherwise, after successfully parsing an expression (by invoking Expr) the parser expects the next token to be a close parenthesis. So, Factor checks that fact. If everything looks good, then Factor simply returns the node generated by Expr \u0026ndash; there\u0026rsquo;s no need to generate another node that just indicates an expression is wrapped in parenthesis. If the token after parsing the expression is not a close parenthesis, then Factor returns an error node.\nNow, what happens if the lookahead token is an ID? That\u0026rsquo;s simple \u0026ndash; Factor will just generate a node for that ID and return it!\nFinally, if neither of those is true, Factor simply returns an error node.\nLet\u0026rsquo;s make this a little more concrete by writing that in pseudocode. We will assume the following are defined:\n Node(T, X, Y, Z \u0026hellip;): A polymorphic function that generates an appropriately typed node (according to T) in the parse tree that \u0026ldquo;wraps\u0026rdquo; the tokens X, Y, Z, etc. We will play fast and loose with this notation. Error(X): A function that generates an error node because token X was unexpected \u0026ndash; an error node in the final parse tree will generate a diagnostic message. tokenizer(): A function that returns and consumes the next token from the lexical analyzer. lookahead(): A function that returns the lookahead token.   def Factor: if lookhead() == OPEN_PAREN: # Parsing a ( Expr ) # # Eat the lookahead and move forward. curTok = nextToken() # Build a parse tree rooted at an expression, # if possible. nestedExpr = Expr() # There was an error parsing that expression; # we will return an error! if type(nestedExpr) == Error: return nestedExpr # Expression parsing went well. We expect a ) # now. if lookahead() == CLOSE_PAREN: # Eat that close parenthesis. nextToken() # Return the root of the parse tree of the # nested expression. return nestedExpr else: # We expected a ) and did not get it. return Error(lookahead()) else if lookahead() == ID: # Parsing a ID # curTok = nextToken() return Node(curTok) else: # Parsing error! return Error(lookahead()) Writing a function to parse a Factor is relatively straightforward. To get a sense for what it would be like to parse an expression, let\u0026rsquo;s write a part of the Expr subprogram:\ndef Expr: ... leftHandExpr = Expr() if type(leftHandExpr) == Error: return leftHandExpr if lookahead() != PLUS: curTok = nextToken() return Error(curTok) rightHandTerm = Term() if type(rightHandTerm) == Error: return rightHandTerm return Node(Expr, leftHandExpr, rightHandTerm) ... What stands out is an emerging pattern that each of the subprograms will follow. Each subprogram is slowly matching the items from the grammar with the actual tokens that it sees. The subprogram associated with each nonterminal parses the nonterminals used in the production, \u0026ldquo;eats\u0026rdquo; the terminals in those same productions and converts it all into nodes in a parse tree. The subprogram that calls subprograms recursively melds together their return values into a new node that will become part of the overall parse tree, one level up.\nWe Are Homefree I don\u0026rsquo;t know about you, but I think that\u0026rsquo;s pretty cool \u0026ndash; you can build a series of collaborating subprograms named after the nonterminals in a grammar that call each other and, bang!, through the power of recursion, a parse tree is built! I think we\u0026rsquo;re done here.\nOr are we?\nLook carefully at the definition of Expr given in the pseudocode above. What is the name of the first subprogram that is invoked? That\u0026rsquo;s right, Expr. When we invoke Expr again, what is the first subprogram that is invoked? That\u0026rsquo;s right, Expr again. There\u0026rsquo;s no base case \u0026ndash; this spiral will continue until there is no more space available on the stack!\nIt seems like we may have run head-on into a fundamental limitation of recursive-descent parsing: The grammars that it parses cannot contain productions that are left recursive. A production A \\(\\rightarrow \\ldots\\) is (indirect) left recursive \u0026ldquo;when A derives itself as its leftmost symbol using one or more derivations.\u0026rdquo; In other words, \\(A \\rightarrow^{+} A \\ldots \\) is indirectly recursive where \\(\\rightarrow^{+} \\)indicates one or more productions. For example, the production for A in grammar\n is indirect left recursive because A → B → A \\mathtt{b}.\nA production A \\(\\rightarrow \\ldots \\)is direct left recursive when A derives itself as its leftmost symbol using one derivation (e.g., \\(A \\rightarrow A \\ldots\\). The production for Expr in our working example is direct left recursive.\nWhat are we to do?\nNote: The definitions of left recursion are taken from:\nAllen B Tucker and Robert Noonan. 2006. Programming Languages (2nd. ed.). McGraw-Hill, Inc., USA.\nFormalism To The Rescue Stand back, we are about to deploy math!\n There is an algorithm for converting productions that are direct-left recursive into productions that generate the same languages and are not direct-left recursive. In other words, there is hope for recursive-descent parsers yet! The procedure is slightly wordy, but after you see an example it will make perfect sense. Here\u0026rsquo;s the overall process:\nFor the rule that is direct-left recursive, A, rewrite all productions of A as \\(A \\rightarrow A\\alpha_1 | A\\alpha_2 | \\ldots | A\\alpha_n | \\beta_1 | \\ldots | \\beta_n \\) where all (non)terminals β_1 \\ldots β_n are not direct-left recursive. Rewrite the production as\n\\(A \\rightarrow \\beta_{1}A' | \\beta_{2}A' | \\ldots | \\beta_{n}A' \\\\ A' \\rightarrow \\alpha_{1}A' | \\alpha_{2}A' | \\ldots | \\alpha_{n}A' | \\varepsilon\\)\nwhere \\(\\varepsilon\\) is the erasure rule and matches an empty token.\nI know, that\u0026rsquo;s hard to parse (pun intended). An example will definitely make things easier to grok:\nIn\n\\(Expr \\rightarrow Expr + Term | Term\\)\nA is Expr, \\(\\alpha_1is + Term, \\beta_1 is Term\\). Therefore,\nA \\(\\rightarrow \\beta_{1}A' \\\\ A' \\rightarrow \\alpha_{1}A' | \\varepsilon \\)\nbecomes\n\\(Expr \\rightarrow Term Expr' \\\\ Expr' \\rightarrow +TermExpr' | \\varepsilon\\)\nNo sweat! It\u0026rsquo;s just moving pieces around on a chessboard!\nThe Final Countdown We can make those same manipulations for each of the productions in the grammar in our working example and we arrive here:\n\\(\\mathit{Expr} \\rightarrow \\mathit{Term}\\mathit{Expr'} \\\\ \\mathit{Expr'} \\rightarrow + \\mathit{Term}\\mathit{Expr'} | \\epsilon \\\\ \\mathit{Term} \\rightarrow \\mathit{Factor}\\mathit{Term'} \\\\ \\mathit{Term'} \\rightarrow * \\mathit{Factor}\\mathit{Term'} | \\epsilon \\\\ \\mathit{Factor} \\rightarrow ( \\mathit{Expr} ) | \\mathit{id}\\)\nNow that we have a non direct-left recursive grammar we can easily write a recursive-descent parser for the entire grammar. The source code is available online and I encourage you to download and play with it!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/set-relations/",
	"title": "Set relations",
	"tags": [],
	"description": "",
	"content": "reflexive reflexive if, for every element \\(a \\in A\\) we have \\(aRa \\Rightarrow (a, a) \\in R\\)\n \\( A = \\{(a, a): a \\in A\\}\\)  Symmetric symmetric iff \\((x,y) \\in R \\wedge (y,x) \\in R\\)\nTransitive Iff R relates \\(a\\) to \\(b\\) and \\(b\\) to \\( c\\) then \\(a \\) relates to \\(c\\)\n \\(a \u0026lt; b \u0026lt; c \\rightarrow a \u0026lt; c\\) \\(a = b = c \\rightarrow a = c\\)  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/set-theory/",
	"title": "Standard Proof techniques",
	"tags": [],
	"description": "",
	"content": "Disproof by Counterexample Shows that a conjecture is not true by pointing out an example where the conjecture does not hold.\n No nickels 1 quarter + 5 pennies 3 dimes Greedy method is not appropriate with limited change  Proof by Contradiction Proof that the opposite cannot be true.\nSquare root of 2 is irrational  \\(\\sqrt 2 = a/b\\) \\(a/b\\) is simplified a or b or both must be odd (otherwise could be simplified) \\(2 = a^2/b^2\\) \\(a^2 = 2 * b^2\\) \\(a^2\\) must be even (2 times any number is even) \\(a\\) is even as well (odd times odd is odd) \\(a = 2 * k\\) where k is a / 2 \\(2 = (2 * k)^2/b^2 \\rightarrow b^2 = 2k^2\\) \\(b\\) is also odd by this method \\(a\\) and \\(b\\) cannot be odd \\(\\sqrt 2\\) cannot be rational  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/phys2001/test/",
	"title": "Test",
	"tags": [],
	"description": "",
	"content": "ah\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/trees/",
	"title": "Trees",
	"tags": [],
	"description": "",
	"content": " set of nodes first node is root every other node has a \u0026ldquo;parent\u0026rdquo; node  Two Trees  Every node that is not a leaf has 2 child nodes  Binary Trees  Every node has a maximum of 2 children  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/formulas-and-references/",
	"title": "Useful formulas and references",
	"tags": [],
	"description": "",
	"content": "Hard Drives Hard Drive Website Hard Drive Document from nitin\nFormulas  Disk access time = Seek Time + Rotational delay + Transfer time + Controller overhead + Queuing delay Average Disk Access Time = Average seek time + Average rotational delay + Transfer time + Controller overhead + Queuing delay Average Seek Time = 1/3 * Time taken for one full stroke  (Time taken to move from track 1 to track 1 + Time taken to move from track 1 to last track)/2 {0 + (k-1)t}/2 (k-1)t/2   Average Rotational Latency = 1/2 * Time taken for one full rotation Capacity of disk pack = Total number of surfaces * Number of tracks per surface * Number of sectors per track * storage capacity of one sector Formatting Overhead = Number of sectors * Overhead per sector Formatted Disk Space = Total disk space or capacity - formatting overhead Recording density or storage density = Capacity of track / circumference of the track Track Capacity = Recording density of the track * Circumference of the track Data Transfer Rate = Number of heads * Bytes that can be read in one full rotation * Number of rotations in one second = Number of heads * Capacity of one track * Number of rotations in one second Tracks per surface = (Outer radius - Inner radius) / Inter Track gap  Circuit reference  Logic Gates  Full Adder   Computer Arithmetic Computer arithemetic document from nitin\nMemory + Cache  Main Memory and Organization : credit to Robbie Schad Cache Notes Direct Mapping Examples Fully Associative Mapping Examples Set Associate Mapping Examples  Formulas Direct mapped cache  physical address size (bits) = TAG + Line Number + Block/Line Offset \\(\\text{MM(size in bytes)} = 2^{\\text{number Of Bits In Physical Address}} * 2^3\\) \\(\\text{BlockOffset(size in bytes)} = 2^{\\text{Bits In Block Offset}} * 2^3 \\) \\(\\text{number of lines} = 2^{\\text{bits in line number}} = \\frac{\\text{Cache size}}{\\text{Line Size}} \\) \\(\\text{tag directory size} = \\text{number of lines in cache * Number of bits in tag} = \\text{Number of Tags} * \\text{Tag size} \\)  Set Associative Mapped cache  physical address size (bits) = TAG + Set Number + Block/line Offset \\(2^\\text{Set Number (bits)} = \\frac{\\text{Lines in cache}}{\\text{Number of sets}}\\)  Fully associative cache  physical address size (bits) = TAG + Block offset  Pipelining pipelining document from nitin\nFormulas   \\(\\text{Speed Up (S)} = \\frac{\\text{Non-pipelined execution time}}{\\text{Pipelined execution time}}\\)\n  \\(\\text{Efficiency} = \\frac{\\text{Speed Up}}{\\text{Number of stages in Pipelined Architecture}}\\)\n  \\(\\text{Efficiency} = \\frac{\\text{Number of boxes utilized in phase time diagram}}{\\text{Number of boxes in phase time diagram}}\\)\n  \\(\\text{Throughput} = \\frac{\\text{Number of instructions executed}}{\\text{Total time taken}}\\)\n  Non-pipelined execution time = Total number of instructions * Time taken to execute one instruction = n * k clock cycles\n  Pipelined execution time\n= Time taken to execute first instruction + Time take to execute remaining instructions\n= 1 * k clock cycles + (n-1) * 1 clock cycle\n= (k + n-1) clock cycles\n  Cycle time = Maximum delay due to any stage + Delay due to its register\n  delay due to its register = latch delay\n  pipeline time for x tasks = Time taken for 1st task + Time taken for remaining tasks\n= number of phases * cycle time + (total tasks -1) * cycle time\n  MIPS  Mips Theory Mips Reference from Berkeley Mips Reference from Cburch MIPS DATAPATH MIPS PIPELINED DATAPATH  "
}]