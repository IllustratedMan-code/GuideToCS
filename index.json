[
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-1-2021/",
	"title": "10/1/2021",
	"tags": [],
	"description": "",
	"content": "Original is here.\nWe made it into October!! Spooky, spooky!\nCorrections Like in real newspapers (Links to an external site.), we are going to start including Corrections in each edition! We want to make sure that our reporters adhere to the highest standards:\nThe JVM will insert an implicit call to the to-be-instantiated class' default constructor (i.e., the one with no parameters) if the the to-be-constructed (sub)class does not do so explicitly. We\u0026rsquo;ll make this clear with an example:\nclass Parent { Parent() { System.out.println(\u0026#34;I am in the Parent constructor.\u0026#34;); } Parent(int parameter) { System.out.println(\u0026#34;This version of the constructor is not called.\u0026#34;); } } class Child extends Parent { Child() { /* * No explicit call to super -- one is automatically * injected to the parent constructor with no parameters. */ System.out.println(\u0026#34;I am in the Child constructor.\u0026#34;); } } public class DefaultConstructor { public static void main(String args[]) { Child c = new Child(); } } When this program is executed, it will print\nI am in the Parent constructor. I am in the Child constructor. The main function is instantiating an object of the type Child. We can visually inspect that there is no explicit call the super() from within the Child class' constructor. Therefore, the JVM will insert an implicit call to super() which actually invokes Parent().\nHowever, if we make the following change:\nclass Parent { Parent() { System.out.println(\u0026#34;I am in the Parent constructor.\u0026#34;); } Parent(int parameter) { System.out.println(\u0026#34;This version of the constructor is not called.\u0026#34;); } } class Child extends Parent { Child() { /* * No explicit call to super -- one is automatically * injected to the parent constructor with no parameters. */ super(1); System.out.println(\u0026#34;I am in the Child constructor.\u0026#34;); } } public class DefaultConstructor { public static void main(String args[]) { Child c = new Child(); } } Something different happens. We see that there is a call to Child\u0026rsquo;s superclass' constructor (the one that takes a single int-typed parameter). That means that the JVM will not insert an implicit call to super() and we will get the following output:\nThis version of the constructor is not called. I am in the Child constructor.\nThe C++ standard sanctions a main function without a return statement. The standard says: \u0026ldquo;if control reaches the end of main without encountering a return statement, the effect is that of executing return 0;.\u0026rdquo;\nA Different Way to OOP So far we have talked about OOP in the context of Java. Java, and languages like it, are called Class-based OOP languages. In a Class-based OOP, classes and objects exist in different worlds. Classes are used to define/declare\n the attributes and methods of an encapsulation, and the relationships between them.  From these classes, objects are instantiated that contain those attributes and methods and respect the defined/declared hierarchy. We can see this in the example given above: The classes Parent and Child define (no) attributes and (no) methods and define the relationship between them. In main(), a Child is instantiated and stored in the variable c. c is an object of type Child that contains all the data associated with a Child and a Parent and can perform all the actions of a Child and a Parent.\nNothing about Class-based OOP should be different than what you\u0026rsquo;ve learned in the past as you\u0026rsquo;ve worked with C++. There are several problems with Class-based OOP.\n The supported attributes and method of each class must be determined before the application is developed (once the code is compiled and the system is running, an object cannot add, remove or modify its own methods or attributes); The inheritance hierarchy between classes must be determined before the application is developed (once the code is compiled, changing the relationship between classes will require that the application be recompiled!).  In other words, Class-based OOP does not allow the structure of the Classes (nor their relationships) to easily evolve with the implementation of a system.\nThere is another way, though. It\u0026rsquo;s called Prototypal OOP. The most commonly known languages that use Prototypal OOP are JavaScript and Ruby! In Prototypal (which is a very hard word to spell!) OOP there is no distinction between Class and object \u0026ndash; everything is an object! In a Prototypal OOP there is a base object that has no methods or data attributes and every object is able to modify itself (its attributes and methods). To build a new object, the programmer simply copies from an existing object, the new object\u0026rsquo;s so-called prototype, and customizes the copied object appropriately.\nFor example, assume that there is an object called Car that has one attribute (the number of wheels) and one method (start). That object can serve as the prototype car. To \u0026ldquo;instantiate\u0026rdquo; a new Car, the programmer simply copies the existing prototypical car object Car and gives it a name, say, c. The programmer can change the value of c\u0026rsquo;s number of wheels and invoke its method, start. Let\u0026rsquo;s say that the same programmer wants to create something akin to a subclass of Car. The programmer would create a new, completely fresh object (one that has no methods or attributes), name it, say, Tesla, and link the new prototype Tesla object to the existing prototype car Car object through the prototype Tesla object\u0026rsquo;s prototype link (the sequence of links that connects prototype objects to one another is called a prototype chain). If a Tesla has attributes (range, etc) or methods (self_drive) that the prototype car does not, then the programmer would install those methods on the prototype Tesla Tesla. Finally, the programmer would \u0026ldquo;declare\u0026rdquo; that the Tesla object is a prototype Tesla.\n The blue arrows in the diagram above are prototype links. The orange lines indicate where a copy is made.\nHow does inheritance work in such a model? Well, it\u0026rsquo;s actually pretty straightforward: When a method is invoked or an attribute is read/assigned, the runtime will search the prototype chain for the first prototypical object that has such a method or attribute. Mic drop. In the diagram above, let\u0026rsquo;s follow how this would play out when the programmer calls start() on the Model 3 Instance. The Model 3 Instance does not contain a method named start. So, up we go! The Tesla Prototype Object does not contain that me either. All the way up! The Car Prototype Object, does, however, so that method is executed!\nWhat would it look like to override a function? Again, relatively straightforward. If a Tesla performs different behavior than a normal Car when it starts, the programmer creating the Tesla Prototype Object would just add a method to that object with the name start. Then, when the prototype chain is traversed by the runtime looking for the method, it will stop at the start method defined in the Tesla Prototype Object instead of continuing on to the start method in the Car Prototype Object. (The same is true of attributes!)\nThere is (at least) one really powerful feature of this model. Keep in mind that the prototype objects are real things that can be manipulated at runtime (unlike classes which do not really exist after compilation) and prototype objects are linked together to achieve a type of inheritance. With reference to the diagram above, say the programmer changes the definition of the start method on the Car Prototype Object. With only that change, any object whose prototype chain includes the Car Prototype Object will immediately have that new functionality (where it is not otherwise overridden, obviously) \u0026ndash; all without stopping the system!! How cool is that?\nHow scary is that? Can you imagine working on a system where certain methods you \u0026ldquo;inherit\u0026rdquo; change at runtime?\n OOP or Interfaces? Newer languages (e.g., Go, Rust, (new versions of) Java) are experimenting with new features that support one of the \u0026ldquo;killer apps\u0026rdquo; of OOP: The ability to define a function that takes a parameter of type A but that works just the same as long as it is called with an argument whose type is a subtype of A. The function doesn\u0026rsquo;t have care whether it is called with an argument whose type is A or some subtype of A because the language\u0026rsquo;s OOP semantics guarantee that anything the programmer can do with an object of type A, the programmer can do with and object of subtype of A.\nUnfortunately, using OOP to accomplish such a feat may be like killing a fly with a bazooka (or a laptop, like Alex killed that wasp today).\nInstead, modern languages are using a slimmer mechanism known as an interface or a trait. An interface just defines a list of methods that an implementer of that interface must support. Let\u0026rsquo;s see some real Go code that does this \u0026ndash; it\u0026rsquo;ll clear things up:\ntype Readable interface { Read() } This snippet defines an interface with one function (Read) that takes no parameters and returns no value. That interface is named Readable. Simple.\ntype Book struct { title string } This snippet defines a data structure called a Book \u0026ndash; such structs are the closest that Go has to classes.\nfunc (book Book) Read() { fmt.Printf(\u0026#34;Reading the book %v\\n\u0026#34;, book.title) } This snippet simply says that if variable b is of type Book then the programmer can call b.Read(). Now, for the payoff:\nfunc WhatAreYouReading(r Readable) { r.Read() } This function only accepts arguments that implement (i.e., meet the criteria specified in the definition of) the Readable interface. In other words, with this definition, the code in the body of the function can safely assume that it can can call Read on r. And, for the encore:\nbook := Book{title: \u0026#34;Infinite Jest\u0026#34;} WhatAreYouReading(book) This code works exactly like you\u0026rsquo;d expect. book is a valid argument to WhatAreYouReading because it implements the Read method which, implicitly, means that it implements the Readable interface. But, what\u0026rsquo;s really cool is that the programmer never had to say explicitly that Book implements the Readable interface! The compiler checks automatically. This gives the programmer the ability to generate a list of only the methods absolutely necessary for its parameters to implement to achieve the necessary ends \u0026ndash; and nothing unnecessary. Further, it decouples the person implementing a function from the person using the function \u0026ndash; those two parties do not have to coordinate requirements beforehand. Finally, this functionality means that a structure can implement as few or as many interfaces as its designer wants.\nDip Our Toe Into the Pool of Pointers We only had a few minutes to start pointers, but we did make some headway. There will be more on this in the next lecture!\nIt is important to remember that pointers are like any other type \u0026ndash; they have a range of valid values and a set of valid operations that you can perform on those values. What are the range of valid values for a pointer? All valid memory addresses. And what are the valid operations? Addition, subtraction, dereference and assignment.\n In the diagram, the gray area is the memory of the computer. The blue box is a pointer. It points to the gold area of memory. It is important to remember that pointers and their targets both exist in memory! In fact, in true Inception (Links to an external site.)style, a pointer can pointer to a pointer!\nAt the same time that pointers are types, they also have types. The type of a pointer includes the type of the target object. In other words, if the memory in the gold box held an object of type T, the the green box\u0026rsquo;s type would be \u0026ldquo;pointer to type T.\u0026rdquo; If the programmer dereferences the blue pointer, they will get access to the object in memory in the gold.\nIn an ideal scenario, it would always be the case that the type of the pointer and the type of the object at the target of the pointer are the same. However, that\u0026rsquo;s not always the case. Come to the next lecture to see what can go wrong when that simple fact fails to hold!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-4-2021/",
	"title": "10/4/2021",
	"tags": [],
	"description": "",
	"content": "Original is here One day closer to Candy Corn!\nCorrections When we were discussing the nature of the type of pointers, we specified that the range of valid values for a pointer are all memory addresses. In some languages this may be true. However, some other languages specify that the range of valid values for a pointer are all memory addresses and a special null value that explicitly specifies a pointer does not point to a target.\nWe also discussed the operations that you can perform on a pointer-type variable. What we omitted was a discussion of an operation that will fetch the address of a variable in memory. For languages that use pointers to support indirect addressing (see below), such an operation is required. In C/C++, this operation is performed using the address of (\u0026amp;) operator.\nPointers We continued the discussion of pointers that we started on Friday! On Friday we discussed that pointers are just like any other type \u0026ndash; they have valid values and defined operations that the programmer can perform on those values.\nThe Pros of Pointers Though a very famous and influential computer scientist (Links to an external site.) once called his invention of null references a \u0026ldquo;billion dollar mistake\u0026rdquo; (he low balled it, I think!), the presence and power of pointers in a language is important for at least two reasons:\n Without pointers, the programmer could not utilize the power of indirection. Pointers give the programmer the power to address and manage heap-dynamic memory.  Indirection gives the programmer the power to link between different objects in memory \u0026ndash; something that makes writing certain data structures (like trees, graphs, linked lists, etc) easier. Management of heap-dynamic memory gives the programmer the ability to allocate, manipulate and deallocate memory at runtime. Without this power, the programmer would have to know before execution the amount of memory their program will require.\nThe Cons of Pointers Their use as a means of indirection and managing heap-dynamic memory are powerful, but misusing either can cause serious problems.\nPossible Problems when Using Pointers for Indirection As we said in the last lecture, as long as a pointer targets memory that contains the expected type of object, everything is a-okay. Problems arise, however, when the target of the pointer is an area in memory that does not contain an object of the expected type (including garbage) and/or the pointer targets an area of memory that is inaccessible to the program.\nThe former problem can arise when code in a program writes to areas of memory beyond their control (this behavior is usually an error, but is very common). It can also arise because of a use after free. As the name implies, a use-after-free error occurs when a program uses memory after it has been freed. There are two common scenarios that give rise to a use after free:\n Scenario 1:  One part of a program (part A) frees an area of memory that held a variable of type T that it no longer needs Another part of the program (part B) has a pointer to that very memory A third part of the program (part C) overwrites that \u0026ldquo;freed\u0026rdquo; area of memory with a variable of type S Part B accesses the memory assuming that it still holds a variable of Type T   Scenario 2:  One part of a program (part A) frees an area of memory that held a variable of type T that it no longer needs Part A never nullifies the pointer it used to point to that area of memory though the pointer is now invalid because the program has released the space A second part of the program (part C) overwrites that \u0026ldquo;freed\u0026rdquo; area of memory with a variable of type S Part A incorrectly accesses the memory using the invalid pointer assuming that it still holds a variable of Type T    Scenario 2 is depicted visually in the following scenario and intimates why use-after-free errors are considered security vulnerabilities:\n In the example shown visually above, the program\u0026rsquo;s use of the invalid pointer means that the user of the invalid pointer can now access an object that is at a higher privilege level (Restricted vs Regular) than the programmer intended. When the programmer calls a function through the invalid pointer they expect that a method on the Regular object will be called. Unfortunately, a method on the Restricted object will be called instead. Trouble!\nThe latter problem occurs when a pointer targets memory beyond the program\u0026rsquo;s control. This most often occurs when the program sets a variable\u0026rsquo;s address to 0 (NULL, null, nil) to indicate that it is invalid but later uses that pointer without checking its validity. For compiled languages this often results in the dreaded segmentation fault and for interpreted languages it often results in other anomalous behavior (like Java\u0026rsquo;s Null Pointer Exception (NPE)). Neither are good!\nPossible Solutions Wouldn\u0026rsquo;t it be nice if we had a way to make sure that the pointer being dereferenced is valid so we fall victim to some of the aforementioned problems? What would be the requirements of such a solution?\n Pointers to areas of memory that have been deallocated cannot be dereferenced. The type of the object at the target of a pointer always matches the programmer\u0026rsquo;s expectation.  Your author describes two potential ways of doing this. First, are tombstones. Tombstones are essentially an intermediary between a pointer and its target. When the programming language implements pointers and uses tombstones for protection, a new tombstone is allocated for each pointer the programmer generates. The programmer\u0026rsquo;s pointer targets the tombstone and the tombstone targets the pointer\u0026rsquo;s actual target. The tombstone also contains an extra bit of information: whether it is valid. When the programmer first instantiates a pointer to some target a the compiler/interpreter\n generates a tombstone whose target is a sets the valid bit of the tombstone to valid points the programmer\u0026rsquo;s pointer to the tombstone.  When the programmer dereferences their pointer, the compiler/runtime will check to make sure that the target tombstone\u0026rsquo;s valid flag is set to valid before doing the actual dereference of the ultimate target. When the programmer \u0026ldquo;destroys\u0026rdquo; the pointer (by releasing the memory at its target or by some other means), the compiler/runtime will set the target tombstone\u0026rsquo;s valid flag to invalid. As a result, if the programmer later attempts to dereference the pointer after it was destroyed, the compiler/runtime will see that the tombstone\u0026rsquo;s valid flag is invalid and generate an appropriate error.\nThis process is depicted visually in the following diagram.\nTombstones.png\nThis seems like a great solution! Unfortunately, there are downsides. In order for the tombstone to provide protection for the entirety of the program\u0026rsquo;s execution, once a tombstone has been allocated it cannot be reclaimed. It must remain in place forever because it is always possible that the programmer can incorrectly reuse an invalid pointer. As soon as the tombstone is deallocated, the protection that it provides is gone. The other problem is that the use of tombstones adds an additional layer of indirection to dereference a pointer and every indirection causes memory accesses. Though memory access times are small, they are not zero \u0026ndash; the cost of these additional memory accesses add up.\nWhat about a solution that does not require an additional level of indirection? There is a so-called lock-and-key technique. This protection method requires that the pointer hold an additional piece of information beyond the address of the target: the key. The memory at the target of the pointer is also required to hold a key. When the system allocates memory it sets the keys of the pointer and the target to be the same value. When the programmer dereferences a pointer, the two keys are compared and the operation is only allowed to continue if the keys are the same. The process is depicted visually below.\n With this technique, there is no additional memory access \u0026ndash; that\u0026rsquo;s good! However, there are still downsides. First, there is a speed cost. For every dereference there must be a check of the equality of the keys. Depending on the length of the key that can take a significant amount of time. Second, there is a space cost. Every pointer and block of allocated memory now must have enough space to store the key. For systems where memory allocations are done in big chunks, the relative size overhead of storing, say, and 8byte key is not significant. However, if the system allocates many small areas of memory, the relative size overhead is tremendous. Moreover, the more heavily the system relies on pointers the more space will be used to store keys rather than meaningful data.\nWell, let\u0026rsquo;s just make the keys smaller? Great idea. There\u0026rsquo;s only one problem: The smaller the keys the fewer unique key values. Fewer unique key values mean that it is more likely an invalid pointer randomly points to a chunk of memory with a matching key. In this scenario, the protection afforded by the scheme is vitiated. (I just wanted to type that word \u0026ndash; I\u0026rsquo;m not even sure I am using it correctly!)\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/10-6-2021/",
	"title": "10/6/2021",
	"tags": [],
	"description": "",
	"content": "Original is here.\nI love Reese\u0026rsquo;s Pieces.\nCorrections None to speak of!!\nPointers for Dynamic Memory Management We finished up our discussion of pointers in today\u0026rsquo;s class. In the previous class, we talked about how pointers have two important roles in programming languages:\n indirection \u0026ndash; referring to other objects dynamic memory management \u0026ndash; \u0026ldquo;handles\u0026rdquo; for areas of memory that are dynamically allocated and deallocated by the system.  On Monday we focused on the role of pointers in indirection and how to solve some of the problems that can arise from using pointers in that capacity. In today\u0026rsquo;s class, we focused on the role of pointers in dynamic memory management.\nAs tools for dynamic memory management, the programmer can use pointers to target blocks (N.B.: I am using blocks as a generic term for memory and am not using it in the sense of a block [a.k.a. page] as defined in the context of operating systems) of dynamic memory that are allocated and deallocated by the operating system for use by an application. The programmer can use these pointers to manipulate what is stored in those blocks and, ultimately, release them back to the operating system when they are no longer needed.\nMemory in the system is a finite resource. If a program repeatedly asks for memory from the system without releasing previous allocations back to the system, there will come a time when the memory is exhausted. In order to be able to release existing allocations back to the operating system for reuse by other applications, the programmer must not lose track of those existing allocations. When there is a memory allocation from the operating system to the application that can no longer be reached by a pointer in the application, that memory allocation is leaked. Because the application no longer has a pointer to it, there is no way for the application to release it back to the system. Leaked memory belongs to the leaking application until it terminates.\nFor some programs this is fine. Some applications run for a short, defined period of time. However, there are other programs (especially servers) that are written specifically to operate for extended periods of time. If such applications leak memory, they run the risk of exhausting the system\u0026rsquo;s memory resources and failing (Links to an external site.).\nPreventing Memory Leaks System behavior will be constrained when those systems are written in languages that do not support using pointers for dynamic memory management. However, what we learned (above) is that it is not always easy to use pointers for dynamic memory management correctly. What are some of the tools that programming languages provide to help the programmer manage pointers in their role as managers of dynamic memory.\nReference Counting In a reference-counted memory management system, each allocated block of memory given to the application by the system contains a reference count. That reference count, well, counts the number of references to the object. In other words, for every pointer to an operating-system allocated block of memory, the reference count on that block increases. Every time that a pointer\u0026rsquo;s target is changed, the programming language updates the reference counts of the old target (decrement) and the new target (increment), if there is a new target (the pointer could be changed to null, in which case there is no new target). When a block\u0026rsquo;s reference count reaches zero, the language knows that the block is no longer needed, and automatically returns it to the system! Pretty cool.\n The scenario depicted visually shows the reference counting process. At time (a), the programmer allocates a block of memory dynamically from the operating system and puts an application object in that block. Assume that the application object is a node in a linked list. The first node is the head of the list. Because the programmer has a pointer that targets that allocation, the block\u0026rsquo;s reference count at time (a) is 1. At time (b), the programmer allocates a second block of memory dynamically from the system and puts a second application object in that block \u0026ndash; another node in the linked list (the tail of the list). Because the head of the list is referencing the tail of the list, the reference count of the tail is 1. At time (c) the programmer deletes their pointer (or reassigns it to a different target) to the head of the linked list. The programming language decrements the reference count of the block of memory holding the head node and deallocates it because the reference count has dropped to 0. Transitively, the pointer from the head application object to the tail application object is deleted and the programming language decrements the reference count of its target, the block of memory holding the tail application object (time (d)). The reference count of the block of memory holding the tail application object is now 0 and so the programming language automatically deallocates the associated storage (time (e)). Voila \u0026ndash; an automatic way to handle dynamic memory management.\nThere\u0026rsquo;s only one problem. What if the programmer wants to implement a circularly linked list?\n Because the tail node points to the head node, and the head node points to the tail node, even after the programmer\u0026rsquo;s pointer to the head node is deleted or retargeted, the reference counts of the two nodes will never drop to 0. In other words, even with reference-counted automatic memory management, there could still be a memory leak! Although there are algorithms to break these cycles, it\u0026rsquo;s important to remember that reference counting is not a panacea. Python is a language that manages memory using reference counting.\nGarbage Collection Garbage collection (GC) is another method of automatically managing dynamically allocated memory. In a GC\u0026rsquo;d system, when a programmer allocates memory to store an object and no space is available, the programming language will stop the execution of the program (a so-called GC pause) to calculate which previously allocated memory blocks are no longer in use and can be returned to the system. Having freed up space as a result of cleaning up unused garbage, the allocation requested by the programmer can be satisfied and the execution of the program can continue.\nThe most efficient way to engineer a GC\u0026rsquo;d system is if the programming language allocates memory to the programmer in fixed-size cells. In this scenario, every allocation request from the programmer is satisfied by a block of memory from one of several banks of fixed-size blocks that are stacked back-to-back. For example, a programming language may manage three different banks \u0026ndash; one that holds reserves of X-sized blocks, one that holds reserves of Y-sized blocks and one that holds reserves of Z-sized blocks. When the programmer asks for memory to hold an object that is of size a, the programming language will deliver a block that is just big enough to that object. Because the size of the requested allocation may not be exactly the same size as one of the available fixed-size blocks, space may be wasted.\nThe fixed sizing of blocks in a GC\u0026rsquo;d system makes it easy/fast to walk through every block of memory. We will see shortly that the GC algorithm requires such an operation every time that it stops the program to do a cleanup. Without a consistent size, traversing the memory blocks would require that each block hold a tag indicating its size \u0026ndash; a waste of space and the cause of an additional memory read \u0026ndash; so that the algorithm could dynamically calculate the starting address of the next block.\nWhen the programmer requests an allocation that cannot be satisfied, the programming language stops the execution of the program and does a garbage collection. The classic GC algorithm is called mark and sweep and has three steps:\nEvery block of memory is marked as free using a free bit attached to the block. Of course, this is only true of some of the blocks, but the GC is optimistic! All pointers active at the time the program is paused are traced to their targets. The free bits of those blocks are reset. The blocks that are marked free and released.\nThe process is shown visually below:\n At times (a), (b) and (c), the programmer is allocating and manipulating references to dynamically allocated memory. At time (c), the allocation request for variable z cannot be satisfied because there are no available blocks. A GC pause starts at time (d) and the mark-and-sweep algorithm commences by setting the free bit of every block. At time (e) the pointers are traced and the appropriate free bits are cleared. At time (f) the memory is released from the unused block and its free bit, too, is reset. At time (g) the allocation for variable z can be satisfied, the GC pause completes and the programming language restarts execution of the program.\nThis process seems great, just like reference counting seemed great. However, there is a significant problem: The programmer cannot predict when GC pauses will occur and the programmer cannot predict how long those pauses will take. A GC pause is completely initiated by the programming language and (usually) completely beyond the control of the programmer. Such random pauses of program execution could be extremely harmful to a system that is controlling a system that needs to keep up with interactions from the outside world. For instance, it would be totally unacceptable for an autopilot system to take an extremely long GC pause as it calculates the heading needed to land a plane. There are myriad other systems where pauses are inappropriate.\nThe mark-and-sweep algorithm described above is extremely naive and GC algorithms are the subject of intense research. Languages like go and Java manage memory with a GC and their algorithms are incredibly sophisticated. If you want to know more, please let me know!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/8-23-2021/",
	"title": "8/23/2021",
	"tags": [],
	"description": "",
	"content": "Definitions:  programming language: A system of communicating computational ideas between people and computing machines. high-level programming language: A programming language that is independent of any particular machine architecture. syntax: The rules for constructing structurally valid (note: valid, not correct) computer programs in a particular language. names: A means of identifying \u0026ldquo;entities\u0026rdquo; in a programming language. types: A type denotes the kinds of values that a program can manipulate. (A more specific definition will come later in the course). semantics: The effect of each statement\u0026rsquo;s execution on the program\u0026rsquo;s operation.  Concepts: There are four fundamental components of every programming language: syntax, names, types, and semantics.\nPractice:   For your favorite programming language, attempt to explain the most meaningful parts of its syntax. Think about what are valid identifiers in the language, how statements are separated, whether blocks of code are put inside braces, etc.\n  Next, ask yourself how your favorite language handles types. Are variables in the language given types explicitly or implicitly? If the language is compiled, are types assigned to variables before the code is compiled or as the program executes? These are issues that we will discuss in detail in Module 2.\n  Finally, think about how statements in your favorite programming language affect the program\u0026rsquo;s execution. Does the language have loops? if-then statements? function composition? goto?\n  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/8-25-2021/",
	"title": "8/25/2021",
	"tags": [],
	"description": "",
	"content": "The value of studying PLs  Every new language that you learn gives you new ways to think about and solve problems.  There is a parallel here with natural languages. Certain written/spoken languages have words for concepts that others do not. Linguists have said that people can only conceive ideas for which there are words. In certain programming languages there may be constructs (\u0026ldquo;words\u0026rdquo;) that give you a power to solve problems and write algorithms in new, interesting ways.   You can choose the right tool for the job.  When all you have is a hammer, everything looks like a nail.   Makes you an increasingly flexible programmer.  The more you know about the concepts of programming languages (and the PLs that you know) the easier it is to learn new languages.   Using PLs better Studying PLs will teach you about how languages are implemented. This \u0026ldquo;awareness\u0026rdquo; can give you insight into the \u0026ldquo;right way\u0026rdquo; to do something in a particular language. For instance, if you know that recursion and looping are equally performant and computationally powerful, you can choose to use the one that improves the readability of your code. However, if you know that iteration is faster (and that\u0026rsquo;s important for your application) then you will choose that method for invoking statements repeatedly.  Programming domains  We write programs to solve real-world problems. The problems that we are attempting to solve lend themselves to programming languages with certain characteristics. Some of those real-world problems are related to helping others solve real-world problems (systems programs):  e.g., operating systems, utilities, compilers, interpreters, etc. There are a number of good languages for writing these applications: C, C++, Rust, Python, Go, etc.   But, most of programs are designed/written to solve actually real-world problems:  scientific calculations: these applications need to be fast (parallel?) and mathematically precise (work with numbers of many kinds). Scientific applications were the earliest programming domain and inspired the first high-level programming language, Fortran. artificial intelligence: AI applications manipulate symbols (in particular, lists of symbols) as opposed to numbers. This application requirement gave rise to a special type of language designed especially for manipulating lists, Lisp (List Processor). world wide web: WWW applications must embed code in data (HTML). Because of how WWW applications advance so quickly, it is important that languages for writing these applications support rapid iteration. Common languages for writing web applications are PERL, Python, JavaScript, Ruby, Go, etc. business: business applications need to produce reports, process character-based data, describe and store numbers with specific precision (aka, decimals). COBOL has traditionally been the language of business applications, although new business applications are being written in other languages these days (Java, the .Net languages). machine learning: machine learning applications require sophisticated math and algorithms and most developers do not want to rewrite these when good alternatives are available. For this reason, a language with a good ecosystem of existing libraries makes an ideal candidate for writing ML programs (Python). game development: So-called AAA games must be fast enough to generate lifelike graphics and immersive scenes in near-real time. For this reason, games are often written in a language that is expressive but generates code that is optimized, C++.    This list is non-exhaustive, obviously!\nThe John von Neumann Model of Computing  This computing model has had more influence on the development of PLs than we can imagine. There are two hardware components in this Model (the processor [CPU] and the memory) and they are connected by a pipe.  The CPU pipes data and instructions (see below) to/from the memory (fetch). The CPU reads that data to determine the action to take (decode). The CPU performs that operation (execute). Because there is only one path between the CPU and the memory, the speed of the pipe is a bottleneck on the processor\u0026rsquo;s efficiency.   The Model is interesting because of the way that it stores instructions and data together in the same memory. It is different than the Harvard Architecture where programs and data are stored in different memory. In the Model, every bit of data is accessible according to its address. Sequential instructions are placed nearby in memory.  For instance, in     for (int i = 0; i \u0026lt; 100; i++) { statement1; statement2; statement3; } statement1, statement2 and statement3 are all stored one after the other in memory.\n Modern implementations of the Model make fetching nearby data fast. Therefore, implementing repeated instructions with loops is faster than implementing repeated loops with recursion. Or is it? This is a particular case where learning about PL will help you as a programmer!  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/8-27-2021/",
	"title": "8/27/2021",
	"tags": [],
	"description": "",
	"content": "Programming Paradigms  A paradigm is a pattern or model. A programming paradigm is a pattern of problem-solving thought that underlies a particular genre of programs and languages.  According to their syntax, names and types, and semantics, it is possible to classify languages into one of four categories (imperative, object-oriented, functional and logic). That said, modern researchers in PL are no longer as convinced that these are meaningful categories because new languages are generally a collection of functionality and features and contain bits and pieces from each paradigm.   The paradigms:  Imperative: Imperative languages are based on the centrality of assignment statements to change program state, selection statements to control program flow, loops to repeat statements and procedures for process abstraction (a term we will learn later).  These languages are most closely associated with the von Neumann architecture, especially assignment statements that approximate the piping operation at the hardware level. Examples of imperative languages include C, Fortran, Cobol, Perl.   Object-oriented: Object-oriented languages are based upon a combination of data abstraction, data hiding, inheritance and message passing.  Objects respond to messages by modifying their internal data \u0026ndash; in other words, they become active. The power of inheritance is that an object can reuse an implementation without having to rewrite the code. These languages, too, are closely associated with the von Neumann architecture and (usually) inherit selection statements, assignment statements and loops from imperative programming languages. Examples of object-oriented languages include Smalltalk, Ruby, C++, Java, Python, JavaScript.   Functional: Functional programming languages are based on the concept that functions are first-class objects in the language \u0026ndash; in other words, functions are just another type like integers, strings, etc.  In a functional PL, functions can be passed to other functions as parameters and returned from functions. The loops and selection statements of imperative programming languages are replaced with composition, conditionals, and recursion in functional PLs. A subset of functional PLs are known as pure functional PLs because functions those languages have no side-effects (a side-effect occurs in a function when that function performs a modification that can be seen outside the function \u0026ndash; e.g., changing a value of a parameter, changing a global variable, etc). Examples of functional languages include Lisp, Scheme, Haskell, ML, JavaScript, Python.   Logic: Simply put, logic programming languages are based on describing what to compute and not how to compute it.  Prolog (and its variants) are really the only logic programming language in widespread use.      Language Evaluation Criteria (New Material Alert) There are four (common) criteria for evaluating a programming language:\n Readability: A metric for describing how easy/hard it is to comprehend the meaning of a computer program written in a particular language.   Overall simplicity: The number of basic concepts that a PL has.\n Feature multiplicity: Having more than one way to accomplish the same thing. Operator overloading: Operators perform different computation depending upon the context (i.e., the type of the operands) Simplicity can be taken too far. Consider machine language.    Orthogonality: How easy/hard it is for the constructs of a language to be combined to build higher-level control and data structures.\n Alternate definition: The mutual independence of primitive operations. Orthogonal example: any type of entity in a language can be passed as a parameter to a function. Non-orthogonal example: only certain entities in a language can be used as a return value from a function (e.g., in C/C++ you cannot return an array). This term comes from the mathematical concept of orthogonal vectors where orthogonal means independent. The more orthogonal a language, the fewer exceptional cases there are in the language\u0026rsquo;s semantics. The more orthogonal a language, the slower the language: The compiler/interpreter must be able to compute based on every single possible combination of language constructs. If those combinations are restricted, the compiler can make optimizations and assumptions that will speed up program execution.    Data types: Data types make it easier to understand the meaning of variables.\n e.g., the difference between int userHappy = 0; and bool userHappy = True;    Syntax design\n A PL\u0026rsquo;s reserved words should make things clear. For instance, it is easier to match the beginnings and endings of loops in a language that uses names rather than { }s. The PL\u0026rsquo;s syntax should evoke the operation that it is performing.  For instance, a + should perform some type of addition operation (mathematical, concatenation, etc)       Writeability  Includes all the aspects of Readability, and Expressiveness: An expressive language has relatively convenient rather than cumbersome way of specifying computations.   Reliability: How likely is it that a program written in a certain PL is correct and runs without errors.  Type checking: a language with type checking is more reliable than one without type checking; type checking is testing for operations that compute on variables with incorrect types at compile time or runtime.  Type checking is better done at runtime. A strongly typed programming language is one that is always able to detect type errors either at compile time or runtime.   Exception handling (the ability of a program to intercept runtime errors and take corrective action) and aliasing (when two or more distinct names in a program point to the same resource) affect the PL\u0026rsquo;s reliability.        In truth, there are so many things that affect the reliability of a PL.               The easier a PL is to read and write, the more reliable the code is going to be.   Cost: The cost of writing a program in a certain PL is a function of  The cost to train programmers to use that language The cost of writing the program in that language The time/speed of execution of the program once it is written The cost of poor reliability The cost of maintenance \u0026ndash; most of the time spent on a program is in maintaining it and not developing it!    "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/8-30-2021/",
	"title": "8/30/2021",
	"tags": [],
	"description": "",
	"content": "Today we learned a more complete definition of imperative programming languages and studied the defining characteristics of variables. Unfortunately we did not get as far as I wanted during the class which means that there is some new material in this edition of the Daily PL!\nImperative Programming Languages Any language that is an abstraction of the von Neumann Architecture can be considered an imperative programming language.\nThere are 5 calling cards of imperative programming languages:\n state, assignment statements, and expressions: Imperative programs have state. Assignment statements are used to modify the program state with computed values from expressions  state: The contents of the computer\u0026rsquo;s memory as a program executes. expression: The fundamental means of specifying a computation in a programming language. As a computation, they produce a value. assignment statement: A statement with the semantic effect of destroying a previous value contained in memory and replacing it with a new value. The primary purpose of the assignment statement is to have a side effect of changing values in memory. As Sebesta says, \u0026ldquo;The essence of the imperative programming languages is the dominant role of the assignment statement.\u0026rdquo;   variables: The abstraction of the memory cell. loops: Iterative form of repetition (for, while, do \u0026hellip; while, foreach, etc) selection statements: Conditional statements (if/then, switch, when) procedural abstraction: A way to specify a process without providing details of how the process is performed. The primary means of procedural abstraction is through definition of subprograms (functions, procedures, methods).  Variables There are 6 attributes of variables. Remember, though, that a variable is an abstraction of a memory cell.\n type: Collection of a variable\u0026rsquo;s valid data values and the collection of valid operations on those values. name: String of characters used to identify the variable in the program\u0026rsquo;s source code. scope: The range of statements in a program in which a variable is visible. Using the yet-to-be-defined concept of binding, there is an alternative definition: The range of statements where the name\u0026rsquo;s binding to the variable is active. lifetime: The period of time during program execution when a variable is associated with computer memory. address: The place in memory where a variable\u0026rsquo;s contents (value) are stored. This is sometimes called the variable\u0026rsquo;s l-value because only a variable associated with an address can be placed on the left side of an assignment operator. value: The contents of the variable. The value is sometimes call the variable\u0026rsquo;s r-value because a variable with a value can be used on the right side of an assignment operator.  Looking forward to Binding (New Material Alert) A binding is an association between an attribute and an entity in a programming language. For example, you can bind an operation to a symbol: the + symbol can be bound to the addition operation.\nBinding can happen at various times:\n Language design (when the language\u0026rsquo;s syntax and semantics are defined or standardized) Language implementation (when the language\u0026rsquo;s compiler or interpreter is implemented) Compilation Loading (when a program [either compiled or interpreted] is loaded into memory) Execution  A static binding occurs before runtime and does not change throughout program execution. A dynamic binding occurs at runtime and/or changes during program execution.\nNotice that the six \u0026ldquo;things\u0026rdquo; we talked about that characterize variables are actually attributes!! In other words, those attributes have to be bound to variables at some point. When these bindings occur is important for users of a programming language to understand. We will discuss this on Wednesday! blob:https://1492301-4.kaf.kaltura.com/903896d9-2341-4dd3-9709-ca344de08719\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-1-2021/",
	"title": "9/1/2021",
	"tags": [],
	"description": "",
	"content": "Welcome to the Daily PL for September 1st, 2021! As we turn the page from August to September, we started the month discussing variable lifetime and scope. Lifetime is related to the storage binding and scope is related to the name binding. Before we learned that new material, however, we went over an example of the different bindings and their times in an assignment statement.\nBinding Example Consider a Python statement like this:\nvrb = arb + 5 Recall that a binding is an association between an attribute and an entity. What are some of the possible bindings (and their times) in the statement above?\n The symbol + (entity) must be bound to an operation (attribute). In a language like Python, that binding can only be done at runtime. In order to determine whether the operation is a mathematical addition, a string concatenation or some other behavior, the interpreter needs to know the type of arb which is only possible at runtime. The numerical literal 5 (entity) must be bound to some in-memory representation (attribute). For Python, it appears that the interpreter chooses the format for representing numbers in memory (https://docs.python.org/3/library/sys.html#sys.int%5Finfo (Links to an external site.), https://docs.python.org/3/library/sys.html#sys.float%5Finfo (Links to an external site.)) which means that this binding is done at the time of language implementation. The value (attribute) of the variables vrb and arb (entities) are bound at runtime. Remember that the value of a variable is just another binding.  This is not an exhaustive list of the bindings that are active for this statement. In particular, the variables vrb and arb must be bound to some address, lifetime and scope. Discussing those bindings requires more information about the statement\u0026rsquo;s place in the source code.\nVariables' Storage Bindings The storage binding is related to the variable\u0026rsquo;s lifetime (the time during which a variable is bound to memory). There are four common lifetimes:\n  static: Variable is bound to storage before execution and remains bound to the same storage throughout program execution.\n Variables with static storage binding cannot share memory with other variables (they need their storage throughout execution). Variables with static storage binding can be accessed directly (in other words, their access does not require redirection through a pointer) because the address of their storage is constant throughout execution. Direct addressing means that accesses are faster. Storage for variables with static binding does not need to be repeatedly allocated and deallocated throughout execution \u0026ndash; this will make program execution faster. In C++, variables with static storage binding are declared using the static keyword inside functions and classes. Variables with static storage binding are sometimes referred to as history sensitive because they retain their value throughout execution.    stack dynamic: Variable is bound to storage when it\u0026rsquo;s declaration statements are elaborated (the time when a declaration statement is executed).\n Variables with stack dynamic storage bindings make recursion possible because their storage is allocated anew every time that their declaration is elaborated. To fully understand this point it is necessary to understand the way that function invocation is handled using a runtime stack. We will cover this topic next week. Stay tuned! Variables with stack dynamic storage bindings cannot be directly accessed. Accesses must be made through an intermediary which makes them slower. Again, this will make more sense when we discuss the typical mechanism for function invocation. The storage for variables with stack dynamic storage bindings are constantly allocated and deallocated which adds to runtime overhead.  Variables with stack dynamic storage bindings are not history sensitive.\n  Explicit heap dynamic: Variable is bound to storage by explicit instruction from the programmer. E.g., new / malloc in C/C++.\n The binding to storage is done at runtime when these explicit instructions are executed. The storage sizes can be customized for the use. The storage is hard to manage and requires careful attention from the programmer. The storage for variables with explicit heap dynamic storage bindings are constantly allocated and deallocated which adds to runtime overhead.    Implicit heap dynamic: Variable is bound to storage when it is assigned a value at runtime.\n All storage bindings for variables in Python are handled in this way. https://docs.python.org/3/c-api/memory.html (Links to an external site.) When a variable with implicit heap dynamic storage bindings is assigned a value, storage for that variable is dynamically allocated. Allocation and deallocation of storage for variables with implicit heap dynamic storage bindings is handled automatically by the language compiler/interpreter. (More on this when we discuss memory management techniques in Module 3).    Variables' Name Bindings See the Pl for the Video.\nThis new material is presented above as Episode 1 of PL After Dark. Below you will find a written recap!\nScope is the range of statements in which a variable is visible (either referencable or assignable). Using the vocabulary of bindings, scope can also be defined as the collection of statements which can access a name binding. In other words, scope determines the binding of a name to a variable.\nIt is easy to get fooled into thinking that a variable\u0026rsquo;s name is intrinsic to the variable. However, a variable\u0026rsquo;s name is just another binding like address, storage, value, etc. There are two scopes that most languages employ:\n local: A variable is locally scoped to a unit or block of a program if it is declared there. In Python, a variable that is the subject of an assignment is local to the immediate enclosing function definition. For instance, in   def add(a, b): total = a + b return total total is a local variable.\n global: A variable is globally scoped when it is not in any local scope (terribly unhelpful, isn\u0026rsquo;t it?) Using global variables breaks the principles of encapsulation and data hiding.  For a variable that is used that is not local, the compiler/interpreter must determine to which variable the name refers. Determining the name/variable binding can be done statically or dynamically:\nStatic Scoping This is sometimes also known as lexical scoping. Static scoping is the type of scope that can be determined using only the program\u0026rsquo;s source code. In a statically scoped programming language, determining the name/variable binding is done iteratively by searching through a block\u0026rsquo;s nesting static parents. A block is a section of code with its own scope (in Python that is a function or a class and in C/C++ that is statements enclosed in a pair of {}s). The static parent of a block is the block in which the current block was declared. The list of static parents of a block are the block\u0026rsquo;s static ancestors.\nDynamic Scoping Dynamic scoping is the type of scope that can be determined only during program execution. In a dynamically scoped programming language, determining the name/value binding is done iteratively by searching through a block\u0026rsquo;s nesting dynamic parents. The dynamic parent of a block is the block from which the current block was executed. Very few programming languages use dynamic scoping (BASH, PERL [optionally]) because it makes checking the types of variables difficult for the programmer (and impossible for the compiler/interpreter) and because it increases the \u0026ldquo;distance\u0026rdquo; between name/variable binding and use during program execution. However, dynamic binding makes it possible for functions to require fewer parameters because dynamically scoped non local variables can be used in their place.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-10-2021/",
	"title": "9/10/2021",
	"tags": [],
	"description": "",
	"content": "In today\u0026rsquo;s edition of the Daily PL we will recap our discussion from today that covered expressions, order of evaluation, short-circuit evaluation and referential transparency.\nExpressions An expression is the means of specifying computations in a programming language. Informally, it is anything that yields a value. For example,\n 5 is an expression (value 5) 5 + 2 is an expression (value 7) Assuming fun is a function that returns a value, fun() is an expression (value is the return value) Assuming f is a variable, f is an expression (the value is the value of the variable)  Certain languages allow more exotic statements to be expressions. For example, in C/C++, the = operator yields a value (the value of the expression on the right operand). It is this choice by the language designer that allows a C/C++ programmer to write\nint a, b, c, d; a = b = c = d = 5; to initialize all four variables to 5.\nWhen we discuss functional programming languages, we will see how many more things are expressions that programmers typically think are simply statements.\nOrder of Evaluation Programmers learn the associativity and precedence of operations in their languages. That knowledge enables them to mentally calculate the value of statements like 5 + 4 * 3 / 2.\nWhat programmers often forget to learn about their language, is the order of evaluation of operands. Take several of those constants from the previous expression and replace them with variables and function calls:\n5 + a() * c / b() The questions abound:\n Is a() executed before the value of variable c is retrieved? Is b() executed before c()? Is b() executed at all?  In a language with functional side effects, the answer to these questions matter. Why? Consider that a could have a side effect that changes c. If the value of c is retrieved before the execution of a() then the expression will evaluate to a certain value and if the value of c is retrieved after execution of a() then the expression will evaluate to a different value.\nCertain languages define the order of evaluation of operands (Python, Java) and others do not (C/C++). There are reasons why defining the order is a good thing:\n The programmer can depend on that order and benefit from the consistency The program\u0026rsquo;s readability is improved. The program\u0026rsquo;s reliability is improved.  But there is at least one really good reason for not defining that order: optimizations. If the compiler/interpreter can move around the order of evaluation of those operands, it may be able to find a way to generate faster code!\nShort-circuit Evaluation Languages with short-circuit evaluation take these potential optimizations one step further. For a boolean expression, the compiler will stop evaluating the expression as soon as the result is fixed. For instance, in a() \u0026amp;\u0026amp; b(), if a() is false, then the entire statement will always be false, no matter the value of b(). In this case, the compiler/interpreter will simply not execute b(). On the other hand, in a() || b() if a() is true, then the entire statement will always be true, no matter the value of b(). In this case, the compiler/interpreter will simply not execute b().\nA programmer\u0026rsquo;s reliance on this type of behavior in a programming language is very common. For instance, this is a common idiom in C/C++:\nint *variable = nullptr; ... if (variable != nullptr \u0026amp;\u0026amp; *variable \u0026gt; 5) { ... } In this code, the programmer is checking to see whether there is memory allocated to variable before they attempt to read that memory. This is defensive programming thanks to short-circuit evaluation.\nReferential Transparency Most of these issues would not be a problem if programmer\u0026rsquo;s wrote functions that did not have side effects (remember that those are called pure functions). There are languages that will not allow side effects and those languages support referential transparency: A function has referential transparency if its value (its output) depends only on the value of its parameter(s). In other words, if given the same inputs, a referentially transparent function always gives the same output.\nPut It All Together Try you hand at the practice quiz Expressions, precedence, associativity and coercions to check your understanding of the material we covered in class on Friday and the material from your assigned reading! For the why, check out relational.cpp .\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-13-2021/",
	"title": "9/13/2021",
	"tags": [],
	"description": "",
	"content": "In today\u0026rsquo;s edition of the Daily PL we will recap our discussion from today that covered subprograms, polymorphism and coroutines!\nSubprograms A subprogram is a type of abstraction. It is process abstraction where the how of a process is hidden from the user who concerns themselves only with the what. A subprogram provides process abstraction by naming a collection of statements that define parameterized computations. Again, the collection of statements determines how the process is implemented. Subprogram parameters give the user the ability to control the way that the process executes. There are three types of subprograms:\n Procedure: A subprogram that does not return a value. Function: A subprogram that does return a value. Method: A subprogram that operates with an implicit association to an object; a method may or may not return a value.  Pay close attention to the book\u0026rsquo;s explanation and definitions of terms like parameter, parameter profile, argument, protocol, definition, and declaration.\nSubprograms are characterized by three facts:\n A subprogram has only one entry point Only one subprogram is active at any time Program execution returns to the caller upon completion  Polymorphism Polymorphism allows subprograms to take different types of parameters on different invocations. There are two types of polymorphism:\n ad-hoc polymorphism: A type of polymorphism where the semantics of the function may change depending on the parameter types. parametric polymorphism: A type of polymorphism where subprograms take an implicit/explicit type parameter used to define the types of their subprogram\u0026rsquo;s parameters; no matter the value of the type parameter, in parametric polymorphism the subprogram\u0026rsquo;s semantics are always the same.  Ad-hoc polymorphism is sometimes call function overloading (C++). Subprograms that participate in ad-hoc polymorphism share the same name but must have different protocols. If the subprograms' protocols and names were the same, how would the compiler/interpreter choose which one to invoke? Although a subprogram\u0026rsquo;s protocol includes its return type, not all languages allow ad-hoc polymorphism to depend on the return type (e.g., C++). See the various definitions of add in the C++ code here: subprograms.cpp . Note how they all have different protocols. Further, note that not all the versions of the function add perform an actual addition! That\u0026rsquo;s the point of ad-hoc polymorphism \u0026ndash; the programmer can change the meaning of a function.\nFunctions that are parametrically polymorphic are sometimes called function templates (C++) or generics (Java, soon to be in Go, Rust). A parametrically polymorphic function is like the blueprint for a house with a variable number of floors. A home buyer may want a home with three stories \u0026ndash; the architect takes their variably floored house blueprint and \u0026ldquo;stamps out\u0026rdquo; a version with three floors. Some \u0026ldquo;new\u0026rdquo; languages call this process monomorphization (Links to an external site.). See the definition of minimum in the C++ code here: subprograms.cpp . Note how there is only one definition of the function. The associated type parameter is T. The compiler will \u0026ldquo;stamp out\u0026rdquo; copies of minimum for different types when it is invoked. For example, if the programmer writes\nauto m = minimum(5, 4); then the compiler will generate\nint minimum(int a, int b) { return a \u0026lt; b ? a : b; } behind the scenes.\nCoroutines Just when you thought that you were getting the hang of subprograms, a new kid steps on the block: coroutines. Sebesta defines coroutines as a subprogram that cooperates with a caller. The first time that a programmer uses a coroutine, they call it at which point program execution is transferred to the statements of the coroutine. The coroutine executes until it yields control. The coroutine may yield control back to its caller or to another coroutine. When the coroutine yields control, it does not cease to exist \u0026ndash; it simply goes dormant. When the coroutine is again invoked \u0026ndash; resumed \u0026ndash; the coroutine begins executing where it previously yielded. In other words, coroutines have\n multiple entry points full control over execution until they yield the property that only one is active at a time (although many may be dormant)  Coroutines could be used to write a card game. Each player is a coroutine that knows about the player to their left (that is, a coroutine). The PlayerA coroutine performs their actions (perhaps drawing a card from the deck, etc) and checks to see if they won. If they did not win, then the PlayerA coroutine yields to the PlayerB coroutine who performs the same set of actions. This process continues until a player no longer has someone to their left. At that point, everything unwinds back to the point where PlayerA was last resumed \u0026ndash; the signal that a round is complete. The process continues by resuming PlayerA to start another round of the game. Because each player is a coroutine, it never ceased to exist and, therefore, retains information about previous draws from the deck. When a player finally wins, the process completes. To see this in code, check out cardgame.py .\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-20-2021/",
	"title": "9/20/2021",
	"tags": [],
	"description": "",
	"content": "This is an issue of the Daily PL that you are going to want to make sure that you keep safe \u0026ndash; definitely worth framing and passing on to your children! You will want to make sure you remember where you were when you first learned about \u0026hellip;\nFormal Program Semantics Although we have not yet learned about it (we will, don\u0026rsquo;t worry!), there is a robust set of theory around the way that PL designers describe the syntax of their language. You can use regular expressions, context-free grammars, parsers (recursive-descent, etc) and other techniques for defining what is a valid program.\nOn the other hand, there is less of a consensus about how a program language designer formally describes the semantics of programs written in their language. The codification of the semantics of a program written in a particular is known as formal program semantics. In other words, formal program semantics are a precise mathematical description of the semantics of an executing program. Sebesta uses the term dynamic semantics which is defines as the \u0026ldquo;meaning[] of the expressions, statements and program units of a programming language.\u0026rdquo;\nThe goal of defining formal program semantics is to understand and reason about the behavior of programs. There are many, many reasons why PL designers want a formal semantics of their language. However, there are two really important reasons: With formal semantics it is possible to prove that\n two programs calculate the same result (in other words, that two programs are equivalent), and a program calculates the correct result.  The alternative to formal program semantics are standards promulgated by committees that use natural language to define the meaning of program elements. Here is an example of a page from the standard for the C programming language:\n If you are interested, you can find the C++ language standard , the Java language standard , the C language standard , the Go language standard and the Python language standard all online.\nTesting vs Proving There is absolutely a benefit to testing software. No doubt about it. However, testing that a piece of software behaves a certain way does not prove that it operates a certain way.\n\u0026ldquo;Program testing can be used to show the presence of bugs, but never to show their absence!\u0026quot; - Edsger Dijkstra\nThere is an entire field of computer science known as formal methods whose goal is to understand how to write software that is provably correct. There are systems available for writing programs about which things can be proven. There is PVS, Coq ,Isabelle , and TLA+ , to name a few. PVS is used by NASA to write its mission-critical software and even it makes an appearance in the movie The Martian .\nThree Types of Formal Semantics There are three common types of formal semantics. It is important that you know the names of these systems, but we will only focus on one in this course!\n Operational Semantics: The meaning of a program is defined by how the program executes on an idealized virtual machine. Denotational Semantics: Program units \u0026ldquo;denote\u0026rdquo; mathematical functions and those functions transform the mathematically defined state of the program. Axiomatic Semantics: The meaning of the program is based on proof rules for each programming unit with an emphasis on proving the correctness of a program.  We will focus on operational semantics only!\nOperational Semantics Program State We have referred to the state of the program throughout this course. We have talked about how statements in imperative languages can have side effects that affect the value of the state and we have talked about how the assignment statement\u0026rsquo;s raison d\u0026rsquo;etre is to change a program\u0026rsquo;s state. For operational semantics, we have to very precisely define a program\u0026rsquo;s state.\nAt all times, a program has a state. A state is just a function whose domain is the set of defined program variables and whose range is V * T where V is the set of all valid variable values (e.g., 5, 6.0, True, \u0026ldquo;Hello\u0026rdquo;, etc) and T is the set of all valid variable types (e.g., Integer, Floating Point, Boolean, String, etc). In other words, you can ask a state about a particular variable and, if it is defined, the function will return the variable\u0026rsquo;s current value and its type.\nIt is important to note that PL researchers have math envy. They are not mathematicians but they like to use Greek symbols. So, here we go:\n\\begin{equation*} \\sigma(x) = (v, \\tau) \\end{equation*}\nThe state function is denoted with the  .  always represents some arbitrary variable type. Generally, v represents a value. So, you can read the definition above as \u0026ldquo;Variable x has value v and type  in state .\u0026rdquo;\nProgram Configuration Between execution steps (a term that we will define shortly), a program is always in a particular configuration:\n\\begin{equation*} \u0026lt;e, \\sigma\u0026gt; \\end{equation*}\nThis means that the program in state \nis about to evaluate expression e.\nProgram Steps A program step is an atomic (indivisible) change from one program configuration to another. Operational semantics defines steps using rules. The general form of a rule is\n\\begin{equation*} \\frac{premises}{conclusion} \\end{equation*}\nThe conclusion is generally written like \u0026lt;e, \u0026gt;  (v, , ). This statement means that, when the premises hold, the rule evaluates to a value (v), type () and (possibly modified) state (') after a single step of execution of a program in configuration \u0026lt;e, \u0026gt;. Note that rules do not yield configurations. All this will make sense when we see an example.\nExample 1: Defining the semantics of variable access. In STIMPL, the expression to access a variable, say i, is written like Variable(\u0026ldquo;i\u0026rdquo;). Our operational semantic rule for evaluating such an access should \u0026ldquo;read\u0026rdquo; something like: When the program is about to execute an expression to access variable i in a state , the value of the expression will be the triple of i\u0026rsquo;s value, i\u0026rsquo;s type and the unchanged state .\u0026rdquo; In other words, the evaluation of the next step of a program that is about to access a value is the value and type of the variable being accessed and the program\u0026rsquo;s state is unchanged.\nLet\u0026rsquo;s write that formally!\n\\begin{equation*} \\(\\frac{\\sigma(x) \\rightarrow (v, \\tau)} {\u0026lt;\\text{Variable}(x), \\sigma \u0026gt; \\rightarrow (v, \\tau, \\sigma)}\\) \\end{equation*}\nState Update How do we write down that the state is being changed? Why would we want to change the state? Let\u0026rsquo;s answer the second question first: we want to change the state when, for example, there is an assignment statement. If  (\u0026ldquo;i\u0026rdquo;) = (4, Integer) and then the program evaluated an expression like Assign(Variable(\u0026ldquo;i\u0026rdquo;), IntLiteral(2)), we don\u0026rsquo;t want the \nfunction to return (4, Integer) any more! We want it to return (2, Integer). We can define that mathematically like:\n\\begin{equation*} \\sigma[(v,\\tau)/x](y)= \\begin{cases} \u0026amp; \\sigma(y) \\quad y \\ne x \\ \u0026amp;(v,\\tau) \\quad y=x \\end{cases} \\end{equation*}\nThis means that if you are querying the updated state for the variable that was just reassigned (x), then return its new value and type (m and  ). Otherwise, just return the value that you would get from accessing the existing \n.\nExample 2: Defining the semantics of variable assignment (for a variable that already exists). In STIMPL, the expression to overwrite the value of an existing variable, say i, with, say, an integer literal 5 is written like Assign(Variable(\u0026quot;i\u0026quot;), IntLiteral(5)). Our operational semantic rule for evaluating such an assignment should \u0026ldquo;read\u0026rdquo; something like: When the program is about to execute an expression to assign variable i to the integer literal 5 in a state  and the type of the variable i in state  is Integer, the value of the expression will be the triple of 5, Integer and the changed state ' which is exactly the same as state \nexcept where (5, Integer) replaced i\u0026rsquo;s earlier contents.\u0026quot; That\u0026rsquo;s such a mouthful! But, I think we got it. Let\u0026rsquo;s replace some of those literals with symbols for abstraction purposes and then write it down!\n\\begin{equation*} \\frac{\u0026lt;e, \\sigma\u0026gt; \\longrightarrow (v, \\tau, \\sigma'), \\sigma(x) \\longrightarrow (*,, \\tau)} {\u0026lt;\\text{Assign(Variable)}(x, e), \\sigma \u0026gt; \\longrightarrow (v, \\tau, \\sigma' [(v, \\tau)/x])} \\end{equation*}\nLet\u0026rsquo;s look at it step-by-step:\n\\begin{equation*} \u0026lt;Assign(Variable(x),e),\\sigma\u0026gt; \\end{equation*}\nis the configuration and means that we are about to execute an expression that will assign value of expression e to variable x. But what is the value of expression e? The premise\n\\begin{equation} \u0026lt;e,\\sigma\u0026gt;(v,\\tau, \\sigma) \\end{equation}\ntells us that the value and type of e when evaluated in state  is v, and . Moreover, the premise tells us that the state may have changed during evaluation of expression e and that subsequent evaluation should use a new state, \n\u0026lsquo;. Our mouthful above had another important caveat: the type of the value to be assigned to variable x must match the type of the value already stored in variable x. The second premise\n\\begin{equation*} \\sigma(x)\\longrightarrow(*, \\tau) \\end{equation*}\ntells us that the types match \u0026ndash; see how the s are the same in the two premises? (We use the * to indicate that we don\u0026rsquo;t care what that value is!)\nNow we can just put together everything we have and say that the expression assigning the value of expression e to variable x evaluates to\n\\begin{equation*} (v,\\tau,\\sigma[(v,\\tau)/x]) \\end{equation*}\nThat\u0026rsquo;s Great, But Show Me Code! Well, Will, that\u0026rsquo;s fine and good and all that stuff. But, how do I use this when I am implementing STIMPL? I\u0026rsquo;ll show you! Remember the operational semantics for variable access:\n\\begin{equation*} \\(\\frac{\\sigma(x) \\rightarrow (v, \\tau)} {\u0026lt;\\text{Variable}(x), \\sigma \u0026gt; \\rightarrow (v, \\tau, \\sigma)}\\) \\end{equation*}\nCompare that with the code for it\u0026rsquo;s implementation in the STIMPL skeleton that you are provided for Assignment 1:\ndef evaluate(expression, state): ... case Variable(variable_name=variable_name): value = state.get_value(variable_name) if value == None: raise InterpSyntaxError(f\u0026#34;Cannot read from {variable_name}before assignment.\u0026#34;) return (*value, state) At this point in the code we are in a function named evaluate whose first parameter is the next expression to evaluate and whose second parameter is a state. Does that sound familiar? That\u0026rsquo;s because it\u0026rsquo;s the same as a configuration! We use pattern matching to select the code to execute. The pattern is based on the structure of expression and we match in the code above when expression is a variable access. Refer to Pattern Matching in Python for the exact form of the syntax. The state variable is an instance of the State object that provides a method called get_value (see Assignment 1: Implementing STIMPL for more information about that function) that returns a tuple of (v, ) In other words, get_value works the same as . So,\nvalue = state.get_value(variable_name) is a means of implementing the premise of the operational semantics.\nreturn (*value, state) yields the final result! Pretty cool, right?\nLet\u0026rsquo;s do the same analysis for assignment:\n\\(\\frac{\u0026lt;e,\\sigma\u0026gt;\\longrightarrow(v,\\tau,\\sigma),\\sigma(x)\\longrightarrow(*,\\tau)}{\u0026lt;Assign(Variable(x),e),\u0026gt;\\longrightarrow(v,\\tau,[(v,\\tau)/x])}\\)\nAnd here\u0026rsquo;s the implementation:\ndef evaluate(expression, state): ... case Assign(variable=variable, value=value): value_result, value_type, new_state = evaluate(value, state) variable_from_state = new_state.get_value(variable.variable_name) _, variable_type = variable_from_state if variable_from_state else (None, None) if value_type != variable_type and variable_type != None: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Assignment: Cannot assign {value_type}to {variable_type}\u0026#34;\u0026#34;\u0026#34;) new_state = new_state.set_value(variable.variable_name, value_result, value_type) return (value_result, value_type, new_state) First, look at\nvalue_result, value_type, new_state = evaluate(value, state) which is how we are able to find the values needed to satisfy the left-hand premise. value_result is v, value_type is  and new_state is \u0026rsquo;.\nvariable_from_state = new_state.get_value(variable.variable_name) is how we are able to find the values needed to satisfy the right-hand premise. Notice that we are using new_state (') to get variable.variable_name (x). There is some trickiness in_, variable_type = variable_from_state if variable_from_state else (None, None) to set things up in case we are doing the first assignment to the variable (which sets its type), so ignore that for now! Remember that in our premises we guaranteed that the type of the variable in state ' matches the type of the expression:\nif value_type != variable_type and variable_type != None: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Assignment: Cannot assign {value_type}to {variable_type}\u0026#34;\u0026#34;\u0026#34;) performs that check!\nnew_state = new_state.set_value(variable.variable_name, value_result, value_type) generates a new, new state ([(v,)/x]) and\nreturn (value_result, value_type, new_state) yields the final result!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-22-2021/",
	"title": "9/22/2021",
	"tags": [],
	"description": "",
	"content": "Like other popular newspapers that do in-depth analysis of popular topics (Links to an external site.), this edition of the Daily PL is part 2/2 of an investigative report on \u0026hellip;\nFormal Program Semantics In our previous class, we discussed the operational semantics of variable access and variable assignment. In this class we explored the operational semantics of the addition operator and the if/then statement.\nA Quick Review of Concepts At all times, a program has a state. A state is just a function whose domain is the set of defined program variables and whose range is V * T where V is the set of all valid variable values (e.g., 5, 6.0, True, \u0026ldquo;Hello\u0026rdquo;, etc) and T is the set of all valid variable types (e.g., Integer, Floating Point, Boolean, String, etc). In other words, you can ask a state about a particular variable and, if it is defined, the function will return the variable\u0026rsquo;s current value and its type.\nHere is the formal definition of the state function:\n\\begin{equation*} \\(\\sigma(x) = (v, \\tau)\\) \\end{equation*}\nThe state function is denoted with the  .  always represents some arbitrary variable type. Generally, v represents a value. So, you can read the definition above as \u0026ldquo;Variable x has value v and type  in state .\u0026rdquo;\nBetween execution steps, a program is always in a particular configuration:\n\\begin{equation*} \u0026lt;e, \\sigma\u0026gt; \\end{equation*}\nThis notation means that the program in state  is about to evaluate expression e.\nA program step is an atomic (indivisible) change from one program configuration to another. Operational semantics defines steps using rules. The general form of a rule is\n\\begin{equation*} \\frac{premises}{conclusion} \\end{equation*}\nThe conclusion is generally written like \u0026lt;e, \u0026gt;  (v, , ) which means that when the premises hold, the expression e evaluated in state  evaluates to a value (v), type () and (possibly modified) state (') after a single step of execution.\nDefining the Semantics of the Addition Expression In STIMPL, the expression to \u0026ldquo;add\u0026rdquo; two values n1 and n2 is written like Add(n1, n2). By the rules of the STIMPL language, for an addition to be possible, n1 and n2 must\n have the same type and have Integer, Floating Point or String type.  Because every unit in STIMPL has a value, we will define the operational semantics using two arbitrary expressions, e1 and e2. The program configuration to which we are giving semantics is\n\\begin{equation*} \u0026lt;Add(e_1),e_2),\\sigma\u0026gt; \\end{equation*}\nBecause our addition operator applies when its operands are three different types, we technically need three different rules for its evaluation. Let\u0026rsquo;s start with the operational semantics for add when its operands are of type Integer:\n\\begin{equation*} \\frac{\u0026lt;e_1,\\sigma\u0026gt;(v_1,Integer,\\sigma),\u0026lt;e_2,\\sigma\u0026gt;(v_2,Integer,\\sigma \\prime)}{\u0026lt;Add(e1,e2),\u0026gt;(v1+v2,Integer,\\sigma\\prime)} \\end{equation*}\nLet\u0026rsquo;s look at the premises. First, there is\n\\begin{equation*} \u0026lt;e_1,\\sigma\u0026gt;(v1,Integer,\\sigma \\prime) \\end{equation*}\nwhich means that, when evaluated in state , expression e1 has the value v1 and type Integer and may modify the state (to '). Notice that we are not using  for the resulting type of the evaluation? Why? Because using  indicates that this rule applies when the evaluation of e1 in state  evaluates to any type (which we \u0026ldquo;assign\u0026rdquo; to  in case we want to use it again in a later premise). Instead, we are explicitly writing Integer which indicates that this rule only defines the operational semantics for Add(e1, e2) in state  when the expression e1 evaluates to a value of type Integer in state \n.\nAs for the second premise\n\\begin{equation*} \u0026lt;e_2,\\sigma \\prime\u0026gt;(v_2,Integer,\\sigma\\prime \\prime) \\end{equation*}\nwe see something very similar. Again, our premise prescribes that, when evaluated in state ' (note the ' there), e2\u0026rsquo;s type is an Integer. It is for this reason that we can be satisfied that this rule only applies when the types of the Add\u0026rsquo;s operands match and are integers! We \u0026ldquo;thread through\u0026rdquo; the (possibly) modified ' when evaluating e2 to enforce the STIMPL language\u0026rsquo;s definition that operands are evaluated strictly left-to-right.\nAs for the conclusion,\n\\begin{equation*} (v_1+v_2,Integer,\\sigma \\prime \\prime) \\end{equation*}\nshows the value of this expression. We will assume here that + works as expected for two integers. Because the operands are integers, we can definitively write that the type of the addition will be an integer, too. We use '' as the resulting state because it\u0026rsquo;s possible that evaluation of the expressions of both e1 and e2 caused side effects.\nThe rule that we defined covers only the operational semantics for addition of two integers. The other cases (for floating-point and string types) are almost copy/paste.\nNow, how does that translate to an actual implementation?\ndef evaluate(expression, state): match expression: ... case Add(left=left, right=right): result = 0 left_result, left_type, new_state = evaluate(left, state) right_result, right_type, new_state = evaluate(right, new_state) if left_type != right_type: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Add: Cannot add {left_type}to {right_type}\u0026#34;\u0026#34;\u0026#34;) match left_type: case Integer() | String() | FloatingPoint(): result = left_result + right_result case _: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Cannot add {left_type}s\u0026#34;\u0026#34;\u0026#34;) return (result, left_type, new_state) In this snippet, the local variables left and right are the equivalent of e1 and e2, respectively, in the operational semantics. After initializing a variable to store the result, the evaluation of the premises is accomplished. new_state matches '' after being assigned and reassigned in those two evaluations. Next, the code checks to make sure that the types of the operands matches. Finally, if the types of the operands is an integer, then the result is just a traditional addition (+ in Python).\nYou can see the implementation for the other types mixed in this code as well. Convince yourself that the code above handles all the different cases where an Add is valid in STIMPL.\nDefining the Semantics of the If/Then/Else Expression In STIMPL, we write an If/Then/Else expression like If(c, t, f) where c is any boolean-typed expression, t is the expression to evaluate if the value of c is true and f is the expression to evaluate if the value of c is false. The value/type/updated state of the entire expression is the value/type/updated state that results from evaluating t when c is true and the value/type/updated state that results from evaluating f when c is false. This means that we are required to write two different rules to completely define the operational semantics of the If/Then/Else expression: one for the case where c is true and the other for the case when c is false. Sounds like the template that we used for the Add expression, doesn\u0026rsquo;t it? Because the two cases are almost the same, we will only go through writing the rule for when the condition is true:\n\\begin{equation*} \\frac{\u0026lt;c,\\sigma\u0026gt;\\longrightarrow(True,Boolean,\\sigma \\prime),\u0026lt;t,\\sigma \\prime\u0026gt;\\longrightarrow(v,\\tau,\\sigma\\prime \\prime)}{\u0026lt;If(c,t,f),\u0026gt;(v,\\tau, \\sigma \\prime \\prime)} \\end{equation*}\nAs in the premises for the operational semantics of the Add operator, the first premise in the operational semantics above uses literals to guarantee that the rule only applies in certain cases:\n\\begin{equation*} \u0026lt;c,\\sigma \\prime\u0026gt;\\longrightarrow(True,Boolean,\\sigma\\prime \\prime) \\end{equation*}\nmeans that the rule only applies when c, evaluated in state , has a value of True and a boolean type. We use the second premise\n\\begin{equation*} \u0026lt;t,\\sigma\\prime\u0026gt;(v,\\tau,\\sigma \\prime \\prime) \\end{equation*}\nto \u0026ldquo;get\u0026rdquo; some values that we will use in the conclusion. v and  are the value and the type, respectively, of t when it is evaluated in state '. Note that we evaluate t in state ' because the evaluation of the condition statement may have modified state  and we want to thread that through. Evaluation of t in state ' may modify ', generating ''. The combination of these premises are combined to define that the entire expression evaluates to\n\\begin{equation*} (v,\\tau,\\sigma\\prime \\prime) \\end{equation*}\nAgain, the pattern is the same for writing the operational semantics when the condition is false.\nLet\u0026rsquo;s look at how this translates into actual working code:\ndef evaluate(expression, state): match expression: ... case If(condition=condition, true=true, false=false): condition_value, condition_type, new_state = evaluate(condition, state) if not isinstance(condition_type, Boolean): raise InterpTypeError(\u0026#34;Cannot branch on non-boolean value!\u0026#34;) result_value = None result_type = None if condition_value: result_value, result_type, new_state = evaluate(true, new_state) else: result_value, result_type, new_state = evaluate(false, new_state) return (result_value, result_type, new_state) The local variables condition, true and false match c, t and f, respectively from the rule in the operational semantics. The first step in the implementation is to determine the value/type/updated state when c is evaluated in state . Immediately after doing that, the code checks to make sure that the condition statement has boolean type. Remember how our rule only applies when this is the case? Next, depending on whether the condition evaluated to true or false, the appropriate next expression is evaluated in the ' state (new_state). It is the result of that evaluation that is the ultimate value of the expression and what is returned.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-24-2021/",
	"title": "9/24/2021",
	"tags": [],
	"description": "",
	"content": "As we conclude the penultimate week of September, we are turning the page from imperative programming and beginning our work on object-oriented programming!\nThe Definitions of Object-Oriented Programming We started off by attempting to describe object-oriented programming using two different definitions:\n A language with support for abstraction of abstract data types (ADTs). (from Sebesta) A language with support for objects, containers of data (attributes, properties, fields, etc.) and code (methods). (from Wikipedia (Links to an external site.))  As graduates of CS1021C and CS1080C, the second definition is probably not surprising. The first definition, however, leaves something to be desired. Using Definition (1) means that we have to a) know the definition of abstraction and abstract data types and b) know what it means to apply abstraction to ADTs.\nAbstraction (Reprise) There are two fundamental types of abstractions in programming: process and data. We have talked about the former but the latter is new. When we talked previously about process abstractions, we did attempt to define the term abstraction but it was not satisfying.\nSebesta formally defines abstraction as the view or representation of an entity that includes only the most significant attributes. This definition seems to align with our notion of abstraction especially the way we use the term in phrases like \u0026ldquo;abstract away the details.\u0026rdquo; It didn\u0026rsquo;t feel like a good definition to me until I thought of it this way:\nConsider that you and I are both humans. As humans, we are both carbon based and have to breath to survive. But, we may not have the same color hair. I can say that I have red hair and you have blue hair to point out the significant attributes that distinguish us. I need not say that we are both carbon based and have to breath to survive because we are both human and we have abstracted those facts into our common humanity.\nWe returned to this point at the end of class when we described how inheritance is the mechanism of object-oriented programming that provides abstraction over ADTs. Abstract Data Types (ADTs)\nNext, we talked about the second form of abstraction available to programmers: data abstraction. As functions, procedures and methods are the syntactic and semantic means of abstracting processes in programming languages, ADTs are the syntactic and semantic means of abstracting data in programming languages. ADTs combine (encapsulate) data (usually called the ADT\u0026rsquo;s attributes, properties, etc) and operations that operate on that data (usually called the ADT\u0026rsquo;s methods) into a single entity.\nWe discussed that hiding is a significant advantage of ADTs. ADTs hide the data being represented and allow that data\u0026rsquo;s manipulation only through pre-defined methods, the ADT\u0026rsquo;s interface. The interface typically gives the ADT\u0026rsquo;s user the ability to manipulate/access the data internal to the type and perform other semantically meaningful operations (e.g., sorting a list).\nWe brainstormed some common ADTs:\n Stack Queue List Array Dictionary Graph Tree  These are are so-called user-defined ADTs because they are defined by the user of a programming language and composed of primitive data types.\nNext, we tackled the question of whether primitives are a type of ADT. A primitive type like floating point numbers would seem to meet the definition of an abstract data type:\n It\u0026rsquo;s underlying representation is hidden from the user (the programmer does not care whether FPs are represented according to IEEE754 or some other specification) There are operations that manipulate the data (addition, subtraction, multiplation, division).  The Requirements of an Object-Oriented Programming Language ADTs are just one of the three requirements that your textbook\u0026rsquo;s author believes are required for a language to be considered object oriented. Sebesta believes that, in addition to ADTs, an object-oriented programming language requires support for inheritance and dynamic method binding.\nInheritance It is inheritance where OOPs provide abstraction for ADTs. Inheritance allows programmers to abstract ADTs into common classes that share common characteristics. Consider three ADTs that we identified: trees, linked lists and graphs. These three ADTs all have nodes (of some kind or another) which means that we could abstract them into a common class: node-based things. A graph would inherit from the node-based things ADT so that its implementer could concentrate on what makes it distinct \u0026ndash; its edges, etc.\nDon\u0026rsquo;t worry if that is too theoretical. It does not negate the fact that, through inheritance, we are able to implement hierarchies that can be \u0026ldquo;read\u0026rdquo; using \u0026ldquo;is a\u0026rdquo; the way that inheritance is usually defined. With inheritance, cats inherit from mammals and \u0026ldquo;a cat is a mammal\u0026rdquo;.\nSubclasses inherit from ancestor classes. In Java, ancestor classes are called superclasses and subclasses are called, well, subclasses. In C++, ancestor classes are called base classes and subclasses are called derived classes. Subclasses inherit both data and methods.\nDynamic Method Binding In an OOP, a variable that is typed as Class A can be assigned anything that is actually a Class A or subclass thereof. We have not officially covered this yet, but in OOP a subclass can redefine a method defined in its ancestor.\nAssume that every mammal can make a noise. That means that every dog can make a noise just like every cat can make a noise. Those noises do not need to be the same, though. So, a cat \u0026ldquo;overrides\u0026rdquo; the mammal\u0026rsquo;s default noise and implements their own (meow). A dog does likewise (bark). A programmer can define a variable that holds a mammal and that variable can contain either a dog or a cat. When the programmer invokes the method that causes the mammal to make noise, then the appropriate method must be called depending on the actual type in the variable at the time. If the mammal held a dog, it would bark. If the mammal held a cat, it would meow.\nThis resolution of methods at runtime is known as dynamic method binding.\nOOP Example with Inheritance and Dynamic Method Binding abstract class Mammal { protected int legs = 0; Mammal() { legs = 0; } abstract void makeNoise(); } class Dog extends Mammal { Dog() { super(); legs = 4; } void makeNoise() { System.out.println(\u0026#34;bark\u0026#34;); } } class Cat extends Mammal { Cat() { super(); legs = 4; } void makeNoise() { System.out.println(\u0026#34;meow\u0026#34;); } } public class MammalDemo { static void makeARuckus(Mammal m) { m.makeNoise(); } public static void main(String args[]) { Dog fido = new Dog(); Cat checkers = new Cat(); makeARuckus(fido); makeARuckus(checkers); } } This code creates a hierarchy with Mammal at the top as the superclass of both the Dog and the Cat. In other words, Dog and Cat inherit from Mammal. The abstract keyword before class Mammal indicates that Mammal is a class that cannot be directly instantiated. We will come back to that later. The Mammal class declares that there is a method that each of its subclasses must implement \u0026ndash; the makeNoise function. If a subclass of Mammal fails to implement that function, it will not compile. The good news is that Cat and Dog do both implement that function and define behavior in accordance with their personality!\nThe function makeARuckus has a parameter whose type is a Mammal. As we said above, in OOP that means that I can assign to that variable a Mammal or anything that inherits from Mammal. When we call makeARuckus with an argument whose type is Dog, the function relies of dynamic method binding to make sure that the proper makeNoise function is called \u0026ndash; the one that barks \u0026ndash; even though makeARuckus does not know whether m is a generic Mammal, a Dog or a Cat. It is because of dynamic method binding that the code above generates\nbark meow as output.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-27-2021/",
	"title": "9/27/2021",
	"tags": [],
	"description": "",
	"content": "It\u0026rsquo;s the last week of September but the first full week of OOP. Let\u0026rsquo;s do this!\nOverriding in OOP Recall the concept of inheritance that we discussed in the last class. Besides its utility as a formalism that describes the way a language supports abstraction of ADTs (and, therefore, makes it a plausibly OO language), inheritance provides a practical benefit in software engineering. Namely, it allows developers to build hierarchies of types.\nHierarchies are composed of pairs of classes \u0026ndash; one is the superclass and the other is the subclass. A superclass could conceivably be itself a subclass. A subclass could itself be a superclass. In terms of a family tree, we could say that the subclass is a descendant of the superclass (Note: remember that the terms superclass and subclass are not always the ones used by the languages themselves; C++ refers to them as base and derived classes, respectively).\nA subclass inherits both the data and methods from its superclass(es). However, as Sebesta says, \u0026ldquo;\u0026hellip; the features and capabilities of the [superclass] are not quite right for the new use.\u0026rdquo; Overriding methods allows the programmer to keep most of the functionality of the baseclass and customize the parts that are \u0026ldquo;not quite right.\u0026rdquo;\nAn overridden method is defined in a subclass and replaces the method with the same name (and usually protocol) in the parent.\nThe official documentation and tutorials for Java describe overriding in the language this way:\u0026ldquo;An instance method in a subclass with the same signature (name, plus the number and the type of its parameters) and return type as an instance method in the superclass overrides the superclass\u0026rsquo;s method.\u0026quot; The exact rules for overriding methods in Java are online at the language specification .\nLet\u0026rsquo;s make it concrete with an example:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } In this example, Car is the superclass of Tesla and Chevrolet. The Car class defines a method named ignite. That method will ignite the engine of the car \u0026ndash; an action whose mechanics differ based on the car\u0026rsquo;s type. In other words, this is a perfect candidate for overriding. Both Tesla and Chevrolet implement a method with the same name, return value and parameters, thereby meeting Java\u0026rsquo;s requirements for overriding. In Java, the @Override is known as an annotation. Annotations are \u0026ldquo;a form of metadata [that] provide data about a program that is not part of the program itself.\u0026quot; Annotations in Java are attached to particular syntactic units. In this case, the @Override annotation is attached to a method and it tells the compiler that the method is overriding a method from its superclass. If the compiler does not find a method in the superclass(es) that is capable of being overridden by the method, an error is generated. This is a good check for the programmer. (Note: C++ offers similar functionality through the override specifier (Links to an external site.).)\nLet\u0026rsquo;s say that the programmer actually implemented the Tesla class like this:\nclass Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite(int testing) { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } The ignite method implemented in Tesla does not override the ignite method from Car because it has a different set of parameters. The @Override annotation tells the compiler that the programmer thought they were overriding something. An error is generated and the programmer can make the appropriate fix. Without the @Override annotation, the code will compile but produce incorrect output when executed.\nAssume that the following program exists:\npublic class CarDemo { public static void main(String args[]) { Car c = new Car(); Car t = new Tesla(); Car v = new Chevrolet(); c.ignite(); t.ignite(); v.ignite(); } } This code instantiates three different cars \u0026ndash; the first is a generic Car, the second is a Tesla and the third is a Chevrolet. Look carefully and note that the type of each of the three is actually stored in a variable whose type is Car and not a more-specific type (ie, Tesla or Chevy). This is not a problem because of dynamic dispatch. At runtime, the JVM will find the proper ignite function and invoke it according to the variable\u0026rsquo;s actual type and not its static type. Because ignite is overridden by Chevy and Tesla, the output of the program above is:\nIgniting a generic car\u0026#39;s engine! Igniting a Tesla\u0026#39;s engine! Igniting a Chevrolet\u0026#39;s engine! Most OOP languages provide the programmer the option to invoke the method they are overriding from the superclass. Java is no different. If an overriding method implementation wants to invoke the functionality of the method that it is overriding, it can do so using the super keyword.\nclass Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } With these changes, the program now outputs:\nIgniting a generic car\u0026#39;s engine! Igniting a generic car\u0026#39;s engine! Igniting a Tesla\u0026#39;s engine! Igniting a generic car\u0026#39;s engine! Igniting a Chevrolet\u0026#39;s engine! New material alert: What if the programmer does not want a subclass to be able to customize the behavior of a certain method? For example, no matter how you subclass Dog, it\u0026rsquo;s noise method is always going to bark \u0026ndash; no inheriting class should change that. Java provides the final keyword to guarantee that the implementation of a method cannot be overridden by a subclass. Let\u0026rsquo;s change the code for the classes from above to look like this:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } void start() { System.out.println(\u0026#34;Starting a car ...\u0026#34;); if (this.ignite()) { System.out.println(\u0026#34;Ignited the engine!\u0026#34;); } else { System.out.println(\u0026#34;Did NOT ignite the engine!\u0026#34;); } } final boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } Notice that ignite in the Car class has a final before the return type. This makes ignite a final method : \u0026ldquo;A method can be declared final to prevent subclasses from overriding or hiding it\u0026rdquo;. (C++ has something similar \u0026ndash; the final specifier .) Attempting to compile the code above produces this output:\nCarDemo.java:30: error: ignite() in Tesla cannot override ignite() in Car boolean ignite() { ^ overridden method is final CarDemo.java:43: error: ignite() in Chevrolet cannot override ignite() in Car boolean ignite() { ^ overridden method is final 2 errors Subclass vs Subtype In OOP there is fascinating distinction between subclasses and subtypes. All those classes that inherit from other classes are considered subclasses. However, they are not all subtypes. For a type/class S to be a subtype of type/class T, the following must hold\nAssume that (t) is some provable property that is true of t, an object of type T. Then (s)\nmust be true as well for s, an object of type S.\nThis formal definition can be phrased simply in terms of behaviors: If it is possible to pass objects of type T as arguments to a function that expects objects of type S without any change in the behavior, then S is a subtype of T. In other words, a subtype behaves exactly like the \u0026ldquo;supertype\u0026rdquo;.\nBarbara Liskov who pioneered the definition and study of subtypes put it this way (Links to an external site.): \u0026ldquo;If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2, then S is a subtype of T.\u0026rdquo;\nOpen Recursion Open recursion in an OO PL is a fancy term for the combination of a) functionality that gives the programmer the ability to refer to the current object from within a method (usually through a variable named this or self) and b) dynamic dispatch. . Thanks to open recursion, some method A of class C can call some method B of the same class. But wait, there\u0026rsquo;s more! (Links to an external site.) Continuing our example, in open recursion, if method B is overriden in class D (a subclass of C), then the overriden version of the method is invoked when called from method A on an object of type D even though method A is only implemented by class C. Wild! It is far easier to see this work in real life than talk about it abstractly. So, consider our cars again:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } void start() { System.out.println(\u0026#34;Starting a car ...\u0026#34;); if (this.ignite()) { System.out.println(\u0026#34;Ignited the engine!\u0026#34;); } else { System.out.println(\u0026#34;Did NOT ignite the engine!\u0026#34;); } } boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } The start method is only implemented in the Car class. At the time that it is compiled, the Car class has no awareness of any subclasses (ie, Tesla and Chevrolet). Let\u0026rsquo;s run this code and see what happens:\npublic class CarDemo { public static void main(String args[]) { Car c = new Car(); Car t = new Tesla(); Car v = new Chevrolet(); c.start(); t.start(); v.start(); } } Here\u0026rsquo;s the output:\nStarting a car ... Igniting a generic car\u0026#39;s engine! Ignited the engine! Starting a car ... Igniting a Tesla\u0026#39;s engine! Ignited the engine! Starting a car ... Igniting a Chevrolet\u0026#39;s engine! Did NOT ignite the engine! Wow! Even though the implementation of start is entirely within the Car class and the Car class knows nothing about the Tesla or Chevrolet subclasses, when the start method is invoked on object\u0026rsquo;s of those types, the call to this\u0026rsquo;s ignite method triggers the execution of code specific to the type of car!\nHow cool is that?\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-3-2021/",
	"title": "9/3/2021",
	"tags": [],
	"description": "",
	"content": "Welcome to The Daily PL for 9/3/2021. We spent most of Friday reviewing material from Episode 1 of PL After Dark and going over scoping examples in C++ and Python. Before continuing, make sure that you have viewed Episode 1 of PL After Dark.\nScope We briefly discussed the difference between local and global scope.\nIt is easy to get fooled into thinking that a variable\u0026rsquo;s name is intrinsic to the variable. However, a variable\u0026rsquo;s name is just another binding like address, storage, value, etc.\nAs a programmer, when a variable is local determining the name/variable binding is straightforward. Determining the name/variable binding becomes more complicated (and more important) when source code uses a non-local name to reference a variable. In cases like this, determining the name/variable binding depends on whether the language is statically or dynamically scoped.\nStatic Scoping This is sometimes also known as lexical scoping. Static scoping is the type of scope that can be determined using only the program\u0026rsquo;s source code. In a statically scoped programming language, determining the name/variable binding is done iteratively by searching through a block\u0026rsquo;s nesting static parents. A block is a section of code with its own scope (in Python that is a function or a class and in C/C++ that is statements enclosed in a pair of {}s). The static parent of a block is the block in which the current block was declared. The list of static parents of a block are the block\u0026rsquo;s static ancestors.\nHere is pseudocode for the algorithm of determining the name/variable binding in a statically scoped programming language:\ndef resolve(name, current_scope) -\u0026gt; variable s = current_scope while (s != InvalidScope) if s.contains(name) return s.variable(name) s = s.static_parent_scope() return NameError For practice doing name/variable binding in a statically scoped language, play around with an example in Python: static_scope.py\nConsider this \u0026hellip; Python and C++ have different ways of creating scopes. In Python and C++ a new scope is created at the beginning of a function definition (and that scope contains the function\u0026rsquo;s parameters automatically). However, Python and C++ differ in the way that scopes are declared (or not!) for variables used in loops. Consider the following Python and C++ code (also available at loop_scope.cpp and loop_scope.py :\ndef f(): for i in range(1, 10): print(f\u0026#34;i (in loop body): {i}\u0026#34;) print(f\u0026#34;i (outside loop body): {i}\u0026#34;) void f() { for (int i = 0; i\u0026lt;10; i++) { std::cout \u0026lt;\u0026lt; \u0026#34;i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // The following statement will cause a compilation error  // because i is local to the code in the body of the for  // loop.  // std::cout \u0026lt;\u0026lt; \u0026#34;i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } In the C++ code, the for loop introduces a new scope and i is in that scope. In the Python code, the for loop does not introduce a new scope and i is in the scope of f. Try to run the following Python code also available here at loop_scope_error.py to see why this distinction is important:\ndef f(): print(f\u0026#34;i (outside loop body): {i}\u0026#34;) for i in range(1, 10): print(f\u0026#34;i (in loop body): {i}\u0026#34;) Dynamic Scoping Dynamic scoping is the type of scope that can be determined only during program execution. In a dynamically scoped programming language, determining the name/value binding is done iteratively by searching through a block\u0026rsquo;s nesting dynamic parents. The dynamic parent of a block is the block from which the current block was executed. Very few programming languages use dynamic scoping (BASH, Perl [optionally] are two examples) because it makes checking the types of variables difficult for the programmer (and impossible for the compiler/interpreter) and because it increases the \u0026ldquo;distance\u0026rdquo; between name/variable binding and use during program execution. However, dynamic binding makes it possible for functions to require fewer parameters because dynamically scoped non local variables can be used in their place.\ndef resolve(name, current_scope) -\u0026gt; variable s = current_scope while (s != InvalidScope) if s.contains(name) return s.variable(name) s = s.dynamic_parent_scope() return NameError For practice doing name/variable binding in a dynamically scoped language, play around with an example in Python: dynamic_scope.py . Note that because Python is intrinsically a statically scoped language, the example includes some hacking of the Python interpreter to emulate dynamic scoping. Compare the dynamic in the aforementioned Python code with the resolve function in the pseudocode and see if there are differences!\nReferencing Environment (New Material Alert) The referencing environment of a statement contains all the name/variable bindings visible at that statement. NOTE: The example in the book on page 224 is absolutely horrendous \u0026ndash; disregard it entirely. Consider the example online here: referencing_environment.py . Play around with that code and make sure that you understand why certain variables are in the referencing environment and others are not.\nIn case you think that this is theoretical and not useful to you as a real, practicing programmer, take a look at the official documentation of the Python execution model and see how the language relies on the concept of referencing environments: naming-and-binding .\nScope and Lifetime Are Not the Same (New Material Alert) It is common for programmers to think that the scope and the lifetime of a variable are the same. However, this is not always true. Consider the following code in C++ (also available at scope_new_lifetime.cpp)\n#include \u0026lt;iostream\u0026gt; void f(void) { static int variable = 4; } int main() { f(); return 0; } In this program, the scope of variable is limited to the function f. However, the lifetime of variable is the entire program. Just something to keep in mind when you are programming!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/9-8-2021/",
	"title": "9/8/2021",
	"tags": [],
	"description": "",
	"content": "Welcome to The Daily PL for September 8, 2021. I\u0026rsquo;m not lying when I say that this is the best. edition. ever. There is new material included in this edition which will be covered in a forthcoming episode of PL After Dark. When that video is available, this post will be updated!\nRecap The Type Characteristics of a Language\nIn today\u0026rsquo;s lecture we talked about types (again!). In particular, we talked about the two independent axis of types for a programming language: whether a PL is statically or dynamically typed and whether it is strongly or weakly typed. In other words, the time of the binding of type/variable in a language is independent of that language\u0026rsquo;s ability to detect type errors.\n A statically typed language is one where the type/variable binding is done before the code is run and does not change throughout program execution. A dynamically typed language is one where the type/variable binding is done at runtime and/or may change throughout program execution.    A strongly typed language is one where type errors are always detected (either at before or during program execution) A weakly typed language is one that is, well, not strongly typed.   In order to have a completely a satisfying definition of strongly typed language, we defined type error as any error that occurs when an operation is attempted on a type for which it is not well defined. In Python, \u0026quot;3\u0026quot; + 5 results in a TypeError: can only concatenate str (not \u0026quot;int\u0026quot;) to str. In this example, the operation is + and the types are str and int.\nCertain strongly typed languages appear to be weakly typed because of coercions. A coercion occurs when the language implicitly converts a variable of one type to another. C++ allows the programmer to define operations that will convert the type of a variable from, say, type a to type b. If the compiler sees an expression using a variable of type b where only a variable of type a is valid, then it will invoke that conversion operation automatically. While this adds to the language\u0026rsquo;s flexibility, the conversion behavior may hide the fact that a type error exists and, ultimately, make code more difficult to debug. Note that coercions are done implicitly \u0026ndash; a change between types done at the explicit request of the programmer is know as a (type)cast.\nFinally, before digging in to actual types, we defined type system: A type system is the set of types supported by a language and the rules for their usage.\nAggregate Data Types Aggregate data types are data types composed of one or more basic, or primitive, data types. Do not ask me to write a specific definition for primitive data type \u0026ndash; it will only get us into a circular mess :-)\nArray An array is a homogeneous (i.e., all its elements must be of the same type) aggregate data type in which an individual element is accessed by its position (i.e., index) in the aggregate. There are myriad design decisions associated with a language\u0026rsquo;s implementation of arrays (the type of the index, whether their size must be fixed or whether it can be dynamic, etc.) One of those design decisions is the way that a language lays out a two dimensional array in memory. There are two options: row-major order and column-major order. For a second, forget the concept of rows and columns altogether and consider that you access two dimensional arrays by letters and numbers. See the following diagram:\nThe memory of actual computers is linear. Therefore, two dimensional arrays must be flattened. In \u0026ldquo;letter major\u0026rdquo; order, the slices of the array identified by letters are stored in memory one after the other. In \u0026ldquo;number major\u0026rdquo; order, the slices of the array identified by numbers are stored in memory one after another. Notice that, in \u0026ldquo;letter major\u0026rdquo; order, the numbers \u0026ldquo;change fastest\u0026rdquo; and that, in \u0026ldquo;number major\u0026rdquo; order, the letters \u0026ldquo;change fastest\u0026rdquo;.\nSubstitute \u0026ldquo;row\u0026rdquo; for \u0026ldquo;letter\u0026rdquo; and \u0026ldquo;column\u0026rdquo; for \u0026ldquo;number\u0026rdquo; and, voila, you understand!! The C programming language stores arrays in row-major order; Fortran stores arrays in column-major order.\nKeep in mind that this description is only one way (or many) to store two dimensional arrays. There are (Links to an external site.) others (Links to an external site.).\nAssociative Arrays, Records, Tuples, Lists, Unions, Algebraic Data Types, Pattern Matching, List Comprehensions, and Equivalence All that, and more, in Episode 2 of PL After Dark!\nNote: In this video, I said that Python\u0026rsquo;s Lists function as arrays and that Python does not have true arrays. Your book implies as much in the section on Lists. However, I went back to check, and it does appear that there is a standard module in Python that provides arrays, in certain cases. Take a look at the documentation here: python arrays . The commonly used NumPy package also provides an array type: numpy arrays . While the language, per se, does not define an array type, the presence of the modules (particularly the former) is important to note. Sorry for the confusion!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/ecology/ch1/",
	"title": "Chapter 1",
	"tags": [],
	"description": "",
	"content": "Terms Ecology The study of the interactions between an organism and its biological and physical environment.\nTrait A combination of characteristics in an individual. A trait that increases evolutionary fitness is an adaptation.\n Hair color resistance to antibiotics.  Population All of the organisms of a particular species in a particular area.\n The population of snow hares in the Himalayas.  Fitness The degree to which an organism is likely to survive in an environment. The fitter the organism, the more likely they are to survive in an environment suited to their adaptations. This is more useful in the context of the population.\nLevels of Ecological organization  organism population community ecosystem  Proximate Vs Ultimate Proximate Factors Direct or Immediate causes/effects on the organism. The How.\n Changes in photoperiod Temperature changes  Ultimate Factors The Indirect reason for an effect on the organism. The Why.\nScientific Method and Hypothesis testing Good hypothesis lead to testable predictions. It is also falsifiable.\n Observation Hypothesis Measurements Results Conclusion  Predictions A prediction is an observation or a result that is expected if the hypothesis is true.\nHypotheses and tests  Focus on a population Why does the population exist? What unique challenges does it face? Are circumstances changing? Will the population grow, shrink, disturbed  Resources  reproduction Survival Mate Food Shelter  *\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/ecology/ch-2/",
	"title": "Chapter 2",
	"tags": [],
	"description": "",
	"content": "review  mutation allele gene frequencies genotype phenotype  Important terms for the course  Evolution is the change of populations (species) over time due to heritable changes. Natural Selection is the \u0026ldquo;selection\u0026rdquo; of heritable traits due to their fitness Adaptation is the tendency for natural selection to select changes most well suited to the environment. Fitness is the likelihood that an organism will survive (pass its genes down) in its current environment.  Mechanisms for evolutionary change  Natural Selection Genetic drift Gene Flow Mutation pressure  Natural section, genetic drift, gene flow genetic drift The random change of allele frequency based on non-random mating. Only affects populations with limited populations (non-random mating)\nGene flow Individuals can move from population to population, or form a new population. These fluctuations can change the allele frequencies in a population.\nIsolation Differentiation is affected by Isolation. Isolation prevents gene flow.\nHardy Weinberg  \\(P^2 + 2PQ + Q^2\\)  Adaptation is an undirected process Traits are not created to adapt to the environment. Traits instead are randomly generated (through mutation) and the traits that are well suited to the environment are selected for.\nPhenotypic Plasticity Development of physiological variation among phenotypes induced directly by the environment.\n Occurs during the development of an individual. Himalayan Rabbits are a good example, they have areas of black fur if born in a cold environment.  Adaptive landscape An adaptive landscape is a three dimensional plot, where the x and y axis refer to allele frequency and the z axis refers to fitness. \u0026ldquo;Peaks\u0026rdquo; and \u0026ldquo;Valleys\u0026rdquo; show how different allele frequencies affect fitness and why genetic drift can be helpful for adaptation.\nRed Queen Hypothesis Increased fitness in the short term does not always end well for the species. This discrepancy of fitness between two related organisms (parasites/hosts, predators/prey) is related to generational rate. One of the species in the relationship cannot keep up evolutionarily with its pair species. This may result in extinction of prey or predator or both.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/ecology/ch-3/",
	"title": "Chapter 3",
	"tags": [],
	"description": "",
	"content": "Adaptation to environmental factors  Change factors that limit growth (amount of food, composition of air) Change Tolerance Range (Temperature tolerance, ph tolerance ) Change Behavior to adapt beyond Range ( avoid cold areas )  Limiting environmental factors  physical factors (strength, ability to fly) Physical resources Tolerance bounds  optimality and Principle of allocation Adaptation to one challenge may decrease for another. The more an organism adapts to a stable environment (optimality), the less able it is to adapt to new environments that pose new challenges (moving from hot to cold, fresh water to salt water or vice versa)\n Large beaks make it easier to break large nuts, but harder to eat insects out of trees. Homeostasis allows humans to survive in a variety of environmental conditions, but requires more (is limited by) food.  Normal Distribution and measures of variation Many biological factors rely on modifications to the normal distribution.\n quantified as a probability or confidence interval (95% of samples fall within this range of values) \\(\\text{Sample Variance} = \\frac{\\sum_1^n (\\bar x - x_i)^2}{n-1} = s^2\\) \\(\\text{Sample Mean} = \\frac{\\sum_1^n}{n} = \\bar x\\) \\(\\text{confidence statistic (percentage)} = 1-\\alpha\\) \\(\\text{confidence interval} = \\bar x \\pm t_{1-\\alpha} * s/\\sqrt n\\) \\(\\text{Standard Error (Estimate of the mean)} = \\frac{s^2}{n}\\)  Description of the plots used in ecology  y axis refers to the frequency of individuals (higher is more) x axis refers to some limiting factor (food, temperature) Peak refers to the mean of the sample  P Value The P value is the probability that the Null hypothesis is true. This value is calculated with a statistical test (t test when using a sample).\nExample  \\(H_0 = \\text{Cancer is randomly occuring}\\) \\(H_a = \\text{Cancer is caused by radiation damage}\\) \\(P \u0026lt; 0.05\\) There is less than a 5 percent chance that cancer is randomly occurring.  behavioral Thermoregulation. Animals will move to areas that are conducive to their optimal temperature range.\n Snakes bask in the sun to warm up (ectotherms) Humans will go inside (or put on clothes) to warm up (endotherms) Monkeys will bathe in hot springs in cold climates. (endotherms)  Adaptive radiation The tendency for species to diversify to fill open ecological niches.\n Early Plants Finches  Osmoregulation (water stress) and photosynthesis Organisms alter their internal concentration of solute (or consume water) to regulate their internal water content.\n Skin and scales prevent unintended evaporation of water C3 pathway is shared by all plants (the calvin cycle) C4 pathway allows plants to save water by closing the stomata when necessary.  Adaptation vs Plasticity Some animals have larger optimal ranges of physical resources than others (plasticity)\n Evolution thorugh natural selection adapts organisms to their environments Plasticity refers to the ability of an organism to survive in different environments Humans could be said to have high plasticity due to our tool making Octopi make estensive use of RNA editing sites to alter their expression Octopus RNA editing sites (plasticity?)  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/",
	"title": "Discrete Structures",
	"tags": [],
	"description": "",
	"content": "Discrete Structures as taught by Ken Berman\nSet Theory Sets unordered collection (group) of zero or more distinct objects\n set theory (operations about sets) S,T,U,A,B \u0026hellip; for sets List elements in curly braces {a, b, c} Sets are equal if and only if they contain the same elements.  Set Builder  \\(\\{x:P(x)\\}\\), \\(\\{x|P(x)\\}\\). The set of all x such that P(x) {x: x is an integer where x\u0026gt;0 and x\u0026lt;5}  Set relations  \\(\\forall\\) for all \\(\\rightarrow\\) implies \\(\\leftrightarrow\\) if and only if \\(\\exists\\) exists \\(\\nexists\\) does not exist \\(\\wedge\\) and \\(\\vee\\) or \\(x\\in S\\) x in S \\(x \\notin S\\) x not in S  Empty set  \\(\\varnothing = \\{\\}\\)  Subset and superset  \\(S\\subseteq T\\) subset \\(S \\supseteq T\\) superset \\(S\\subset T\\) proper set \\(S \\supset T\\) proper superset  Cardinality  \\(|S|\\) Cardinality of S or the number of elements in S  Universal Set  \\(U\\) is the set containing all other sets (in the problem) universe of discorse  Union  \\(\\{a,b,c\\} \\cup \\{c,e\\} = \\{a,b,c,e\\}\\)  Intersection  \\(\\{a,b,c\\} \\cap \\{c,e\\} = \\{c\\}\\)  Disjointedness \\(A \\cap B = \\varnothing\\)\nSet difference  \\(A - B\\) \\(\\{a,b,c\\} - \\{c,e\\} = \\{a, b\\}\\) Set of all elements in A but not B Compliment with universal set  Compliment  \\(\\bar A = U - A\\)  Symmetric difference  \\(A \\bigoplus B = A \\cup B - A \\cap B\\)  Cartesian product  \\(A \\times B = \\{(a,b) | a \\in A \\text{\\: and \\:} b \\in B\\}\\) \\(|A| \\times |B| = |A \\times B|\\)  Generalized Union and Intersection Union or intersection of many sets.\nStandard Proof techniques Disproof by Counterexample Shows that a conjecture is not true by pointing out an example where the conjecture does not hold.\n No nickels 1 quarter + 5 pennies 3 dimes Greedy method is not appropriate with limited change  Proof by Contradiction Proof that the opposite cannot be true.\nSquare root of 2 is irrational  \\(\\sqrt 2 = a/b\\) \\(a/b\\) is simplified a or b or both must be odd (otherwise could be simplified) \\(2 = a^2/b^2\\) \\(a^2 = 2 ** b^2\\) \\(a^2\\) must be even (2 times any number is even) \\(a\\) is even as well (odd times odd is odd) \\(a = 2 ** k\\) where k is a / 2 \\(2 = (2 ** k)^2/b^2 \\rightarrow b^2 = 2k^2\\) \\(b\\) is also odd by this method \\(a\\) and \\(b\\) cannot be odd \\(\\sqrt 2\\) cannot be rational  Trees  set of nodes first node is root every other node has a \u0026ldquo;parent\u0026rdquo; node  Two Trees  Every node that is not a leaf has 2 child nodes  Binary Trees  Every node has a maximum of 2 children  Logic Boolean operators    Negation NOT Unary \\(\\neg\\)     Conjunction AND Binary \\(\\wedge\\)   Disjunction OR Binary \\(\\vee\\)   Exclusive OR XOR Binary \\(\\bigoplus\\)   Implication IMPLIES Binary \\(\\rightarrow\\)   Bi-conditional IFF Binary \\(\\leftrightarrow\\)    Negation    p \\(\\neg p\\)     T F   F T    Conjunction    p q \\(p \\wedge q\\)     F F F   F T F   T F F   T T T    Disjunction    p q \\(p \\vee q\\)     F F F   F T T   T F T   T T T    Exclusive Or    p q \\(p \\bigoplus q\\)     F F F   F T T   T F T   T T F    Implication    p q \\(p \\rightarrow q\\)     F F T   F T T   T F F   T T T    Bi-conditional    p q \\(p \\leftrightarrow q\\)     F F T   F T F   T F F   T T T    Normal forms Disjunctive Normal Form (DNF)    p q r \\(f\\) Clause Conjunction     F F F T \\(\\neg p \\wedge \\neg q \\vee \\neg r\\)   F F T F    F T F T \\(\\neg p \\wedge \\neg q \\wedge r\\)   F T T T \\(\\neg p \\wedge q \\wedge r\\)   T F F F    T F T F    T T F T \\(p \\wedge q \\wedge \\neg r\\)   T T T T \\(p \\wedge q \\wedge r\\)     Take all of the true statements in the table and write a clause for them Concatenate all of the true clauses together with a disjunction statement \\(\\vee\\) \\(\\neg f \\Leftrightarrow (\\neg p \\wedge \\neg q \\wedge \\neg r) \\vee (\\neg p \\wedge q \\wedge \\neg r) \\vee ( \\neg p \\wedge q \\wedge r) \\vee (p \\wedge q \\wedge r) \\vee (p \\wedge q \\wedge \\neg r) \\vee (p \\wedge q \\wedge r)\\)  Conjunctive Normal Form (CNF)  Negate the DNF form \\(\\neg (\\neg f) \\Leftrightarrow f\\) Use demorgans law to distribute  Expression Trees A binary tree representation of the logical expression\n Set relations reflexive reflexive if, for every element \\(a \\in A\\) we have \\(aRa \\Rightarrow (a, a) \\in R\\)\n \\( A = \\{(a, a): a \\in A\\}\\)  Symmetric symmetric iff \\((x,y) \\in R \\wedge (y,x) \\in R\\)\nTransitive Iff R relates \\(a\\) to \\(b\\) and \\(b\\) to \\( c\\) then \\(a \\) relates to \\(c\\)\n \\(a \u0026lt; b \u0026lt; c \\rightarrow a \u0026lt; c\\) \\(a = b = c \\rightarrow a = c\\)  Modular arithmetic  \\(x \\equiv y (\\text{mod} \\: n) \\leftrightarrow (x-y) \\: \\text {mod} \\: n = 0\\)  Addition Tables  Z mod 4    + 0 1 2 3     0 \\((0 + 0) \\mod 4 = 0\\) 1 2 3   1 \\((1 + 0) \\mod 4 = 1\\) 2 3 0   2 \\((2 + 0) \\mod 4 = 1\\) 3 0 1   3 \\((3 + 0) \\mod 4 = 3\\) 0 1 2      Multiplication tables  Z mod 4    x 0 1 2 3     0 \\((0 \\cdot 0) \\mod 4 = 0\\) 0 0 0   1 \\((1 \\cdot 0) \\mod 4 = 0\\) 1 2 3   2 \\((2 \\cdot 0) \\mod 4 = 0\\) 2 0 2   3 \\((3 \\cdot 0) \\mod 4 = 0\\) 3 2 1      Exam 1 review All-Slides\nSet Theory Union  \\(S = A \\cup B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F T   F T T   F F F    Intersection  \\(S = A \\cap B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F F   F T F   F F F    Difference  \\(S = A - B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F T   F T T   F F F    Symmetric difference  \\(S = A \\bigoplus B\\) \\((a \\in S \\iff (a \\in A \\quad \\text{and} \\quad a \\ni B)\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T F   T F T   F T T   F F F    Demorgans law \\(\\neg (A \\cup B) = \\neg A \\cap \\neg B\\)\nPrinciple of Inclusion-Exclusion \\(|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\)\nProof Techniques  Counterexample Contradiction Induction Trees  Trees  n nodes n-1 edges leaf nodes = intermediate nodes + 1 Total nodes = intermediate nodes + leaf nodes  power sets  \\(A= \\{a, b, c\\}\\) \\(P(A) = \\varnothing , \\{a\\}, \\{b\\}, \\{c\\}, \\{a, b\\}, \\{a, c\\}, \\{b, c\\}, \\{a, b, c\\}\\) \\(|P(A)| = 2^{|A|} = 2^3 = 8\\)  Propositional logic  All F = contradiction All T = Tautology CNF conjunction of all disjunction clauses, unsatisfiable when all combinations of clauses are present DNF disjunction of all conjunction clauses Logic  NP and NP-completeness  P = problem that can be solved in polynomial time NP = non-deterministic polynomial (unknown if it can be solved in polynomial time) NP-complete = any NP problem A can be reduced to problem B  Functions and relations  One to one -\u0026gt; (injective) Onto () -\u0026gt; surjective One to one and Onto -\u0026gt; Bijective Density Equivalence relations  Reflexive, \\(a, a \\in R \\: \\text{for every a in A}\\) Symmetric, \\((b, a \\in R\\: \\text{ whenver} \\: a, b \\in R\\) Transitive, \\((a, b) \\in R \\text{ and } (b, c) \\in R \\text{ then } (a, c) \\in R \\text{ where } a, b, c \\in A \\)   Asymmetric, \\((a, b) \\in R \\text{ implies } (b, a) \\not\\in R\\) AntiSymmetric, assymetric except for the case \\((a, b) \\in R \\rightarrow (b, a) \\in R\\) where \\(b\\) is equal to \\(a\\) Poset (partially ordered set)  reflexive Antisymmetric Transitive    Mod Arithmetic  \\((x + y) \\mod k = (x \\mod k \\quad + \\quad y \\mod k) \\mod k \\) \\(b^{n-1} = 1 \\mod n \\)  Exam 2 review All-Slides after the last exam.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/ecology/",
	"title": "Ecology and Evolution",
	"tags": [],
	"description": "",
	"content": "Ecology and Evolution\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/exam-1-review/",
	"title": "Exam 1 review",
	"tags": [],
	"description": "",
	"content": "All-Slides\nSet Theory Union  \\(S = A \\cup B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F T   F T T   F F F    Intersection  \\(S = A \\cap B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F F   F T F   F F F    Difference  \\(S = A - B\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T T   T F T   F T T   F F F    Symmetric difference  \\(S = A \\bigoplus B\\) \\((a \\in S \\iff (a \\in A \\quad \\text{and} \\quad a \\ni B)\\)     \\(A\\) \\(B\\) \\(A \\cup B\\)     T T F   T F T   F T T   F F F    Demorgans law \\(\\neg (A \\cup B) = \\neg A \\cap \\neg B\\)\nPrinciple of Inclusion-Exclusion \\(|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\)\nProof Techniques  Counterexample Contradiction Induction Trees  Trees  n nodes n-1 edges leaf nodes = intermediate nodes + 1 Total nodes = intermediate nodes + leaf nodes  power sets  \\(A= \\{a, b, c\\}\\) \\(P(A) = \\varnothing , \\{a\\}, \\{b\\}, \\{c\\}, \\{a, b\\}, \\{a, c\\}, \\{b, c\\}, \\{a, b, c\\}\\) \\(|P(A)| = 2^{|A|} = 2^3 = 8\\)  Propositional logic  All F = contradiction All T = Tautology CNF conjunction of all disjunction clauses, unsatisfiable when all combinations of clauses are present DNF disjunction of all conjunction clauses Logic  NP and NP-completeness  P = problem that can be solved in polynomial time NP = non-deterministic polynomial (unknown if it can be solved in polynomial time) NP-complete = any NP problem A can be reduced to problem B  Functions and relations  One to one -\u0026gt; (injective) Onto () -\u0026gt; surjective One to one and Onto -\u0026gt; Bijective Density Equivalence relations  Reflexive, \\(a, a \\in R \\: \\text{for every a in A}\\) Symmetric, \\((b, a \\in R\\: \\text{ whenver} \\: a, b \\in R\\) Transitive, \\((a, b) \\in R \\text{ and } (b, c) \\in R \\text{ then } (a, c) \\in R \\text{ where } a, b, c \\in A \\)   Asymmetric, \\((a, b) \\in R \\text{ implies } (b, a) \\not\\in R\\) AntiSymmetric, assymetric except for the case \\((a, b) \\in R \\rightarrow (b, a) \\in R\\) where \\(b\\) is equal to \\(a\\) Poset (partially ordered set)  reflexive Antisymmetric Transitive    Mod Arithmetic  \\((x + y) \\mod k = (x \\mod k \\quad + \\quad y \\mod k) \\mod k \\) \\(b^{n-1} = 1 \\mod n \\)  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/exam-2-review/",
	"title": "Exam 2 review",
	"tags": [],
	"description": "",
	"content": "All-Slides after the last exam.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/hard-vs-micro/",
	"title": "HardWiredVsMicroProgrammed;",
	"tags": [],
	"description": "",
	"content": "Intro Describes a \u0026ldquo;big Picture\u0026rdquo; of computer systems through an example of a \u0026ldquo;simple\u0026rdquo; computer. This computer is Big Endian (reads the leftmost (most significant) bit first)\nThe Basic Computer Two Principal functional parts  Data path section where processing occurs Control section decodes instructions and leaves control sequence for Data path section  Two types of control units  Hard wired controllers Micro programmed controllers  figure 1   Description of figure 1 Data Path Sections\n   Single 12-bit-wide bus\nexchanges information between pairs of registers\n     Registers + 256 X 12 bit RAM\ncontrolled by 16 control signals\n   Load(L) signals\nActive L clocks or loads bus contents into register on next rising pulse from the system clock\n     Enabled(E) signals\nActive E signal enables the tristate outputs of the register or makes contents of register available to bus\n     register tranfer from example\nContents of Register A to B\n Requires EA signal (contents of A can be read by bus) Requires LB signal (contents of bus can be moved into B)      Arithmetic-Logic-Unit (ALU) A circuit capable of adding or subtracting two 12 bit numbers When the first bit of the Accumulator(ACC) is 1 (Negative Flag) it is considered a negative number (represented using 2s compliment)\n Two input registers  Accumulator (ACC) Register B   Two control signals  Add (A) Subtract (S)    RAM memory    Registers\n   Memory Address Register (MAR)\nRegister where address from memory is temporarily stored\n     Memory Data Register (MDR)\nRegister where data (word) corresponding to the address in MAR is temporarily stored\n       Control Signals\n   Read (R)\ncopies Word stored in RAM at address specified by MAR to MDR\n     Write (W)\ncopies contents (Word) of MDR to RAM at address specified by MAR\n       Steps to write and read data to the RAM\n   Read\n 8-bit address is stored in register MAR Active R signal is supplied to the RAM Data is copied from RAM at address specified by MAR to MDR register       Write\n 8-bit address is stored in register MAR a Word is stored in register MDR Active W signal is supplied to RAM Data is copied from MDR to RAM at address specified by MAR       Note about I/O implementation\nIn this specific computer design, all I/O devices are memory mapped This means that several memory locations are reserved for the I/O devices writing and reading to these I/O devices works in the same way as any other location in the ram.\n    Program Counter (PC) Register    Increment Program Counter (IP) control signal\nWhen PC recieves an IP signal it increments contents of PC by 1\n     Instruction Register (IR)\nHolds the instruction that is about to be executed and provides opcode to the controller/sequencer\n  Computer\u0026rsquo;s Instruction Set A set of all instructions that the computer can process.\nInstruction A 12-bit word made up of a 4-bit opcode and an 8-bit operand address\noperation code (opcode) specifies the action to be taken by the computer\noperand The data that the opcode operates on. Located at a memory location specified by the 8 bit address in the instruction.\nTable 1  Table 1 description    Op-Code\nThe base 10 identity of the 4 bit code(in this computer) at the beginning of every instruction. Specifies the action that will be performed, such as loading, moving values. Not shown is the Fetch Instruction (an instruction used to load the next instruction) and can be thought of as having the opcode 0000, but it really doesn\u0026rsquo;t require one (in Hard-Wired)\n     Opcode are specified in assembly by the Mnemonic\nThe Mnemonic makes the assembly code much more human readable\n LDA (Load accumlulator(ACC)) is equivalent to op-code 0001 STA (Store ACC) is equivalent to op-code 0002 The codes 8(1000) - 15(1111) are all equivalent to the HLT Mnemonic       Register Transfers\nThis column shows how data is transferred from one register to another until the action specified by the opcode has been accomplished.\n LDA 00100010 is an example instruction LDA (Loads data from RAM to accumlulator register) by following the steps required to access data from RAM  MAR \u0026lt;\u0026ndash; IR moves (copies) the contents of the Instruction Register(IR) to the Memory Address Register MDR \u0026lt;\u0026ndash; RAM(MAR) moves the data at address specified by MAR to the MDR register ACC \u0026lt;\u0026ndash; MDR moves the contents of MDR to ACC ACC \u0026lt;\u0026ndash; RAM is then accomplished.         Active Control Signals\nThis column shows the signals that are required (power to signal pin) to perform the specified register transfers. Signal Pins are shown in Figure 1\n MAR \u0026lt;\u0026ndash; IR requires the EI (enable instruction register) and the LM (Load MAR) signals MDR \u0026lt;\u0026ndash; RAM(MAR) requires the R (read) signal    Hard-Wired Control Unit Figure 2  Description of Figure 2 Internal organization of a hard-wired control unit A hard wired version of our example computer. A control unit consists of a Ring counter, an instruction decoder, and a Control matrix\n   Instruction Register (IR)\nContains the current instruction (opcode + operand) The instruction register sends the Op-code (first 4 bits) to the instruction decoder. Each line to the Instruction Decoder represents a bit of the opcode.\n     Instruction Decoder\n Receives the opcode sent to it by the IR Interprets the opcode as a specific signal. Sends a signal to the control matrix corresponding to the opcode from the IR Each line to the Control matrix represents a different signal (a set of pins)  Figure 4\n     Negative Flag\nThe leading bit (\u0026ldquo;negative flag\u0026rdquo;) of the ACC register is fed into the control matrix allowing for Boolean logic within the control matrix.\n     Ring Counter\nSix Consecutive active signals that cycle continuously with every beat of the system clock\n Ring Pulse or Ring Counter Pulse When the signal becomes active (ring pulse T0 means when T0 becomes active)  A useful article about ring counters Table 2\n     Control Matrix\nMost important part of the control unit The control matrix sends out signals to every register in the computer as shown in Figure 1 and facilitates all of the instructions listed in Table 1 using those signals.\n  Figure 3 A visualization of the inner workings of the ring counter Figure 4 A visualization of the inner working of the Instruction Decoder. Shows how each opcode corresponds to an output line. Table 2 Times at which each Control Signal must be active in order to execute the hard-wired Basic Computer\u0026rsquo;s instructions. Fetch instruction The fetch instruction is executed every time the Ring Counter loops. This facilitates the next instruction being sent to the Control unit. This instruction is actually executed during the same ring counter loop as any other instructions. So when an opcode is sent to the control matrix, fetch is executed And whatever instruction is specified by the opcode. This ensures that the next instruction is fetched by the end of the ring counter loop.\nHow does the computer choose which signals to use for an opcode?   LM signal for example, the signal to load data into MAR according to Figure 1 LM has a T3 on the LDA and SDA Rows and a T0 on the Fetch row\n An AND operation is performed between each Tx and its instruction signal (in the column). An OR operation is performed between each AND operation. As this can be represented with bits (1s or 0s) These operations can be simplified to an arithmetic expression LM = T0 + T3*LDA + T3*STA. T0 does not need an AND because the fetch instruction is executed every ring counter cycle. According to the expression: LM is active when T0 is active and when T3 and (LDA or STA) is active    JN(jump negative) Row All Tx in this row have an AND operation with the value of NF(Negative Flag) as well as the instruction signal JN. This provides another level of conditional logic based on the value of the ACC negative flag. The arithmetic expression for the LP column is: LP = T3*JN*NF + T3*JMP(Jump)\n  A list of all the conditional expressions for the control signals (where * = AND, + = OR) is located in Figure 6\n  Figure 5 A hard wired example of how the control matrix would work Figure 6 A list of possible conditional expressions for the example computer. Micro-programmed Control Unit In the Hard Wired control unit example, the signals that come from the control matrix do so because of an actual circuit that is wired to perform the conditional logic shown in Figure 5. In a Micro-Programmed Control Unit an opcode is sent to the Control unit where it fetches a list of Micro-Instructions that together perform the instruction from a memory. The control unit can be thought of as a Computer within a computer.\nMicro-routine A micro-routine is a set of Micro-Instructions that implement an instruction.\nMicro-Instruction Similar to the instruction in a hard-wired computer, the micro-instruction operates on the hard wired circuits within the control unit. A micro-instruction is composed of bits that might correspond to a control signal(LM for example).\nFigure 7 A block diagram of an example micro-programmed control unit. 32 X 24 Control ROM(Read-only memory)  32 24-bit long Micro-Instructions can be stored in the ROM memory A Word is 24 bits in the context of the micro-programmable Control unit Micro-instructions in this example are composed of two fields  16-bit control signal field Each bit corresponds to a control signal 8-bit next-address field address(in ROM) of next micro-instruction to be executed. which permits additional Boolean logic shown in Figure 8.   Micro-instructions(Words) from the Control ROM are fed into the micro-instruction register.  24-bit Micro-instruction Register  analogous to the external computer\u0026rsquo;s Instruction Register. 16 signal lines are the same as the lines coming from the control matrix in Figure 2 and are connected to the signal pins shown in Figure 1. triggered by a falling clock edge See this article about signal edges  micro-counter register  Analogous to the external computer\u0026rsquo;s Program counter Register recieves input from the Multiplexer triggered by a rising clock edge. signal edges  Multiplexer (data selector) Chooses between 3 values to send to the micro-counter register\n Output of Address ROM Output from Current Address Incrementer Address stored in next-address field of the current micro-instruction (CRJA) The conditional logic is shown in the description of Figure 8  16 X 5 Address ROM Fed by outer computer\u0026rsquo;s Instruction Register. The contents of the Instruction Register can be found in Table 3\n maps opcode of external computer\u0026rsquo;s instruction to starting address of corresponding micro-routine. The first Micro-Instruction of the routine Address zero of Address ROM contains address of fetch routine in the Control ROM Other addresses in ROM correspond to the opcodes(external computer) in Table 1 addresses are of micro-routines in Control ROM  Note about signal edges The micro-counter is triggered by a rising clock edge (along with all operations in data path in Figure 1). The Micro-instruction register is triggered by a falling clock edge. In a series of steps:\n micro-counter is triggered (positive edge), presenting the new Micro-instruction address to the control ROM Previous micro-instruction is converted to signals which are sent to external system control ROM presents micro-instruction Word to micro-instruction register Micro-instruction register is triggered (negative edge) causing it to receive the micro-instruction Word  Table 3 Mapping of Op-codes to the contents of the Address ROM in the Control Unit Example using the ADD instruction ADD instruction has the 3 opcode which maps to the 09 micro-routine start address. The address is then goes to the multiplexer -\u0026gt; micro-counter -\u0026gt; Control ROM -\u0026gt; Micro-instruction register\nFigure 8 Next address field of the micro-instruction register.  CD is condition bit when CD is 1 (MAP is zero) the multiplexer\u0026rsquo;s select lines produce a 00 or a 10 (binary) based on the Negative Flag of the ACC register (of external computer). 00 00 selects incrementer address, 10 selects CRJA address MAP causes next microinstruction to be obtained from address ROM When the MAP bit is 1 (Multiplexer\u0026rsquo;s select line produce a 01(binary) (selects the address ROM)) HLT stops clock, terminating execution of activities in the entire computer CRJA (5 bits) is the control ROM jump address field (next-address field). When (CD, MAP is 0) then multiplexer\u0026rsquo;s select lines produce a 10(binary), selecting the CRJA field as the address to the next micro-instruction (in Control ROM).  With no branches, (CD=0, MAP=0, CRJA = address of next instruction in routine) Last micro-instruction in the fetch routine should have map=1 to take an instruction from the address ROM The last micro-instruction in a routine should have 00000(binary) as its CRJA field, CD=0, MAP=0 branching back to the fetch micro-routine.    Table 4 A micro-program (set of all micro-routines) that implements described instruction set from When loaded into Control ROM\n Microroutine Name (Mnemonic) The functional shortened name of each operation specified by the op-codes (Load Accumulator = LDA = 1) Used in assembly programming.\nColumns Micro-Instruction Address to Address of Next Micro-Instruction When taken together, forms the raw contents (changing hexadecimal to binary) of the Micro Control ROM\n   Address-ROM Address (Op-code)\ninput of instruction from external computer that is equivalent to first address of Micro-instruction in micro-routine.\n     Micro-Instruction Address\nMicro-Instruction addresses (2 bits) listed sequentially as part of a micro-routine\n LDA micro-routine = Opcode 1  03 04 05         Control Signal Field\nSame order of signals (bits) as the in Table 2 bits are equivalent to signals which are equivalent to pins.\n     CD\nConditional bit, when 1 causes multiplexer to depend on value of NF if MAP bit is 0\n     MAP\nWhen MAP bit is 1, multiplexer sends address of the first Micro-Instruction for the next micro-routine\n     Halt\nWhen HLT bit is 1, the Micro-Instruction Register will trigger the HALT signal, stopping the clock.\n     Address of Next micro-instruction\nOnly matters if HLT and MAP are not 1 (arbitrary values). The Micro-Instruction Address of the next Micro-Instruction in a non-branching program. Refer to description of Figure 8 for more explanation.\n  Comment This column provides a simple explanation for what happens in the micro-programmable control unit after each Micro-Instruction.\nFetch micro-routine  Control ROM addresses 0, 1, 2 (3 Micro-Instructions)  Micro-Instruction 0  Activates control Signal bits EP, LM Moves data from the Program Counter Register to the Memory Access Register MAR now contains address in RAM of the next instruction CD and MAP bits are zero, next Micro-Instruction corresponds to the address in the CRJA field (Micro-Instruction 1)   Micro-Instruction 1  Activates control Signal bit R Reads the data at adress MAR into Memory Data Register from RAM CD and MAP bits are zero, use CRJA field again (Micro-Instruction 2)   Micro-Instruction 2  Activates control Signal bits ED, LI, IP ED, LI moves the Word from MDR to the Instruction Register IP increments the PC Register New instruction is in IR PC now points to the next instruction MAP is now 1, so the next micro-instruction is from Address ROM (specified by op-code)   Signal pin description is in Figure 1    JN micro-routine  Control ROM addresses 0F, 10, 11  Micro-Instruction 0F  Does nothing except set the CD bit to 1 Execution of the next micro-instruction now depends on value of Negative Flag If NF is 0, increments Micro Program counter 0F + 1 = 10, Micro-Instruction 10 If NF is 1, uses CRJA field (Micro-Instruction 11)   Micro-Instruction 10  Does nothing (no set signal bits) MAP, CD = 0, uses CRJA field (Micro-Instruction 00 (Fetch routine)) PC is not altered, so next instruction is executed normally   Micro-Instruction 11  Activates Control Signal Bits EI, LP Moves contents of IR(least significan eight bits) to PC Next Instruction at location corresponding to value of the least significant 8 bits of IR Control of the computer is transferred to the \u0026ldquo;Jump address\u0026rdquo;      Hard-Wired vs. Micro-programmed Computers  Large majority of computers today are micro-programmed Micro-programmed computers are much more flexible you don\u0026rsquo;t have to make a new computer to change instruction set, only alter Control ROM Changing the firmware is the same as changing the contents of the Control ROM Hard-Wired architecture can not be easily(almost impossible) changed New architecture has do be designed at the hardware level (can\u0026rsquo;t add new instructions easily) Hard-Wired computers are faster (micro-program doesn\u0026rsquo;t have to react to input) and can be easier to manufacture (cheaper).  Old figures "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "I am going to be using this website as a way to consolidate my notes and material for all of my CS classes. DISCLAIMER: I am not responsible for trouble caused by incorrect information here, use at your own risk\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/how-to-write-a-micro-instruction/",
	"title": "How to write a micro-instruction",
	"tags": [],
	"description": "",
	"content": "A micro-instruction is just a binary (often converted to hex) number that represents a list of control signals, and points to the next micro-instruction. Micro-instructions can be changed by changing the A set of micro-instructions forms a micro-routine (otherwise known as an instruction).\nControl signals To understand the micro-instruction, you must understand what the control signals do. I will be referencing the Hard vs Micro paper. It has LA (load accumulator), and EA (enable accumulator) signals. They are essentially just wires connected to the control unit (referenced by the 16 coming out of CONTROL) Each register also generally has a clock signal (CLK), but that is connected directly to the clock circuit and is not required for the micro-instructions. The naming system for the signals is somewhat arbitrary but usually follows the pattern (L, E) (register). This is subverted somewhat by the ALU, but that will most likely be directly defined. The other main difference is the inclusion of I, which just means increment.\nLoad (L) A load signal allows the contents of a register to be changed by the state of the bus (a collection of wires). This signal must be paired with an E signal to do anything.\nEnable (E) An enable signal changes the state of the bus to match the contents of the register. Say a register contains the value 1111, when the enable signal is used for that register, the contents of the bus will now be 1111. This signal must be paired with a L signal to do anything.\nMicro-instructions A micro-instruction is a binary number composed of several fields stored at a specific address in the ROM (read only memory) of the control unit.\nControl field This is just a binary representation of the signals. In the hard vs micro document, the letters are signals are abbreviated into the following order:\n ILELAWLELELEASEL Expanded: IP LP EP LM R W LD ED LI EI LA EA A S E  A micro instruction will generally contain a pair of signals (L E) that transfers the contents of one register into another. The contents of the enabled register moves into the loaded register through the bus. A 1 means that the signal is on while a 0 means the signal is off.\nNext Address field This field has 4 different sub fields. it is used to determine the next micro instruction.\nCD CD (short for condition) is used for conditional logic. This bit makes the next micro-instruction depend on the value of the negative flag. This is generally off, unless you want conditional logic, see JN micro-routine\nMAP the map bit uses the next micro-instruction in the control ROM, see Fetch-routine . This is generally off.\nCRJA field This field consists of an address in the control ROM\nLDA micro-routine here is a step by step on how to write this instruction with micro-instructions.\n Find out where the data is coming from. Map out the sub-steps:  IR -\u0026gt; MAR RAM -\u0026gt; MDR MDR -\u0026gt; ACC   determine the signals for each substep:  EI, LM R ED, LA   Link the micro-instructions with the next address field.  address of next micro-instruction (04) in this case address of next micro-instruction (05) in this case link back to fetch (00) in this case   Write the codes!  (0001000001000000)(0)(0)(0)(0100) (0000010000000000)(0)(0)(0)(0101) (0000000010010000)(0)(0)(0)(0000)   Optional convert to hex  82004 20005 4800    "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/",
	"title": "Intro To Comp Systems",
	"tags": [],
	"description": "",
	"content": "Intro to comp systems as taught by Nitin.\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/logic/",
	"title": "Logic",
	"tags": [],
	"description": "",
	"content": "Boolean operators    Negation NOT Unary \\(\\neg\\)     Conjunction AND Binary \\(\\wedge\\)   Disjunction OR Binary \\(\\vee\\)   Exclusive OR XOR Binary \\(\\bigoplus\\)   Implication IMPLIES Binary \\(\\rightarrow\\)   Bi-conditional IFF Binary \\(\\leftrightarrow\\)    Negation    p \\(\\neg p\\)     T F   F T    Conjunction    p q \\(p \\wedge q\\)     F F F   F T F   T F F   T T T    Disjunction    p q \\(p \\vee q\\)     F F F   F T T   T F T   T T T    Exclusive Or    p q \\(p \\bigoplus q\\)     F F F   F T T   T F T   T T F    Implication    p q \\(p \\rightarrow q\\)     F F T   F T T   T F F   T T T    Bi-conditional    p q \\(p \\leftrightarrow q\\)     F F T   F T F   T F F   T T T    "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/mock-exam-questions/",
	"title": "Mock Exam Questions",
	"tags": [],
	"description": "",
	"content": "Question 1 and 2 seem to reference this article: 8086-8088\nQ1 If a physical branch target address is 5A230 when CS = 5200, what will it be if the CS = 7800 ?\n We need to find the offset, where offset + CS(shifted 4 bits) = physical branch target address. CS(C segment register) Offset = Physical branch target address - CS Offset = 5A230 - 52000 = 8230 (hex)  The offset is then used to find the physical branch target adress.\n 78000 + 8230 = 80230 = physical branch target address  Q2 Given that the EA of a data is 2359 and DS = 490B, what is the PA of data?\n EA (effective Address), DS (D segment register), PA(Physical Address).  Physical address is given by EA + DS(shifted 4 bits).\n DS = 490B0 (hex) EA = 2359 (hex) PA = 490B0 + 2359 = 4B409  Q3 Assuming, W, X, Y and Z as memory addresses. Write a program using any machine sequence that will carry out the following: Z  W + (Z-X).\n.globl main .text main: lw $t1, Z #load z into temporary register 1 lw $t2, X #load x into temporary register 2 sub $s1, $t1, $t2 #s1 \u0026lt;- t1-t2 lw $t1, W #load w into temporary register 1 add $s1, $t1, $s1 #s1 \u0026lt;- t1 sw $s1, Z # stores s1 into Z li $v0, 1 #prints z lw $a0, Z syscall li $v0, 10 #exits syscall .data Z: .word 12 #arbitrary value X: .word 10 #arbitrary value W: .word 5 #arbitrary value Downloadable solution prints 7\nQ4 Assume that the code below is run on a machine with a 2 GHz clock that requires the following number of cycles for each instruction: add, addi, sll, sra take 4cc each, lw takes 5cc, bne, beq take 3cc each. How many seconds will it take to execute this code. The values of registers are $4=0x20, $5= 0x20, $6= 0x30, $7= 0x10.\n.globl main .text main: sra $6, $6, 2 # changes original value of 0x30 / 2^2 = 0xC sll $7, $7, 2 # original value = 0x10, 0x10 * 2^2 = 0x40 add $8, $0, $0 # sets register 8 to 0 L2: add $12, $4, $8 #marks L2, $12 \u0026lt;- 0x20 + $8, $8 is iterator lw $12, 0($12) # $12 = memory at address of ($12) on stack add $9, $0, $0 # $9 = 0; L1: add $11, $5, $9 #marks L1, $11 = 0x20 + $9, $9 is iterator lw $11, 0($11) #$11 = memory at address of ($11) on stack addi $9, $9, 4 #$9 = $9 + 4 bne $9, $7, L1 # goes to L1 if $9 != $7 0x0 + 0x4 * 0x10 = 0x40 = $9 loop executes 0x10 times addi $8, $8, 4 # $8 = $8 + 4 beq $8, $6, L2 # goes to L2 if $8 == $6 exits before $8 can equal $6 Instruction definitions  sra (shift right arithmetic) (sra destination, origin, shift(in bits)) rounds down sll (shift left logical) (sll destination, origin, shift) bne (bne r1, r2, branch address) goes to branch address if r1 != r2 beq (beq, r1, r2, branch address) goest to branch address if r1 == r2  Calculate number of clock cycles Before loops  sra 4cc sll 4cc add 4cc total = 12cc  L2  add 4cc lw 5cc add 4cc L1 clocks addi 4cc beq 3cc total = (20cc + L1 clocks)*1 loop  L3  add 4cc lw 5cc addi 4ccc bne 3cc total = (16cc * 16 loops) = 256 clocks  Total clocks 12cc + 20cc + 256cc = 288cc\nCalculate time 288cc/(2*10^9cc/s) = 1.44 * 10^-7 seconds\nQ5 X[i] = A[B[i]] + C[i+4]\n starting address of A in $1 starting address of B in $2 starting address of C in $3 starting address of X in $4 i value in register $5  Q5 Solution download\n.globl main .text main: sll $s4, $5, 2 # multiplies i * 4 to conform to address form add $t2, $2, $s4 # gets address of B[i] offsets address of B by i lw $t3, ($t2) # sets t3 to value at address t3 = B[i] sll $t1, $t3, 2 # sets t1 to t3 * 4 to conform to address form add $t2, $1, $t1 # offsets addres value in $1 by $t1 A[B[i]] lw $s1, ($t2) # sets t3 to value at address $t2 t3 = A[B[i]] addi $t1, $5, 4 # offsets i by 4 = i+4 sll $t1, $5, 2 # multiplies i*4 to conform to address form add $t2, $3, $t1 # offsets C address by i+4 lw $s2, ($t2) # s2 = C[i+4] add $t1, $s1, $s2 # adds A[B[i]] + C[i+4] add $s3, $4, $s4 # offsets address of X by i sw $t1, ($s3) # stores $t1 to address of ($s3) Q6 The memory units that follow are specified by the number of words times the number of bits per word. How many address lines and input/output data lines are needed in each case? (a) 8K X 16 (b) 2G X 8 (c) 16M X 32 (d) 256K X 64\nPart A  Number of words = 8K Number of bits per word = 16 log base 2 of words = address lines 2^3 * 2^10 = 2^13 = 13 address lines I/O lines = address lines + bits per word 13 + 16 = 29 I/O lines  Part B  2^1 * 2^30 = 2G Address lines = 31 I/O lines = 31 + 8 = 39 39 I/O lines  Part C  2^4 * 2^20 = 16M Address lines = 24 I/O lines = 24 + 32 = 56 56 I/O lines  Part D  2^8 * 2^10 = 256K Address lines = 18 I/O lines = 18 + 64 82 I/O lines  Q7 Find the number of bytes that can be stored in the memories: (a) 8K X 16 (b) 2G X 8 (c) 16M X 32 (d) 256K X 64\nPart A  8 bits per byte 8K words 16 bits per word number of bits = 8K*16 = 2^13 * 2^4 = 2^17 number of bytes = 2^17/2^3 = 2^14 = 16K bytes  Part B  Number of bits = 2G*8 = 2^31*2^3 Number of bytes = 2^31*2^3/2^3 = 2^31 = 2G bytes  Part C  Number of bytes = 16M * 32 / 2^3 = 2^24 * 32 /2^3 = 2^26 = 64M bytes  Part D  Number of bytes = 256K*64/2^3 = 2^18 * 64 /2^3 = 2^24 /2^3 = 2^21 = 2M bytes  TODO Q8 How many 128 x 8 memory chips are needed to provide a memory capacity of 4096 x 16?\n 4096*16 / 128*8 = 64  Q9 Given a 32 x 8 ROM chip with an enable input, show the external connections necessary to construct a 128 x 8 ROM with four chips and a decoder. Useful Video\n 2^5 = 32 5 address lines for each ROM 128/32 = 4 chips 4 outputs on decoder (connected to enable inputs) 2^2 = 4 2 address lines for the decoder 5 + 2 = 7 total lines for complete address   Q10 Assume we have 8GB of word addressable memory with a word size of 64 bits and each refill line of memory stores 16 words. We have a direct-mapped cache with 1024 refill lines. Answer the following questions. More examples\n What is the address format for the cache If we changed the cache to a 2-way set associative cache, how does the format change? If we changed the cache to a fully associative cache, how does the format change?  Solution Part 1  8GB = 8*2^30 bytes 64 bit word = 8 byte word 8GB / 8 byte word = 1GW (gigaword) 1G = 2^30 30 bits for line addresses 16 words per line = 2^4 4 bits for word number on line address format (for MM) = [30-4 bit line address][4 bit word address] 1024 refill lines = 2^10 10 bits for line number 26-10 bits for tag [tag][line number][word number] 16-10-4 = cache memory format  Part 2  2 way set associate cache divide refill lines by 2 1024/2 = 512 = 2^9 26-9 = tag 17-9-4 = 2 way set cache memory format  Part 3  fully associative cache 1 refill line = 2^0 26-0 = tag 26-0-4 = fully associate cache  Hard Drive Sample question This is a writeup of question 1 from here.\nConsider a disk pack with the following specifications 16 surfaces, 128 tracks per surface, 256 sectors per track, 512 bytes per sector\nCapacity of Disk Space  Use the formula Capacity of Disk Space. 16 surfaces * 128 tracks/surface * 256 sectors/track * 512 bytes per sector = \\(2^{28}\\) bytes = 256 MB  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/modular-arithmetic/",
	"title": "Modular arithmetic",
	"tags": [],
	"description": "",
	"content": " \\(x \\equiv y (\\text{mod} : n) \\leftrightarrow (x-y) : \\text {mod} : n = 0\\)  Addition Tables  Z mod 4    + 0 1 2 3     0 \\((0 + 0) \\mod 4 = 0\\) 1 2 3   1 \\((1 + 0) \\mod 4 = 1\\) 2 3 0   2 \\((2 + 0) \\mod 4 = 1\\) 3 0 1   3 \\((3 + 0) \\mod 4 = 3\\) 0 1 2      Multiplication tables  Z mod 4    x 0 1 2 3     0 \\((0 \\cdot 0) \\mod 4 = 0\\) 0 0 0   1 \\((1 \\cdot 0) \\mod 4 = 0\\) 1 2 3   2 \\((2 \\cdot 0) \\mod 4 = 0\\) 2 0 2   3 \\((3 \\cdot 0) \\mod 4 = 0\\) 3 2 1      "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/normal-forms/",
	"title": "Normal forms",
	"tags": [],
	"description": "",
	"content": "Disjunctive Normal Form (DNF)    p q r \\(f\\) Clause Conjunction     F F F T \\(\\neg p \\wedge \\neg q \\vee \\neg r\\)   F F T F    F T F T \\(\\neg p \\wedge \\neg q \\wedge r\\)   F T T T \\(\\neg p \\wedge q \\wedge r\\)   T F F F    T F T F    T T F T \\(p \\wedge q \\wedge \\neg r\\)   T T T T \\(p \\wedge q \\wedge r\\)     Take all of the true statements in the table and write a clause for them Concatenate all of the true clauses together with a disjunction statement \\(\\vee\\) \\(\\neg f \\Leftrightarrow (\\neg p \\wedge \\neg q \\wedge \\neg r) \\vee (\\neg p \\wedge q \\wedge \\neg r) \\vee ( \\neg p \\wedge q \\wedge r) \\vee (p \\wedge q \\wedge r) \\vee (p \\wedge q \\wedge \\neg r) \\vee (p \\wedge q \\wedge r)\\)  Conjunctive Normal Form (CNF)  Negate the DNF form \\(\\neg (\\neg f) \\Leftrightarrow f\\) Use demorgans law to distribute  Expression Trees A binary tree representation of the logical expression\n "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/phys2001/",
	"title": "PHYS2001",
	"tags": [],
	"description": "",
	"content": "PHYS2001 as taught by Kathleen Koenig\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/programming-languages/",
	"title": "Programming Languages",
	"tags": [],
	"description": "",
	"content": "Programming Languages as taught by William Hawkins III\n8/23/2021 Definitions:  programming language: A system of communicating computational ideas between people and computing machines. high-level programming language: A programming language that is independent of any particular machine architecture. syntax: The rules for constructing structurally valid (note: valid, not correct) computer programs in a particular language. names: A means of identifying \u0026ldquo;entities\u0026rdquo; in a programming language. types: A type denotes the kinds of values that a program can manipulate. (A more specific definition will come later in the course). semantics: The effect of each statement\u0026rsquo;s execution on the program\u0026rsquo;s operation.  Concepts: There are four fundamental components of every programming language: syntax, names, types, and semantics.\nPractice:   For your favorite programming language, attempt to explain the most meaningful parts of its syntax. Think about what are valid identifiers in the language, how statements are separated, whether blocks of code are put inside braces, etc.\n  Next, ask yourself how your favorite language handles types. Are variables in the language given types explicitly or implicitly? If the language is compiled, are types assigned to variables before the code is compiled or as the program executes? These are issues that we will discuss in detail in Module 2.\n  Finally, think about how statements in your favorite programming language affect the program\u0026rsquo;s execution. Does the language have loops? if-then statements? function composition? goto?\n  8/25/2021 The value of studying PLs  Every new language that you learn gives you new ways to think about and solve problems.  There is a parallel here with natural languages. Certain written/spoken languages have words for concepts that others do not. Linguists have said that people can only conceive ideas for which there are words. In certain programming languages there may be constructs (\u0026ldquo;words\u0026rdquo;) that give you a power to solve problems and write algorithms in new, interesting ways.   You can choose the right tool for the job.  When all you have is a hammer, everything looks like a nail.   Makes you an increasingly flexible programmer.  The more you know about the concepts of programming languages (and the PLs that you know) the easier it is to learn new languages.   Using PLs better Studying PLs will teach you about how languages are implemented. This \u0026ldquo;awareness\u0026rdquo; can give you insight into the \u0026ldquo;right way\u0026rdquo; to do something in a particular language. For instance, if you know that recursion and looping are equally performant and computationally powerful, you can choose to use the one that improves the readability of your code. However, if you know that iteration is faster (and that\u0026rsquo;s important for your application) then you will choose that method for invoking statements repeatedly.  Programming domains  We write programs to solve real-world problems. The problems that we are attempting to solve lend themselves to programming languages with certain characteristics. Some of those real-world problems are related to helping others solve real-world problems (systems programs):  e.g., operating systems, utilities, compilers, interpreters, etc. There are a number of good languages for writing these applications: C, C++, Rust, Python, Go, etc.   But, most of programs are designed/written to solve actually real-world problems:  scientific calculations: these applications need to be fast (parallel?) and mathematically precise (work with numbers of many kinds). Scientific applications were the earliest programming domain and inspired the first high-level programming language, Fortran. artificial intelligence: AI applications manipulate symbols (in particular, lists of symbols) as opposed to numbers. This application requirement gave rise to a special type of language designed especially for manipulating lists, Lisp (List Processor). world wide web: WWW applications must embed code in data (HTML). Because of how WWW applications advance so quickly, it is important that languages for writing these applications support rapid iteration. Common languages for writing web applications are PERL, Python, JavaScript, Ruby, Go, etc. business: business applications need to produce reports, process character-based data, describe and store numbers with specific precision (aka, decimals). COBOL has traditionally been the language of business applications, although new business applications are being written in other languages these days (Java, the .Net languages). machine learning: machine learning applications require sophisticated math and algorithms and most developers do not want to rewrite these when good alternatives are available. For this reason, a language with a good ecosystem of existing libraries makes an ideal candidate for writing ML programs (Python). game development: So-called AAA games must be fast enough to generate lifelike graphics and immersive scenes in near-real time. For this reason, games are often written in a language that is expressive but generates code that is optimized, C++.    This list is non-exhaustive, obviously!\nThe John von Neumann Model of Computing  This computing model has had more influence on the development of PLs than we can imagine. There are two hardware components in this Model (the processor [CPU] and the memory) and they are connected by a pipe.  The CPU pipes data and instructions (see below) to/from the memory (fetch). The CPU reads that data to determine the action to take (decode). The CPU performs that operation (execute). Because there is only one path between the CPU and the memory, the speed of the pipe is a bottleneck on the processor\u0026rsquo;s efficiency.   The Model is interesting because of the way that it stores instructions and data together in the same memory. It is different than the Harvard Architecture where programs and data are stored in different memory. In the Model, every bit of data is accessible according to its address. Sequential instructions are placed nearby in memory.  For instance, in     for (int i = 0; i \u0026lt; 100; i++) { statement1; statement2; statement3; } statement1, statement2 and statement3 are all stored one after the other in memory.\n Modern implementations of the Model make fetching nearby data fast. Therefore, implementing repeated instructions with loops is faster than implementing repeated loops with recursion. Or is it? This is a particular case where learning about PL will help you as a programmer!  8/27/2021 Programming Paradigms  A paradigm is a pattern or model. A programming paradigm is a pattern of problem-solving thought that underlies a particular genre of programs and languages.  According to their syntax, names and types, and semantics, it is possible to classify languages into one of four categories (imperative, object-oriented, functional and logic). That said, modern researchers in PL are no longer as convinced that these are meaningful categories because new languages are generally a collection of functionality and features and contain bits and pieces from each paradigm.   The paradigms:  Imperative: Imperative languages are based on the centrality of assignment statements to change program state, selection statements to control program flow, loops to repeat statements and procedures for process abstraction (a term we will learn later).  These languages are most closely associated with the von Neumann architecture, especially assignment statements that approximate the piping operation at the hardware level. Examples of imperative languages include C, Fortran, Cobol, Perl.   Object-oriented: Object-oriented languages are based upon a combination of data abstraction, data hiding, inheritance and message passing.  Objects respond to messages by modifying their internal data \u0026ndash; in other words, they become active. The power of inheritance is that an object can reuse an implementation without having to rewrite the code. These languages, too, are closely associated with the von Neumann architecture and (usually) inherit selection statements, assignment statements and loops from imperative programming languages. Examples of object-oriented languages include Smalltalk, Ruby, C++, Java, Python, JavaScript.   Functional: Functional programming languages are based on the concept that functions are first-class objects in the language \u0026ndash; in other words, functions are just another type like integers, strings, etc.  In a functional PL, functions can be passed to other functions as parameters and returned from functions. The loops and selection statements of imperative programming languages are replaced with composition, conditionals, and recursion in functional PLs. A subset of functional PLs are known as pure functional PLs because functions those languages have no side-effects (a side-effect occurs in a function when that function performs a modification that can be seen outside the function \u0026ndash; e.g., changing a value of a parameter, changing a global variable, etc). Examples of functional languages include Lisp, Scheme, Haskell, ML, JavaScript, Python.   Logic: Simply put, logic programming languages are based on describing what to compute and not how to compute it.  Prolog (and its variants) are really the only logic programming language in widespread use.      Language Evaluation Criteria (New Material Alert) There are four (common) criteria for evaluating a programming language:\n Readability: A metric for describing how easy/hard it is to comprehend the meaning of a computer program written in a particular language.   Overall simplicity: The number of basic concepts that a PL has.\n Feature multiplicity: Having more than one way to accomplish the same thing. Operator overloading: Operators perform different computation depending upon the context (i.e., the type of the operands) Simplicity can be taken too far. Consider machine language.    Orthogonality: How easy/hard it is for the constructs of a language to be combined to build higher-level control and data structures.\n Alternate definition: The mutual independence of primitive operations. Orthogonal example: any type of entity in a language can be passed as a parameter to a function. Non-orthogonal example: only certain entities in a language can be used as a return value from a function (e.g., in C/C++ you cannot return an array). This term comes from the mathematical concept of orthogonal vectors where orthogonal means independent. The more orthogonal a language, the fewer exceptional cases there are in the language\u0026rsquo;s semantics. The more orthogonal a language, the slower the language: The compiler/interpreter must be able to compute based on every single possible combination of language constructs. If those combinations are restricted, the compiler can make optimizations and assumptions that will speed up program execution.    Data types: Data types make it easier to understand the meaning of variables.\n e.g., the difference between int userHappy = 0; and bool userHappy = True;    Syntax design\n A PL\u0026rsquo;s reserved words should make things clear. For instance, it is easier to match the beginnings and endings of loops in a language that uses names rather than { }s. The PL\u0026rsquo;s syntax should evoke the operation that it is performing.  For instance, a + should perform some type of addition operation (mathematical, concatenation, etc)       Writeability  Includes all the aspects of Readability, and Expressiveness: An expressive language has relatively convenient rather than cumbersome way of specifying computations.   Reliability: How likely is it that a program written in a certain PL is correct and runs without errors.  Type checking: a language with type checking is more reliable than one without type checking; type checking is testing for operations that compute on variables with incorrect types at compile time or runtime.  Type checking is better done at runtime. A strongly typed programming language is one that is always able to detect type errors either at compile time or runtime.   Exception handling (the ability of a program to intercept runtime errors and take corrective action) and aliasing (when two or more distinct names in a program point to the same resource) affect the PL\u0026rsquo;s reliability.        In truth, there are so many things that affect the reliability of a PL.               The easier a PL is to read and write, the more reliable the code is going to be.   Cost: The cost of writing a program in a certain PL is a function of  The cost to train programmers to use that language The cost of writing the program in that language The time/speed of execution of the program once it is written The cost of poor reliability The cost of maintenance \u0026ndash; most of the time spent on a program is in maintaining it and not developing it!    8/30/2021 Today we learned a more complete definition of imperative programming languages and studied the defining characteristics of variables. Unfortunately we did not get as far as I wanted during the class which means that there is some new material in this edition of the Daily PL!\nImperative Programming Languages Any language that is an abstraction of the von Neumann Architecture can be considered an imperative programming language.\nThere are 5 calling cards of imperative programming languages:\n state, assignment statements, and expressions: Imperative programs have state. Assignment statements are used to modify the program state with computed values from expressions  state: The contents of the computer\u0026rsquo;s memory as a program executes. expression: The fundamental means of specifying a computation in a programming language. As a computation, they produce a value. assignment statement: A statement with the semantic effect of destroying a previous value contained in memory and replacing it with a new value. The primary purpose of the assignment statement is to have a side effect of changing values in memory. As Sebesta says, \u0026ldquo;The essence of the imperative programming languages is the dominant role of the assignment statement.\u0026rdquo;   variables: The abstraction of the memory cell. loops: Iterative form of repetition (for, while, do \u0026hellip; while, foreach, etc) selection statements: Conditional statements (if/then, switch, when) procedural abstraction: A way to specify a process without providing details of how the process is performed. The primary means of procedural abstraction is through definition of subprograms (functions, procedures, methods).  Variables There are 6 attributes of variables. Remember, though, that a variable is an abstraction of a memory cell.\n type: Collection of a variable\u0026rsquo;s valid data values and the collection of valid operations on those values. name: String of characters used to identify the variable in the program\u0026rsquo;s source code. scope: The range of statements in a program in which a variable is visible. Using the yet-to-be-defined concept of binding, there is an alternative definition: The range of statements where the name\u0026rsquo;s binding to the variable is active. lifetime: The period of time during program execution when a variable is associated with computer memory. address: The place in memory where a variable\u0026rsquo;s contents (value) are stored. This is sometimes called the variable\u0026rsquo;s l-value because only a variable associated with an address can be placed on the left side of an assignment operator. value: The contents of the variable. The value is sometimes call the variable\u0026rsquo;s r-value because a variable with a value can be used on the right side of an assignment operator.  Looking forward to Binding (New Material Alert) A binding is an association between an attribute and an entity in a programming language. For example, you can bind an operation to a symbol: the + symbol can be bound to the addition operation.\nBinding can happen at various times:\n Language design (when the language\u0026rsquo;s syntax and semantics are defined or standardized) Language implementation (when the language\u0026rsquo;s compiler or interpreter is implemented) Compilation Loading (when a program [either compiled or interpreted] is loaded into memory) Execution  A static binding occurs before runtime and does not change throughout program execution. A dynamic binding occurs at runtime and/or changes during program execution.\nNotice that the six \u0026ldquo;things\u0026rdquo; we talked about that characterize variables are actually attributes!! In other words, those attributes have to be bound to variables at some point. When these bindings occur is important for users of a programming language to understand. We will discuss this on Wednesday! blob:https://1492301-4.kaf.kaltura.com/903896d9-2341-4dd3-9709-ca344de08719\n9/1/2021 Welcome to the Daily PL for September 1st, 2021! As we turn the page from August to September, we started the month discussing variable lifetime and scope. Lifetime is related to the storage binding and scope is related to the name binding. Before we learned that new material, however, we went over an example of the different bindings and their times in an assignment statement.\nBinding Example Consider a Python statement like this:\nvrb = arb + 5 Recall that a binding is an association between an attribute and an entity. What are some of the possible bindings (and their times) in the statement above?\n The symbol + (entity) must be bound to an operation (attribute). In a language like Python, that binding can only be done at runtime. In order to determine whether the operation is a mathematical addition, a string concatenation or some other behavior, the interpreter needs to know the type of arb which is only possible at runtime. The numerical literal 5 (entity) must be bound to some in-memory representation (attribute). For Python, it appears that the interpreter chooses the format for representing numbers in memory (https://docs.python.org/3/library/sys.html#sys.int%5Finfo (Links to an external site.), https://docs.python.org/3/library/sys.html#sys.float%5Finfo (Links to an external site.)) which means that this binding is done at the time of language implementation. The value (attribute) of the variables vrb and arb (entities) are bound at runtime. Remember that the value of a variable is just another binding.  This is not an exhaustive list of the bindings that are active for this statement. In particular, the variables vrb and arb must be bound to some address, lifetime and scope. Discussing those bindings requires more information about the statement\u0026rsquo;s place in the source code.\nVariables' Storage Bindings The storage binding is related to the variable\u0026rsquo;s lifetime (the time during which a variable is bound to memory). There are four common lifetimes:\n  static: Variable is bound to storage before execution and remains bound to the same storage throughout program execution.\n Variables with static storage binding cannot share memory with other variables (they need their storage throughout execution). Variables with static storage binding can be accessed directly (in other words, their access does not require redirection through a pointer) because the address of their storage is constant throughout execution. Direct addressing means that accesses are faster. Storage for variables with static binding does not need to be repeatedly allocated and deallocated throughout execution \u0026ndash; this will make program execution faster. In C++, variables with static storage binding are declared using the static keyword inside functions and classes. Variables with static storage binding are sometimes referred to as history sensitive because they retain their value throughout execution.    stack dynamic: Variable is bound to storage when it\u0026rsquo;s declaration statements are elaborated (the time when a declaration statement is executed).\n Variables with stack dynamic storage bindings make recursion possible because their storage is allocated anew every time that their declaration is elaborated. To fully understand this point it is necessary to understand the way that function invocation is handled using a runtime stack. We will cover this topic next week. Stay tuned! Variables with stack dynamic storage bindings cannot be directly accessed. Accesses must be made through an intermediary which makes them slower. Again, this will make more sense when we discuss the typical mechanism for function invocation. The storage for variables with stack dynamic storage bindings are constantly allocated and deallocated which adds to runtime overhead.  Variables with stack dynamic storage bindings are not history sensitive.\n  Explicit heap dynamic: Variable is bound to storage by explicit instruction from the programmer. E.g., new / malloc in C/C++.\n The binding to storage is done at runtime when these explicit instructions are executed. The storage sizes can be customized for the use. The storage is hard to manage and requires careful attention from the programmer. The storage for variables with explicit heap dynamic storage bindings are constantly allocated and deallocated which adds to runtime overhead.    Implicit heap dynamic: Variable is bound to storage when it is assigned a value at runtime.\n All storage bindings for variables in Python are handled in this way. https://docs.python.org/3/c-api/memory.html (Links to an external site.) When a variable with implicit heap dynamic storage bindings is assigned a value, storage for that variable is dynamically allocated. Allocation and deallocation of storage for variables with implicit heap dynamic storage bindings is handled automatically by the language compiler/interpreter. (More on this when we discuss memory management techniques in Module 3).    Variables' Name Bindings See the Pl for the Video.\nThis new material is presented above as Episode 1 of PL After Dark. Below you will find a written recap!\nScope is the range of statements in which a variable is visible (either referencable or assignable). Using the vocabulary of bindings, scope can also be defined as the collection of statements which can access a name binding. In other words, scope determines the binding of a name to a variable.\nIt is easy to get fooled into thinking that a variable\u0026rsquo;s name is intrinsic to the variable. However, a variable\u0026rsquo;s name is just another binding like address, storage, value, etc. There are two scopes that most languages employ:\n local: A variable is locally scoped to a unit or block of a program if it is declared there. In Python, a variable that is the subject of an assignment is local to the immediate enclosing function definition. For instance, in   def add(a, b): total = a + b return total total is a local variable.\n global: A variable is globally scoped when it is not in any local scope (terribly unhelpful, isn\u0026rsquo;t it?) Using global variables breaks the principles of encapsulation and data hiding.  For a variable that is used that is not local, the compiler/interpreter must determine to which variable the name refers. Determining the name/variable binding can be done statically or dynamically:\nStatic Scoping This is sometimes also known as lexical scoping. Static scoping is the type of scope that can be determined using only the program\u0026rsquo;s source code. In a statically scoped programming language, determining the name/variable binding is done iteratively by searching through a block\u0026rsquo;s nesting static parents. A block is a section of code with its own scope (in Python that is a function or a class and in C/C++ that is statements enclosed in a pair of {}s). The static parent of a block is the block in which the current block was declared. The list of static parents of a block are the block\u0026rsquo;s static ancestors.\nDynamic Scoping Dynamic scoping is the type of scope that can be determined only during program execution. In a dynamically scoped programming language, determining the name/value binding is done iteratively by searching through a block\u0026rsquo;s nesting dynamic parents. The dynamic parent of a block is the block from which the current block was executed. Very few programming languages use dynamic scoping (BASH, PERL [optionally]) because it makes checking the types of variables difficult for the programmer (and impossible for the compiler/interpreter) and because it increases the \u0026ldquo;distance\u0026rdquo; between name/variable binding and use during program execution. However, dynamic binding makes it possible for functions to require fewer parameters because dynamically scoped non local variables can be used in their place.\n9/3/2021 Welcome to The Daily PL for 9/3/2021. We spent most of Friday reviewing material from Episode 1 of PL After Dark and going over scoping examples in C++ and Python. Before continuing, make sure that you have viewed Episode 1 of PL After Dark.\nScope We briefly discussed the difference between local and global scope.\nIt is easy to get fooled into thinking that a variable\u0026rsquo;s name is intrinsic to the variable. However, a variable\u0026rsquo;s name is just another binding like address, storage, value, etc.\nAs a programmer, when a variable is local determining the name/variable binding is straightforward. Determining the name/variable binding becomes more complicated (and more important) when source code uses a non-local name to reference a variable. In cases like this, determining the name/variable binding depends on whether the language is statically or dynamically scoped.\nStatic Scoping This is sometimes also known as lexical scoping. Static scoping is the type of scope that can be determined using only the program\u0026rsquo;s source code. In a statically scoped programming language, determining the name/variable binding is done iteratively by searching through a block\u0026rsquo;s nesting static parents. A block is a section of code with its own scope (in Python that is a function or a class and in C/C++ that is statements enclosed in a pair of {}s). The static parent of a block is the block in which the current block was declared. The list of static parents of a block are the block\u0026rsquo;s static ancestors.\nHere is pseudocode for the algorithm of determining the name/variable binding in a statically scoped programming language:\ndef resolve(name, current_scope) -\u0026gt; variable s = current_scope while (s != InvalidScope) if s.contains(name) return s.variable(name) s = s.static_parent_scope() return NameError For practice doing name/variable binding in a statically scoped language, play around with an example in Python: static_scope.py\nConsider this \u0026hellip; Python and C++ have different ways of creating scopes. In Python and C++ a new scope is created at the beginning of a function definition (and that scope contains the function\u0026rsquo;s parameters automatically). However, Python and C++ differ in the way that scopes are declared (or not!) for variables used in loops. Consider the following Python and C++ code (also available at loop_scope.cpp and loop_scope.py :\ndef f(): for i in range(1, 10): print(f\u0026#34;i (in loop body): {i}\u0026#34;) print(f\u0026#34;i (outside loop body): {i}\u0026#34;) void f() { for (int i = 0; i\u0026lt;10; i++) { std::cout \u0026lt;\u0026lt; \u0026#34;i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // The following statement will cause a compilation error  // because i is local to the code in the body of the for  // loop.  // std::cout \u0026lt;\u0026lt; \u0026#34;i: \u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } In the C++ code, the for loop introduces a new scope and i is in that scope. In the Python code, the for loop does not introduce a new scope and i is in the scope of f. Try to run the following Python code also available here at loop_scope_error.py to see why this distinction is important:\ndef f(): print(f\u0026#34;i (outside loop body): {i}\u0026#34;) for i in range(1, 10): print(f\u0026#34;i (in loop body): {i}\u0026#34;) Dynamic Scoping Dynamic scoping is the type of scope that can be determined only during program execution. In a dynamically scoped programming language, determining the name/value binding is done iteratively by searching through a block\u0026rsquo;s nesting dynamic parents. The dynamic parent of a block is the block from which the current block was executed. Very few programming languages use dynamic scoping (BASH, Perl [optionally] are two examples) because it makes checking the types of variables difficult for the programmer (and impossible for the compiler/interpreter) and because it increases the \u0026ldquo;distance\u0026rdquo; between name/variable binding and use during program execution. However, dynamic binding makes it possible for functions to require fewer parameters because dynamically scoped non local variables can be used in their place.\ndef resolve(name, current_scope) -\u0026gt; variable s = current_scope while (s != InvalidScope) if s.contains(name) return s.variable(name) s = s.dynamic_parent_scope() return NameError For practice doing name/variable binding in a dynamically scoped language, play around with an example in Python: dynamic_scope.py . Note that because Python is intrinsically a statically scoped language, the example includes some hacking of the Python interpreter to emulate dynamic scoping. Compare the dynamic in the aforementioned Python code with the resolve function in the pseudocode and see if there are differences!\nReferencing Environment (New Material Alert) The referencing environment of a statement contains all the name/variable bindings visible at that statement. NOTE: The example in the book on page 224 is absolutely horrendous \u0026ndash; disregard it entirely. Consider the example online here: referencing_environment.py . Play around with that code and make sure that you understand why certain variables are in the referencing environment and others are not.\nIn case you think that this is theoretical and not useful to you as a real, practicing programmer, take a look at the official documentation of the Python execution model and see how the language relies on the concept of referencing environments: naming-and-binding .\nScope and Lifetime Are Not the Same (New Material Alert) It is common for programmers to think that the scope and the lifetime of a variable are the same. However, this is not always true. Consider the following code in C++ (also available at scope_new_lifetime.cpp)\n#include \u0026lt;iostream\u0026gt; void f(void) { static int variable = 4; } int main() { f(); return 0; } In this program, the scope of variable is limited to the function f. However, the lifetime of variable is the entire program. Just something to keep in mind when you are programming!\n9/8/2021 Welcome to The Daily PL for September 8, 2021. I\u0026rsquo;m not lying when I say that this is the best. edition. ever. There is new material included in this edition which will be covered in a forthcoming episode of PL After Dark. When that video is available, this post will be updated!\nRecap The Type Characteristics of a Language\nIn today\u0026rsquo;s lecture we talked about types (again!). In particular, we talked about the two independent axis of types for a programming language: whether a PL is statically or dynamically typed and whether it is strongly or weakly typed. In other words, the time of the binding of type/variable in a language is independent of that language\u0026rsquo;s ability to detect type errors.\n A statically typed language is one where the type/variable binding is done before the code is run and does not change throughout program execution. A dynamically typed language is one where the type/variable binding is done at runtime and/or may change throughout program execution.    A strongly typed language is one where type errors are always detected (either at before or during program execution) A weakly typed language is one that is, well, not strongly typed.   In order to have a completely a satisfying definition of strongly typed language, we defined type error as any error that occurs when an operation is attempted on a type for which it is not well defined. In Python, \u0026quot;3\u0026quot; + 5 results in a TypeError: can only concatenate str (not \u0026quot;int\u0026quot;) to str. In this example, the operation is + and the types are str and int.\nCertain strongly typed languages appear to be weakly typed because of coercions. A coercion occurs when the language implicitly converts a variable of one type to another. C++ allows the programmer to define operations that will convert the type of a variable from, say, type a to type b. If the compiler sees an expression using a variable of type b where only a variable of type a is valid, then it will invoke that conversion operation automatically. While this adds to the language\u0026rsquo;s flexibility, the conversion behavior may hide the fact that a type error exists and, ultimately, make code more difficult to debug. Note that coercions are done implicitly \u0026ndash; a change between types done at the explicit request of the programmer is know as a (type)cast.\nFinally, before digging in to actual types, we defined type system: A type system is the set of types supported by a language and the rules for their usage.\nAggregate Data Types Aggregate data types are data types composed of one or more basic, or primitive, data types. Do not ask me to write a specific definition for primitive data type \u0026ndash; it will only get us into a circular mess :-)\nArray An array is a homogeneous (i.e., all its elements must be of the same type) aggregate data type in which an individual element is accessed by its position (i.e., index) in the aggregate. There are myriad design decisions associated with a language\u0026rsquo;s implementation of arrays (the type of the index, whether their size must be fixed or whether it can be dynamic, etc.) One of those design decisions is the way that a language lays out a two dimensional array in memory. There are two options: row-major order and column-major order. For a second, forget the concept of rows and columns altogether and consider that you access two dimensional arrays by letters and numbers. See the following diagram:\nThe memory of actual computers is linear. Therefore, two dimensional arrays must be flattened. In \u0026ldquo;letter major\u0026rdquo; order, the slices of the array identified by letters are stored in memory one after the other. In \u0026ldquo;number major\u0026rdquo; order, the slices of the array identified by numbers are stored in memory one after another. Notice that, in \u0026ldquo;letter major\u0026rdquo; order, the numbers \u0026ldquo;change fastest\u0026rdquo; and that, in \u0026ldquo;number major\u0026rdquo; order, the letters \u0026ldquo;change fastest\u0026rdquo;.\nSubstitute \u0026ldquo;row\u0026rdquo; for \u0026ldquo;letter\u0026rdquo; and \u0026ldquo;column\u0026rdquo; for \u0026ldquo;number\u0026rdquo; and, voila, you understand!! The C programming language stores arrays in row-major order; Fortran stores arrays in column-major order.\nKeep in mind that this description is only one way (or many) to store two dimensional arrays. There are (Links to an external site.) others (Links to an external site.).\nAssociative Arrays, Records, Tuples, Lists, Unions, Algebraic Data Types, Pattern Matching, List Comprehensions, and Equivalence All that, and more, in Episode 2 of PL After Dark!\nNote: In this video, I said that Python\u0026rsquo;s Lists function as arrays and that Python does not have true arrays. Your book implies as much in the section on Lists. However, I went back to check, and it does appear that there is a standard module in Python that provides arrays, in certain cases. Take a look at the documentation here: python arrays . The commonly used NumPy package also provides an array type: numpy arrays . While the language, per se, does not define an array type, the presence of the modules (particularly the former) is important to note. Sorry for the confusion!\n9/10/2021 In today\u0026rsquo;s edition of the Daily PL we will recap our discussion from today that covered expressions, order of evaluation, short-circuit evaluation and referential transparency.\nExpressions An expression is the means of specifying computations in a programming language. Informally, it is anything that yields a value. For example,\n 5 is an expression (value 5) 5 + 2 is an expression (value 7) Assuming fun is a function that returns a value, fun() is an expression (value is the return value) Assuming f is a variable, f is an expression (the value is the value of the variable)  Certain languages allow more exotic statements to be expressions. For example, in C/C++, the = operator yields a value (the value of the expression on the right operand). It is this choice by the language designer that allows a C/C++ programmer to write\nint a, b, c, d; a = b = c = d = 5; to initialize all four variables to 5.\nWhen we discuss functional programming languages, we will see how many more things are expressions that programmers typically think are simply statements.\nOrder of Evaluation Programmers learn the associativity and precedence of operations in their languages. That knowledge enables them to mentally calculate the value of statements like 5 + 4 * 3 / 2.\nWhat programmers often forget to learn about their language, is the order of evaluation of operands. Take several of those constants from the previous expression and replace them with variables and function calls:\n5 + a() * c / b() The questions abound:\n Is a() executed before the value of variable c is retrieved? Is b() executed before c()? Is b() executed at all?  In a language with functional side effects, the answer to these questions matter. Why? Consider that a could have a side effect that changes c. If the value of c is retrieved before the execution of a() then the expression will evaluate to a certain value and if the value of c is retrieved after execution of a() then the expression will evaluate to a different value.\nCertain languages define the order of evaluation of operands (Python, Java) and others do not (C/C++). There are reasons why defining the order is a good thing:\n The programmer can depend on that order and benefit from the consistency The program\u0026rsquo;s readability is improved. The program\u0026rsquo;s reliability is improved.  But there is at least one really good reason for not defining that order: optimizations. If the compiler/interpreter can move around the order of evaluation of those operands, it may be able to find a way to generate faster code!\nShort-circuit Evaluation Languages with short-circuit evaluation take these potential optimizations one step further. For a boolean expression, the compiler will stop evaluating the expression as soon as the result is fixed. For instance, in a() \u0026amp;\u0026amp; b(), if a() is false, then the entire statement will always be false, no matter the value of b(). In this case, the compiler/interpreter will simply not execute b(). On the other hand, in a() || b() if a() is true, then the entire statement will always be true, no matter the value of b(). In this case, the compiler/interpreter will simply not execute b().\nA programmer\u0026rsquo;s reliance on this type of behavior in a programming language is very common. For instance, this is a common idiom in C/C++:\nint *variable = nullptr; ... if (variable != nullptr \u0026amp;\u0026amp; *variable \u0026gt; 5) { ... } In this code, the programmer is checking to see whether there is memory allocated to variable before they attempt to read that memory. This is defensive programming thanks to short-circuit evaluation.\nReferential Transparency Most of these issues would not be a problem if programmer\u0026rsquo;s wrote functions that did not have side effects (remember that those are called pure functions). There are languages that will not allow side effects and those languages support referential transparency: A function has referential transparency if its value (its output) depends only on the value of its parameter(s). In other words, if given the same inputs, a referentially transparent function always gives the same output.\nPut It All Together Try you hand at the practice quiz Expressions, precedence, associativity and coercions to check your understanding of the material we covered in class on Friday and the material from your assigned reading! For the why, check out relational.cpp .\n9/13/2021 In today\u0026rsquo;s edition of the Daily PL we will recap our discussion from today that covered subprograms, polymorphism and coroutines!\nSubprograms A subprogram is a type of abstraction. It is process abstraction where the how of a process is hidden from the user who concerns themselves only with the what. A subprogram provides process abstraction by naming a collection of statements that define parameterized computations. Again, the collection of statements determines how the process is implemented. Subprogram parameters give the user the ability to control the way that the process executes. There are three types of subprograms:\n Procedure: A subprogram that does not return a value. Function: A subprogram that does return a value. Method: A subprogram that operates with an implicit association to an object; a method may or may not return a value.  Pay close attention to the book\u0026rsquo;s explanation and definitions of terms like parameter, parameter profile, argument, protocol, definition, and declaration.\nSubprograms are characterized by three facts:\n A subprogram has only one entry point Only one subprogram is active at any time Program execution returns to the caller upon completion  Polymorphism Polymorphism allows subprograms to take different types of parameters on different invocations. There are two types of polymorphism:\n ad-hoc polymorphism: A type of polymorphism where the semantics of the function may change depending on the parameter types. parametric polymorphism: A type of polymorphism where subprograms take an implicit/explicit type parameter used to define the types of their subprogram\u0026rsquo;s parameters; no matter the value of the type parameter, in parametric polymorphism the subprogram\u0026rsquo;s semantics are always the same.  Ad-hoc polymorphism is sometimes call function overloading (C++). Subprograms that participate in ad-hoc polymorphism share the same name but must have different protocols. If the subprograms' protocols and names were the same, how would the compiler/interpreter choose which one to invoke? Although a subprogram\u0026rsquo;s protocol includes its return type, not all languages allow ad-hoc polymorphism to depend on the return type (e.g., C++). See the various definitions of add in the C++ code here: subprograms.cpp . Note how they all have different protocols. Further, note that not all the versions of the function add perform an actual addition! That\u0026rsquo;s the point of ad-hoc polymorphism \u0026ndash; the programmer can change the meaning of a function.\nFunctions that are parametrically polymorphic are sometimes called function templates (C++) or generics (Java, soon to be in Go, Rust). A parametrically polymorphic function is like the blueprint for a house with a variable number of floors. A home buyer may want a home with three stories \u0026ndash; the architect takes their variably floored house blueprint and \u0026ldquo;stamps out\u0026rdquo; a version with three floors. Some \u0026ldquo;new\u0026rdquo; languages call this process monomorphization (Links to an external site.). See the definition of minimum in the C++ code here: subprograms.cpp . Note how there is only one definition of the function. The associated type parameter is T. The compiler will \u0026ldquo;stamp out\u0026rdquo; copies of minimum for different types when it is invoked. For example, if the programmer writes\nauto m = minimum(5, 4); then the compiler will generate\nint minimum(int a, int b) { return a \u0026lt; b ? a : b; } behind the scenes.\nCoroutines Just when you thought that you were getting the hang of subprograms, a new kid steps on the block: coroutines. Sebesta defines coroutines as a subprogram that cooperates with a caller. The first time that a programmer uses a coroutine, they call it at which point program execution is transferred to the statements of the coroutine. The coroutine executes until it yields control. The coroutine may yield control back to its caller or to another coroutine. When the coroutine yields control, it does not cease to exist \u0026ndash; it simply goes dormant. When the coroutine is again invoked \u0026ndash; resumed \u0026ndash; the coroutine begins executing where it previously yielded. In other words, coroutines have\n multiple entry points full control over execution until they yield the property that only one is active at a time (although many may be dormant)  Coroutines could be used to write a card game. Each player is a coroutine that knows about the player to their left (that is, a coroutine). The PlayerA coroutine performs their actions (perhaps drawing a card from the deck, etc) and checks to see if they won. If they did not win, then the PlayerA coroutine yields to the PlayerB coroutine who performs the same set of actions. This process continues until a player no longer has someone to their left. At that point, everything unwinds back to the point where PlayerA was last resumed \u0026ndash; the signal that a round is complete. The process continues by resuming PlayerA to start another round of the game. Because each player is a coroutine, it never ceased to exist and, therefore, retains information about previous draws from the deck. When a player finally wins, the process completes. To see this in code, check out cardgame.py .\n9/20/2021 This is an issue of the Daily PL that you are going to want to make sure that you keep safe \u0026ndash; definitely worth framing and passing on to your children! You will want to make sure you remember where you were when you first learned about \u0026hellip;\nFormal Program Semantics Although we have not yet learned about it (we will, don\u0026rsquo;t worry!), there is a robust set of theory around the way that PL designers describe the syntax of their language. You can use regular expressions, context-free grammars, parsers (recursive-descent, etc) and other techniques for defining what is a valid program.\nOn the other hand, there is less of a consensus about how a program language designer formally describes the semantics of programs written in their language. The codification of the semantics of a program written in a particular is known as formal program semantics. In other words, formal program semantics are a precise mathematical description of the semantics of an executing program. Sebesta uses the term dynamic semantics which is defines as the \u0026ldquo;meaning[] of the expressions, statements and program units of a programming language.\u0026rdquo;\nThe goal of defining formal program semantics is to understand and reason about the behavior of programs. There are many, many reasons why PL designers want a formal semantics of their language. However, there are two really important reasons: With formal semantics it is possible to prove that\n two programs calculate the same result (in other words, that two programs are equivalent), and a program calculates the correct result.  The alternative to formal program semantics are standards promulgated by committees that use natural language to define the meaning of program elements. Here is an example of a page from the standard for the C programming language:\n If you are interested, you can find the C++ language standard , the Java language standard , the C language standard , the Go language standard and the Python language standard all online.\nTesting vs Proving There is absolutely a benefit to testing software. No doubt about it. However, testing that a piece of software behaves a certain way does not prove that it operates a certain way.\n\u0026ldquo;Program testing can be used to show the presence of bugs, but never to show their absence!\u0026quot; - Edsger Dijkstra\nThere is an entire field of computer science known as formal methods whose goal is to understand how to write software that is provably correct. There are systems available for writing programs about which things can be proven. There is PVS, Coq ,Isabelle , and TLA+ , to name a few. PVS is used by NASA to write its mission-critical software and even it makes an appearance in the movie The Martian .\nThree Types of Formal Semantics There are three common types of formal semantics. It is important that you know the names of these systems, but we will only focus on one in this course!\n Operational Semantics: The meaning of a program is defined by how the program executes on an idealized virtual machine. Denotational Semantics: Program units \u0026ldquo;denote\u0026rdquo; mathematical functions and those functions transform the mathematically defined state of the program. Axiomatic Semantics: The meaning of the program is based on proof rules for each programming unit with an emphasis on proving the correctness of a program.  We will focus on operational semantics only!\nOperational Semantics Program State We have referred to the state of the program throughout this course. We have talked about how statements in imperative languages can have side effects that affect the value of the state and we have talked about how the assignment statement\u0026rsquo;s raison d\u0026rsquo;etre is to change a program\u0026rsquo;s state. For operational semantics, we have to very precisely define a program\u0026rsquo;s state.\nAt all times, a program has a state. A state is just a function whose domain is the set of defined program variables and whose range is V * T where V is the set of all valid variable values (e.g., 5, 6.0, True, \u0026ldquo;Hello\u0026rdquo;, etc) and T is the set of all valid variable types (e.g., Integer, Floating Point, Boolean, String, etc). In other words, you can ask a state about a particular variable and, if it is defined, the function will return the variable\u0026rsquo;s current value and its type.\nIt is important to note that PL researchers have math envy. They are not mathematicians but they like to use Greek symbols. So, here we go:\n\\begin{equation*} \\sigma(x) = (v, \\tau) \\end{equation*}\nThe state function is denoted with the  .  always represents some arbitrary variable type. Generally, v represents a value. So, you can read the definition above as \u0026ldquo;Variable x has value v and type  in state .\u0026rdquo;\nProgram Configuration Between execution steps (a term that we will define shortly), a program is always in a particular configuration:\n\\begin{equation*} \u0026lt;e, \\sigma\u0026gt; \\end{equation*}\nThis means that the program in state \nis about to evaluate expression e.\nProgram Steps A program step is an atomic (indivisible) change from one program configuration to another. Operational semantics defines steps using rules. The general form of a rule is\n\\begin{equation*} \\frac{premises}{conclusion} \\end{equation*}\nThe conclusion is generally written like \u0026lt;e, \u0026gt;  (v, , ). This statement means that, when the premises hold, the rule evaluates to a value (v), type () and (possibly modified) state (') after a single step of execution of a program in configuration \u0026lt;e, \u0026gt;. Note that rules do not yield configurations. All this will make sense when we see an example.\nExample 1: Defining the semantics of variable access. In STIMPL, the expression to access a variable, say i, is written like Variable(\u0026ldquo;i\u0026rdquo;). Our operational semantic rule for evaluating such an access should \u0026ldquo;read\u0026rdquo; something like: When the program is about to execute an expression to access variable i in a state , the value of the expression will be the triple of i\u0026rsquo;s value, i\u0026rsquo;s type and the unchanged state .\u0026rdquo; In other words, the evaluation of the next step of a program that is about to access a value is the value and type of the variable being accessed and the program\u0026rsquo;s state is unchanged.\nLet\u0026rsquo;s write that formally!\n\\begin{equation*} \\(\\frac{\\sigma(x) \\rightarrow (v, \\tau)} {\u0026lt;\\text{Variable}(x), \\sigma \u0026gt; \\rightarrow (v, \\tau, \\sigma)}\\) \\end{equation*}\nState Update How do we write down that the state is being changed? Why would we want to change the state? Let\u0026rsquo;s answer the second question first: we want to change the state when, for example, there is an assignment statement. If  (\u0026ldquo;i\u0026rdquo;) = (4, Integer) and then the program evaluated an expression like Assign(Variable(\u0026ldquo;i\u0026rdquo;), IntLiteral(2)), we don\u0026rsquo;t want the \nfunction to return (4, Integer) any more! We want it to return (2, Integer). We can define that mathematically like:\n\\begin{equation*} \\sigma[(v,\\tau)/x](y)= \\begin{cases} \u0026amp; \\sigma(y) \\quad y \\ne x \\ \u0026amp;(v,\\tau) \\quad y=x \\end{cases} \\end{equation*}\nThis means that if you are querying the updated state for the variable that was just reassigned (x), then return its new value and type (m and  ). Otherwise, just return the value that you would get from accessing the existing \n.\nExample 2: Defining the semantics of variable assignment (for a variable that already exists). In STIMPL, the expression to overwrite the value of an existing variable, say i, with, say, an integer literal 5 is written like Assign(Variable(\u0026quot;i\u0026quot;), IntLiteral(5)). Our operational semantic rule for evaluating such an assignment should \u0026ldquo;read\u0026rdquo; something like: When the program is about to execute an expression to assign variable i to the integer literal 5 in a state  and the type of the variable i in state  is Integer, the value of the expression will be the triple of 5, Integer and the changed state ' which is exactly the same as state \nexcept where (5, Integer) replaced i\u0026rsquo;s earlier contents.\u0026quot; That\u0026rsquo;s such a mouthful! But, I think we got it. Let\u0026rsquo;s replace some of those literals with symbols for abstraction purposes and then write it down!\n\\begin{equation*} \\frac{\u0026lt;e, \\sigma\u0026gt; \\longrightarrow (v, \\tau, \\sigma'), \\sigma(x) \\longrightarrow (*,, \\tau)} {\u0026lt;\\text{Assign(Variable)}(x, e), \\sigma \u0026gt; \\longrightarrow (v, \\tau, \\sigma' [(v, \\tau)/x])} \\end{equation*}\nLet\u0026rsquo;s look at it step-by-step:\n\\begin{equation*} \u0026lt;Assign(Variable(x),e),\\sigma\u0026gt; \\end{equation*}\nis the configuration and means that we are about to execute an expression that will assign value of expression e to variable x. But what is the value of expression e? The premise\n\\begin{equation} \u0026lt;e,\\sigma\u0026gt;(v,\\tau, \\sigma) \\end{equation}\ntells us that the value and type of e when evaluated in state  is v, and . Moreover, the premise tells us that the state may have changed during evaluation of expression e and that subsequent evaluation should use a new state, \n\u0026lsquo;. Our mouthful above had another important caveat: the type of the value to be assigned to variable x must match the type of the value already stored in variable x. The second premise\n\\begin{equation*} \\sigma(x)\\longrightarrow(*, \\tau) \\end{equation*}\ntells us that the types match \u0026ndash; see how the s are the same in the two premises? (We use the * to indicate that we don\u0026rsquo;t care what that value is!)\nNow we can just put together everything we have and say that the expression assigning the value of expression e to variable x evaluates to\n\\begin{equation*} (v,\\tau,\\sigma[(v,\\tau)/x]) \\end{equation*}\nThat\u0026rsquo;s Great, But Show Me Code! Well, Will, that\u0026rsquo;s fine and good and all that stuff. But, how do I use this when I am implementing STIMPL? I\u0026rsquo;ll show you! Remember the operational semantics for variable access:\n\\begin{equation*} \\(\\frac{\\sigma(x) \\rightarrow (v, \\tau)} {\u0026lt;\\text{Variable}(x), \\sigma \u0026gt; \\rightarrow (v, \\tau, \\sigma)}\\) \\end{equation*}\nCompare that with the code for it\u0026rsquo;s implementation in the STIMPL skeleton that you are provided for Assignment 1:\ndef evaluate(expression, state): ... case Variable(variable_name=variable_name): value = state.get_value(variable_name) if value == None: raise InterpSyntaxError(f\u0026#34;Cannot read from {variable_name}before assignment.\u0026#34;) return (*value, state) At this point in the code we are in a function named evaluate whose first parameter is the next expression to evaluate and whose second parameter is a state. Does that sound familiar? That\u0026rsquo;s because it\u0026rsquo;s the same as a configuration! We use pattern matching to select the code to execute. The pattern is based on the structure of expression and we match in the code above when expression is a variable access. Refer to Pattern Matching in Python for the exact form of the syntax. The state variable is an instance of the State object that provides a method called get_value (see Assignment 1: Implementing STIMPL for more information about that function) that returns a tuple of (v, ) In other words, get_value works the same as . So,\nvalue = state.get_value(variable_name) is a means of implementing the premise of the operational semantics.\nreturn (*value, state) yields the final result! Pretty cool, right?\nLet\u0026rsquo;s do the same analysis for assignment:\n\\(\\frac{\u0026lt;e,\\sigma\u0026gt;\\longrightarrow(v,\\tau,\\sigma),\\sigma(x)\\longrightarrow(*,\\tau)}{\u0026lt;Assign(Variable(x),e),\u0026gt;\\longrightarrow(v,\\tau,[(v,\\tau)/x])}\\)\nAnd here\u0026rsquo;s the implementation:\ndef evaluate(expression, state): ... case Assign(variable=variable, value=value): value_result, value_type, new_state = evaluate(value, state) variable_from_state = new_state.get_value(variable.variable_name) _, variable_type = variable_from_state if variable_from_state else (None, None) if value_type != variable_type and variable_type != None: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Assignment: Cannot assign {value_type}to {variable_type}\u0026#34;\u0026#34;\u0026#34;) new_state = new_state.set_value(variable.variable_name, value_result, value_type) return (value_result, value_type, new_state) First, look at\nvalue_result, value_type, new_state = evaluate(value, state) which is how we are able to find the values needed to satisfy the left-hand premise. value_result is v, value_type is  and new_state is \u0026rsquo;.\nvariable_from_state = new_state.get_value(variable.variable_name) is how we are able to find the values needed to satisfy the right-hand premise. Notice that we are using new_state (') to get variable.variable_name (x). There is some trickiness in_, variable_type = variable_from_state if variable_from_state else (None, None) to set things up in case we are doing the first assignment to the variable (which sets its type), so ignore that for now! Remember that in our premises we guaranteed that the type of the variable in state ' matches the type of the expression:\nif value_type != variable_type and variable_type != None: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Assignment: Cannot assign {value_type}to {variable_type}\u0026#34;\u0026#34;\u0026#34;) performs that check!\nnew_state = new_state.set_value(variable.variable_name, value_result, value_type) generates a new, new state ([(v,)/x]) and\nreturn (value_result, value_type, new_state) yields the final result!\n9/22/2021 Like other popular newspapers that do in-depth analysis of popular topics (Links to an external site.), this edition of the Daily PL is part 2/2 of an investigative report on \u0026hellip;\nFormal Program Semantics In our previous class, we discussed the operational semantics of variable access and variable assignment. In this class we explored the operational semantics of the addition operator and the if/then statement.\nA Quick Review of Concepts At all times, a program has a state. A state is just a function whose domain is the set of defined program variables and whose range is V * T where V is the set of all valid variable values (e.g., 5, 6.0, True, \u0026ldquo;Hello\u0026rdquo;, etc) and T is the set of all valid variable types (e.g., Integer, Floating Point, Boolean, String, etc). In other words, you can ask a state about a particular variable and, if it is defined, the function will return the variable\u0026rsquo;s current value and its type.\nHere is the formal definition of the state function:\n\\begin{equation*} \\(\\sigma(x) = (v, \\tau)\\) \\end{equation*}\nThe state function is denoted with the  .  always represents some arbitrary variable type. Generally, v represents a value. So, you can read the definition above as \u0026ldquo;Variable x has value v and type  in state .\u0026rdquo;\nBetween execution steps, a program is always in a particular configuration:\n\\begin{equation*} \u0026lt;e, \\sigma\u0026gt; \\end{equation*}\nThis notation means that the program in state  is about to evaluate expression e.\nA program step is an atomic (indivisible) change from one program configuration to another. Operational semantics defines steps using rules. The general form of a rule is\n\\begin{equation*} \\frac{premises}{conclusion} \\end{equation*}\nThe conclusion is generally written like \u0026lt;e, \u0026gt;  (v, , ) which means that when the premises hold, the expression e evaluated in state  evaluates to a value (v), type () and (possibly modified) state (') after a single step of execution.\nDefining the Semantics of the Addition Expression In STIMPL, the expression to \u0026ldquo;add\u0026rdquo; two values n1 and n2 is written like Add(n1, n2). By the rules of the STIMPL language, for an addition to be possible, n1 and n2 must\n have the same type and have Integer, Floating Point or String type.  Because every unit in STIMPL has a value, we will define the operational semantics using two arbitrary expressions, e1 and e2. The program configuration to which we are giving semantics is\n\\begin{equation*} \u0026lt;Add(e_1),e_2),\\sigma\u0026gt; \\end{equation*}\nBecause our addition operator applies when its operands are three different types, we technically need three different rules for its evaluation. Let\u0026rsquo;s start with the operational semantics for add when its operands are of type Integer:\n\\begin{equation*} \\frac{\u0026lt;e_1,\\sigma\u0026gt;(v_1,Integer,\\sigma),\u0026lt;e_2,\\sigma\u0026gt;(v_2,Integer,\\sigma \\prime)}{\u0026lt;Add(e1,e2),\u0026gt;(v1+v2,Integer,\\sigma\\prime)} \\end{equation*}\nLet\u0026rsquo;s look at the premises. First, there is\n\\begin{equation*} \u0026lt;e_1,\\sigma\u0026gt;(v1,Integer,\\sigma \\prime) \\end{equation*}\nwhich means that, when evaluated in state , expression e1 has the value v1 and type Integer and may modify the state (to '). Notice that we are not using  for the resulting type of the evaluation? Why? Because using  indicates that this rule applies when the evaluation of e1 in state  evaluates to any type (which we \u0026ldquo;assign\u0026rdquo; to  in case we want to use it again in a later premise). Instead, we are explicitly writing Integer which indicates that this rule only defines the operational semantics for Add(e1, e2) in state  when the expression e1 evaluates to a value of type Integer in state \n.\nAs for the second premise\n\\begin{equation*} \u0026lt;e_2,\\sigma \\prime\u0026gt;(v_2,Integer,\\sigma\\prime \\prime) \\end{equation*}\nwe see something very similar. Again, our premise prescribes that, when evaluated in state ' (note the ' there), e2\u0026rsquo;s type is an Integer. It is for this reason that we can be satisfied that this rule only applies when the types of the Add\u0026rsquo;s operands match and are integers! We \u0026ldquo;thread through\u0026rdquo; the (possibly) modified ' when evaluating e2 to enforce the STIMPL language\u0026rsquo;s definition that operands are evaluated strictly left-to-right.\nAs for the conclusion,\n\\begin{equation*} (v_1+v_2,Integer,\\sigma \\prime \\prime) \\end{equation*}\nshows the value of this expression. We will assume here that + works as expected for two integers. Because the operands are integers, we can definitively write that the type of the addition will be an integer, too. We use '' as the resulting state because it\u0026rsquo;s possible that evaluation of the expressions of both e1 and e2 caused side effects.\nThe rule that we defined covers only the operational semantics for addition of two integers. The other cases (for floating-point and string types) are almost copy/paste.\nNow, how does that translate to an actual implementation?\ndef evaluate(expression, state): match expression: ... case Add(left=left, right=right): result = 0 left_result, left_type, new_state = evaluate(left, state) right_result, right_type, new_state = evaluate(right, new_state) if left_type != right_type: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Mismatched types for Add: Cannot add {left_type}to {right_type}\u0026#34;\u0026#34;\u0026#34;) match left_type: case Integer() | String() | FloatingPoint(): result = left_result + right_result case _: raise InterpTypeError(f\u0026#34;\u0026#34;\u0026#34;Cannot add {left_type}s\u0026#34;\u0026#34;\u0026#34;) return (result, left_type, new_state) In this snippet, the local variables left and right are the equivalent of e1 and e2, respectively, in the operational semantics. After initializing a variable to store the result, the evaluation of the premises is accomplished. new_state matches '' after being assigned and reassigned in those two evaluations. Next, the code checks to make sure that the types of the operands matches. Finally, if the types of the operands is an integer, then the result is just a traditional addition (+ in Python).\nYou can see the implementation for the other types mixed in this code as well. Convince yourself that the code above handles all the different cases where an Add is valid in STIMPL.\nDefining the Semantics of the If/Then/Else Expression In STIMPL, we write an If/Then/Else expression like If(c, t, f) where c is any boolean-typed expression, t is the expression to evaluate if the value of c is true and f is the expression to evaluate if the value of c is false. The value/type/updated state of the entire expression is the value/type/updated state that results from evaluating t when c is true and the value/type/updated state that results from evaluating f when c is false. This means that we are required to write two different rules to completely define the operational semantics of the If/Then/Else expression: one for the case where c is true and the other for the case when c is false. Sounds like the template that we used for the Add expression, doesn\u0026rsquo;t it? Because the two cases are almost the same, we will only go through writing the rule for when the condition is true:\n\\begin{equation*} \\frac{\u0026lt;c,\\sigma\u0026gt;\\longrightarrow(True,Boolean,\\sigma \\prime),\u0026lt;t,\\sigma \\prime\u0026gt;\\longrightarrow(v,\\tau,\\sigma\\prime \\prime)}{\u0026lt;If(c,t,f),\u0026gt;(v,\\tau, \\sigma \\prime \\prime)} \\end{equation*}\nAs in the premises for the operational semantics of the Add operator, the first premise in the operational semantics above uses literals to guarantee that the rule only applies in certain cases:\n\\begin{equation*} \u0026lt;c,\\sigma \\prime\u0026gt;\\longrightarrow(True,Boolean,\\sigma\\prime \\prime) \\end{equation*}\nmeans that the rule only applies when c, evaluated in state , has a value of True and a boolean type. We use the second premise\n\\begin{equation*} \u0026lt;t,\\sigma\\prime\u0026gt;(v,\\tau,\\sigma \\prime \\prime) \\end{equation*}\nto \u0026ldquo;get\u0026rdquo; some values that we will use in the conclusion. v and  are the value and the type, respectively, of t when it is evaluated in state '. Note that we evaluate t in state ' because the evaluation of the condition statement may have modified state  and we want to thread that through. Evaluation of t in state ' may modify ', generating ''. The combination of these premises are combined to define that the entire expression evaluates to\n\\begin{equation*} (v,\\tau,\\sigma\\prime \\prime) \\end{equation*}\nAgain, the pattern is the same for writing the operational semantics when the condition is false.\nLet\u0026rsquo;s look at how this translates into actual working code:\ndef evaluate(expression, state): match expression: ... case If(condition=condition, true=true, false=false): condition_value, condition_type, new_state = evaluate(condition, state) if not isinstance(condition_type, Boolean): raise InterpTypeError(\u0026#34;Cannot branch on non-boolean value!\u0026#34;) result_value = None result_type = None if condition_value: result_value, result_type, new_state = evaluate(true, new_state) else: result_value, result_type, new_state = evaluate(false, new_state) return (result_value, result_type, new_state) The local variables condition, true and false match c, t and f, respectively from the rule in the operational semantics. The first step in the implementation is to determine the value/type/updated state when c is evaluated in state . Immediately after doing that, the code checks to make sure that the condition statement has boolean type. Remember how our rule only applies when this is the case? Next, depending on whether the condition evaluated to true or false, the appropriate next expression is evaluated in the ' state (new_state). It is the result of that evaluation that is the ultimate value of the expression and what is returned.\n9/24/2021 As we conclude the penultimate week of September, we are turning the page from imperative programming and beginning our work on object-oriented programming!\nThe Definitions of Object-Oriented Programming We started off by attempting to describe object-oriented programming using two different definitions:\n A language with support for abstraction of abstract data types (ADTs). (from Sebesta) A language with support for objects, containers of data (attributes, properties, fields, etc.) and code (methods). (from Wikipedia (Links to an external site.))  As graduates of CS1021C and CS1080C, the second definition is probably not surprising. The first definition, however, leaves something to be desired. Using Definition (1) means that we have to a) know the definition of abstraction and abstract data types and b) know what it means to apply abstraction to ADTs.\nAbstraction (Reprise) There are two fundamental types of abstractions in programming: process and data. We have talked about the former but the latter is new. When we talked previously about process abstractions, we did attempt to define the term abstraction but it was not satisfying.\nSebesta formally defines abstraction as the view or representation of an entity that includes only the most significant attributes. This definition seems to align with our notion of abstraction especially the way we use the term in phrases like \u0026ldquo;abstract away the details.\u0026rdquo; It didn\u0026rsquo;t feel like a good definition to me until I thought of it this way:\nConsider that you and I are both humans. As humans, we are both carbon based and have to breath to survive. But, we may not have the same color hair. I can say that I have red hair and you have blue hair to point out the significant attributes that distinguish us. I need not say that we are both carbon based and have to breath to survive because we are both human and we have abstracted those facts into our common humanity.\nWe returned to this point at the end of class when we described how inheritance is the mechanism of object-oriented programming that provides abstraction over ADTs. Abstract Data Types (ADTs)\nNext, we talked about the second form of abstraction available to programmers: data abstraction. As functions, procedures and methods are the syntactic and semantic means of abstracting processes in programming languages, ADTs are the syntactic and semantic means of abstracting data in programming languages. ADTs combine (encapsulate) data (usually called the ADT\u0026rsquo;s attributes, properties, etc) and operations that operate on that data (usually called the ADT\u0026rsquo;s methods) into a single entity.\nWe discussed that hiding is a significant advantage of ADTs. ADTs hide the data being represented and allow that data\u0026rsquo;s manipulation only through pre-defined methods, the ADT\u0026rsquo;s interface. The interface typically gives the ADT\u0026rsquo;s user the ability to manipulate/access the data internal to the type and perform other semantically meaningful operations (e.g., sorting a list).\nWe brainstormed some common ADTs:\n Stack Queue List Array Dictionary Graph Tree  These are are so-called user-defined ADTs because they are defined by the user of a programming language and composed of primitive data types.\nNext, we tackled the question of whether primitives are a type of ADT. A primitive type like floating point numbers would seem to meet the definition of an abstract data type:\n It\u0026rsquo;s underlying representation is hidden from the user (the programmer does not care whether FPs are represented according to IEEE754 or some other specification) There are operations that manipulate the data (addition, subtraction, multiplation, division).  The Requirements of an Object-Oriented Programming Language ADTs are just one of the three requirements that your textbook\u0026rsquo;s author believes are required for a language to be considered object oriented. Sebesta believes that, in addition to ADTs, an object-oriented programming language requires support for inheritance and dynamic method binding.\nInheritance It is inheritance where OOPs provide abstraction for ADTs. Inheritance allows programmers to abstract ADTs into common classes that share common characteristics. Consider three ADTs that we identified: trees, linked lists and graphs. These three ADTs all have nodes (of some kind or another) which means that we could abstract them into a common class: node-based things. A graph would inherit from the node-based things ADT so that its implementer could concentrate on what makes it distinct \u0026ndash; its edges, etc.\nDon\u0026rsquo;t worry if that is too theoretical. It does not negate the fact that, through inheritance, we are able to implement hierarchies that can be \u0026ldquo;read\u0026rdquo; using \u0026ldquo;is a\u0026rdquo; the way that inheritance is usually defined. With inheritance, cats inherit from mammals and \u0026ldquo;a cat is a mammal\u0026rdquo;.\nSubclasses inherit from ancestor classes. In Java, ancestor classes are called superclasses and subclasses are called, well, subclasses. In C++, ancestor classes are called base classes and subclasses are called derived classes. Subclasses inherit both data and methods.\nDynamic Method Binding In an OOP, a variable that is typed as Class A can be assigned anything that is actually a Class A or subclass thereof. We have not officially covered this yet, but in OOP a subclass can redefine a method defined in its ancestor.\nAssume that every mammal can make a noise. That means that every dog can make a noise just like every cat can make a noise. Those noises do not need to be the same, though. So, a cat \u0026ldquo;overrides\u0026rdquo; the mammal\u0026rsquo;s default noise and implements their own (meow). A dog does likewise (bark). A programmer can define a variable that holds a mammal and that variable can contain either a dog or a cat. When the programmer invokes the method that causes the mammal to make noise, then the appropriate method must be called depending on the actual type in the variable at the time. If the mammal held a dog, it would bark. If the mammal held a cat, it would meow.\nThis resolution of methods at runtime is known as dynamic method binding.\nOOP Example with Inheritance and Dynamic Method Binding abstract class Mammal { protected int legs = 0; Mammal() { legs = 0; } abstract void makeNoise(); } class Dog extends Mammal { Dog() { super(); legs = 4; } void makeNoise() { System.out.println(\u0026#34;bark\u0026#34;); } } class Cat extends Mammal { Cat() { super(); legs = 4; } void makeNoise() { System.out.println(\u0026#34;meow\u0026#34;); } } public class MammalDemo { static void makeARuckus(Mammal m) { m.makeNoise(); } public static void main(String args[]) { Dog fido = new Dog(); Cat checkers = new Cat(); makeARuckus(fido); makeARuckus(checkers); } } This code creates a hierarchy with Mammal at the top as the superclass of both the Dog and the Cat. In other words, Dog and Cat inherit from Mammal. The abstract keyword before class Mammal indicates that Mammal is a class that cannot be directly instantiated. We will come back to that later. The Mammal class declares that there is a method that each of its subclasses must implement \u0026ndash; the makeNoise function. If a subclass of Mammal fails to implement that function, it will not compile. The good news is that Cat and Dog do both implement that function and define behavior in accordance with their personality!\nThe function makeARuckus has a parameter whose type is a Mammal. As we said above, in OOP that means that I can assign to that variable a Mammal or anything that inherits from Mammal. When we call makeARuckus with an argument whose type is Dog, the function relies of dynamic method binding to make sure that the proper makeNoise function is called \u0026ndash; the one that barks \u0026ndash; even though makeARuckus does not know whether m is a generic Mammal, a Dog or a Cat. It is because of dynamic method binding that the code above generates\nbark meow as output.\n9/27/2021 It\u0026rsquo;s the last week of September but the first full week of OOP. Let\u0026rsquo;s do this!\nOverriding in OOP Recall the concept of inheritance that we discussed in the last class. Besides its utility as a formalism that describes the way a language supports abstraction of ADTs (and, therefore, makes it a plausibly OO language), inheritance provides a practical benefit in software engineering. Namely, it allows developers to build hierarchies of types.\nHierarchies are composed of pairs of classes \u0026ndash; one is the superclass and the other is the subclass. A superclass could conceivably be itself a subclass. A subclass could itself be a superclass. In terms of a family tree, we could say that the subclass is a descendant of the superclass (Note: remember that the terms superclass and subclass are not always the ones used by the languages themselves; C++ refers to them as base and derived classes, respectively).\nA subclass inherits both the data and methods from its superclass(es). However, as Sebesta says, \u0026ldquo;\u0026hellip; the features and capabilities of the [superclass] are not quite right for the new use.\u0026rdquo; Overriding methods allows the programmer to keep most of the functionality of the baseclass and customize the parts that are \u0026ldquo;not quite right.\u0026rdquo;\nAn overridden method is defined in a subclass and replaces the method with the same name (and usually protocol) in the parent.\nThe official documentation and tutorials for Java describe overriding in the language this way:\u0026ldquo;An instance method in a subclass with the same signature (name, plus the number and the type of its parameters) and return type as an instance method in the superclass overrides the superclass\u0026rsquo;s method.\u0026quot; The exact rules for overriding methods in Java are online at the language specification .\nLet\u0026rsquo;s make it concrete with an example:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } In this example, Car is the superclass of Tesla and Chevrolet. The Car class defines a method named ignite. That method will ignite the engine of the car \u0026ndash; an action whose mechanics differ based on the car\u0026rsquo;s type. In other words, this is a perfect candidate for overriding. Both Tesla and Chevrolet implement a method with the same name, return value and parameters, thereby meeting Java\u0026rsquo;s requirements for overriding. In Java, the @Override is known as an annotation. Annotations are \u0026ldquo;a form of metadata [that] provide data about a program that is not part of the program itself.\u0026quot; Annotations in Java are attached to particular syntactic units. In this case, the @Override annotation is attached to a method and it tells the compiler that the method is overriding a method from its superclass. If the compiler does not find a method in the superclass(es) that is capable of being overridden by the method, an error is generated. This is a good check for the programmer. (Note: C++ offers similar functionality through the override specifier (Links to an external site.).)\nLet\u0026rsquo;s say that the programmer actually implemented the Tesla class like this:\nclass Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite(int testing) { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } The ignite method implemented in Tesla does not override the ignite method from Car because it has a different set of parameters. The @Override annotation tells the compiler that the programmer thought they were overriding something. An error is generated and the programmer can make the appropriate fix. Without the @Override annotation, the code will compile but produce incorrect output when executed.\nAssume that the following program exists:\npublic class CarDemo { public static void main(String args[]) { Car c = new Car(); Car t = new Tesla(); Car v = new Chevrolet(); c.ignite(); t.ignite(); v.ignite(); } } This code instantiates three different cars \u0026ndash; the first is a generic Car, the second is a Tesla and the third is a Chevrolet. Look carefully and note that the type of each of the three is actually stored in a variable whose type is Car and not a more-specific type (ie, Tesla or Chevy). This is not a problem because of dynamic dispatch. At runtime, the JVM will find the proper ignite function and invoke it according to the variable\u0026rsquo;s actual type and not its static type. Because ignite is overridden by Chevy and Tesla, the output of the program above is:\nIgniting a generic car\u0026#39;s engine! Igniting a Tesla\u0026#39;s engine! Igniting a Chevrolet\u0026#39;s engine! Most OOP languages provide the programmer the option to invoke the method they are overriding from the superclass. Java is no different. If an overriding method implementation wants to invoke the functionality of the method that it is overriding, it can do so using the super keyword.\nclass Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } With these changes, the program now outputs:\nIgniting a generic car\u0026#39;s engine! Igniting a generic car\u0026#39;s engine! Igniting a Tesla\u0026#39;s engine! Igniting a generic car\u0026#39;s engine! Igniting a Chevrolet\u0026#39;s engine! New material alert: What if the programmer does not want a subclass to be able to customize the behavior of a certain method? For example, no matter how you subclass Dog, it\u0026rsquo;s noise method is always going to bark \u0026ndash; no inheriting class should change that. Java provides the final keyword to guarantee that the implementation of a method cannot be overridden by a subclass. Let\u0026rsquo;s change the code for the classes from above to look like this:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } void start() { System.out.println(\u0026#34;Starting a car ...\u0026#34;); if (this.ignite()) { System.out.println(\u0026#34;Ignited the engine!\u0026#34;); } else { System.out.println(\u0026#34;Did NOT ignite the engine!\u0026#34;); } } final boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { super.ignite(); System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } Notice that ignite in the Car class has a final before the return type. This makes ignite a final method : \u0026ldquo;A method can be declared final to prevent subclasses from overriding or hiding it\u0026rdquo;. (C++ has something similar \u0026ndash; the final specifier .) Attempting to compile the code above produces this output:\nCarDemo.java:30: error: ignite() in Tesla cannot override ignite() in Car boolean ignite() { ^ overridden method is final CarDemo.java:43: error: ignite() in Chevrolet cannot override ignite() in Car boolean ignite() { ^ overridden method is final 2 errors Subclass vs Subtype In OOP there is fascinating distinction between subclasses and subtypes. All those classes that inherit from other classes are considered subclasses. However, they are not all subtypes. For a type/class S to be a subtype of type/class T, the following must hold\nAssume that (t) is some provable property that is true of t, an object of type T. Then (s)\nmust be true as well for s, an object of type S.\nThis formal definition can be phrased simply in terms of behaviors: If it is possible to pass objects of type T as arguments to a function that expects objects of type S without any change in the behavior, then S is a subtype of T. In other words, a subtype behaves exactly like the \u0026ldquo;supertype\u0026rdquo;.\nBarbara Liskov who pioneered the definition and study of subtypes put it this way (Links to an external site.): \u0026ldquo;If for each object o1 of type S there is an object o2 of type T such that for all programs P defined in terms of T, the behavior of P is unchanged when o1 is substituted for o2, then S is a subtype of T.\u0026rdquo;\nOpen Recursion Open recursion in an OO PL is a fancy term for the combination of a) functionality that gives the programmer the ability to refer to the current object from within a method (usually through a variable named this or self) and b) dynamic dispatch. . Thanks to open recursion, some method A of class C can call some method B of the same class. But wait, there\u0026rsquo;s more! (Links to an external site.) Continuing our example, in open recursion, if method B is overriden in class D (a subclass of C), then the overriden version of the method is invoked when called from method A on an object of type D even though method A is only implemented by class C. Wild! It is far easier to see this work in real life than talk about it abstractly. So, consider our cars again:\nclass Car { protected boolean electric = false; protected int wheels = 4; Car() { } void start() { System.out.println(\u0026#34;Starting a car ...\u0026#34;); if (this.ignite()) { System.out.println(\u0026#34;Ignited the engine!\u0026#34;); } else { System.out.println(\u0026#34;Did NOT ignite the engine!\u0026#34;); } } boolean ignite() { System.out.println(\u0026#34;Igniting a generic car\u0026#39;s engine!\u0026#34;); return true; } } class Tesla extends Car { Tesla() { super(); electric = true; } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Tesla\u0026#39;s engine!\u0026#34;); return true; } } class Chevrolet extends Car { Chevrolet() { super(); } @Override boolean ignite() { System.out.println(\u0026#34;Igniting a Chevrolet\u0026#39;s engine!\u0026#34;); return false; } } The start method is only implemented in the Car class. At the time that it is compiled, the Car class has no awareness of any subclasses (ie, Tesla and Chevrolet). Let\u0026rsquo;s run this code and see what happens:\npublic class CarDemo { public static void main(String args[]) { Car c = new Car(); Car t = new Tesla(); Car v = new Chevrolet(); c.start(); t.start(); v.start(); } } Here\u0026rsquo;s the output:\nStarting a car ... Igniting a generic car\u0026#39;s engine! Ignited the engine! Starting a car ... Igniting a Tesla\u0026#39;s engine! Ignited the engine! Starting a car ... Igniting a Chevrolet\u0026#39;s engine! Did NOT ignite the engine! Wow! Even though the implementation of start is entirely within the Car class and the Car class knows nothing about the Tesla or Chevrolet subclasses, when the start method is invoked on object\u0026rsquo;s of those types, the call to this\u0026rsquo;s ignite method triggers the execution of code specific to the type of car!\nHow cool is that?\n10/1/2021 Original is here.\nWe made it into October!! Spooky, spooky!\nCorrections Like in real newspapers (Links to an external site.), we are going to start including Corrections in each edition! We want to make sure that our reporters adhere to the highest standards:\nThe JVM will insert an implicit call to the to-be-instantiated class' default constructor (i.e., the one with no parameters) if the the to-be-constructed (sub)class does not do so explicitly. We\u0026rsquo;ll make this clear with an example:\nclass Parent { Parent() { System.out.println(\u0026#34;I am in the Parent constructor.\u0026#34;); } Parent(int parameter) { System.out.println(\u0026#34;This version of the constructor is not called.\u0026#34;); } } class Child extends Parent { Child() { /* * No explicit call to super -- one is automatically * injected to the parent constructor with no parameters. */ System.out.println(\u0026#34;I am in the Child constructor.\u0026#34;); } } public class DefaultConstructor { public static void main(String args[]) { Child c = new Child(); } } When this program is executed, it will print\nI am in the Parent constructor. I am in the Child constructor. The main function is instantiating an object of the type Child. We can visually inspect that there is no explicit call the super() from within the Child class' constructor. Therefore, the JVM will insert an implicit call to super() which actually invokes Parent().\nHowever, if we make the following change:\nclass Parent { Parent() { System.out.println(\u0026#34;I am in the Parent constructor.\u0026#34;); } Parent(int parameter) { System.out.println(\u0026#34;This version of the constructor is not called.\u0026#34;); } } class Child extends Parent { Child() { /* * No explicit call to super -- one is automatically * injected to the parent constructor with no parameters. */ super(1); System.out.println(\u0026#34;I am in the Child constructor.\u0026#34;); } } public class DefaultConstructor { public static void main(String args[]) { Child c = new Child(); } } Something different happens. We see that there is a call to Child\u0026rsquo;s superclass' constructor (the one that takes a single int-typed parameter). That means that the JVM will not insert an implicit call to super() and we will get the following output:\nThis version of the constructor is not called. I am in the Child constructor.\nThe C++ standard sanctions a main function without a return statement. The standard says: \u0026ldquo;if control reaches the end of main without encountering a return statement, the effect is that of executing return 0;.\u0026rdquo;\nA Different Way to OOP So far we have talked about OOP in the context of Java. Java, and languages like it, are called Class-based OOP languages. In a Class-based OOP, classes and objects exist in different worlds. Classes are used to define/declare\n the attributes and methods of an encapsulation, and the relationships between them.  From these classes, objects are instantiated that contain those attributes and methods and respect the defined/declared hierarchy. We can see this in the example given above: The classes Parent and Child define (no) attributes and (no) methods and define the relationship between them. In main(), a Child is instantiated and stored in the variable c. c is an object of type Child that contains all the data associated with a Child and a Parent and can perform all the actions of a Child and a Parent.\nNothing about Class-based OOP should be different than what you\u0026rsquo;ve learned in the past as you\u0026rsquo;ve worked with C++. There are several problems with Class-based OOP.\n The supported attributes and method of each class must be determined before the application is developed (once the code is compiled and the system is running, an object cannot add, remove or modify its own methods or attributes); The inheritance hierarchy between classes must be determined before the application is developed (once the code is compiled, changing the relationship between classes will require that the application be recompiled!).  In other words, Class-based OOP does not allow the structure of the Classes (nor their relationships) to easily evolve with the implementation of a system.\nThere is another way, though. It\u0026rsquo;s called Prototypal OOP. The most commonly known languages that use Prototypal OOP are JavaScript and Ruby! In Prototypal (which is a very hard word to spell!) OOP there is no distinction between Class and object \u0026ndash; everything is an object! In a Prototypal OOP there is a base object that has no methods or data attributes and every object is able to modify itself (its attributes and methods). To build a new object, the programmer simply copies from an existing object, the new object\u0026rsquo;s so-called prototype, and customizes the copied object appropriately.\nFor example, assume that there is an object called Car that has one attribute (the number of wheels) and one method (start). That object can serve as the prototype car. To \u0026ldquo;instantiate\u0026rdquo; a new Car, the programmer simply copies the existing prototypical car object Car and gives it a name, say, c. The programmer can change the value of c\u0026rsquo;s number of wheels and invoke its method, start. Let\u0026rsquo;s say that the same programmer wants to create something akin to a subclass of Car. The programmer would create a new, completely fresh object (one that has no methods or attributes), name it, say, Tesla, and link the new prototype Tesla object to the existing prototype car Car object through the prototype Tesla object\u0026rsquo;s prototype link (the sequence of links that connects prototype objects to one another is called a prototype chain). If a Tesla has attributes (range, etc) or methods (self_drive) that the prototype car does not, then the programmer would install those methods on the prototype Tesla Tesla. Finally, the programmer would \u0026ldquo;declare\u0026rdquo; that the Tesla object is a prototype Tesla.\n The blue arrows in the diagram above are prototype links. The orange lines indicate where a copy is made.\nHow does inheritance work in such a model? Well, it\u0026rsquo;s actually pretty straightforward: When a method is invoked or an attribute is read/assigned, the runtime will search the prototype chain for the first prototypical object that has such a method or attribute. Mic drop. In the diagram above, let\u0026rsquo;s follow how this would play out when the programmer calls start() on the Model 3 Instance. The Model 3 Instance does not contain a method named start. So, up we go! The Tesla Prototype Object does not contain that me either. All the way up! The Car Prototype Object, does, however, so that method is executed!\nWhat would it look like to override a function? Again, relatively straightforward. If a Tesla performs different behavior than a normal Car when it starts, the programmer creating the Tesla Prototype Object would just add a method to that object with the name start. Then, when the prototype chain is traversed by the runtime looking for the method, it will stop at the start method defined in the Tesla Prototype Object instead of continuing on to the start method in the Car Prototype Object. (The same is true of attributes!)\nThere is (at least) one really powerful feature of this model. Keep in mind that the prototype objects are real things that can be manipulated at runtime (unlike classes which do not really exist after compilation) and prototype objects are linked together to achieve a type of inheritance. With reference to the diagram above, say the programmer changes the definition of the start method on the Car Prototype Object. With only that change, any object whose prototype chain includes the Car Prototype Object will immediately have that new functionality (where it is not otherwise overridden, obviously) \u0026ndash; all without stopping the system!! How cool is that?\nHow scary is that? Can you imagine working on a system where certain methods you \u0026ldquo;inherit\u0026rdquo; change at runtime?\n OOP or Interfaces? Newer languages (e.g., Go, Rust, (new versions of) Java) are experimenting with new features that support one of the \u0026ldquo;killer apps\u0026rdquo; of OOP: The ability to define a function that takes a parameter of type A but that works just the same as long as it is called with an argument whose type is a subtype of A. The function doesn\u0026rsquo;t have care whether it is called with an argument whose type is A or some subtype of A because the language\u0026rsquo;s OOP semantics guarantee that anything the programmer can do with an object of type A, the programmer can do with and object of subtype of A.\nUnfortunately, using OOP to accomplish such a feat may be like killing a fly with a bazooka (or a laptop, like Alex killed that wasp today).\nInstead, modern languages are using a slimmer mechanism known as an interface or a trait. An interface just defines a list of methods that an implementer of that interface must support. Let\u0026rsquo;s see some real Go code that does this \u0026ndash; it\u0026rsquo;ll clear things up:\ntype Readable interface { Read() } This snippet defines an interface with one function (Read) that takes no parameters and returns no value. That interface is named Readable. Simple.\ntype Book struct { title string } This snippet defines a data structure called a Book \u0026ndash; such structs are the closest that Go has to classes.\nfunc (book Book) Read() { fmt.Printf(\u0026#34;Reading the book %v\\n\u0026#34;, book.title) } This snippet simply says that if variable b is of type Book then the programmer can call b.Read(). Now, for the payoff:\nfunc WhatAreYouReading(r Readable) { r.Read() } This function only accepts arguments that implement (i.e., meet the criteria specified in the definition of) the Readable interface. In other words, with this definition, the code in the body of the function can safely assume that it can can call Read on r. And, for the encore:\nbook := Book{title: \u0026#34;Infinite Jest\u0026#34;} WhatAreYouReading(book) This code works exactly like you\u0026rsquo;d expect. book is a valid argument to WhatAreYouReading because it implements the Read method which, implicitly, means that it implements the Readable interface. But, what\u0026rsquo;s really cool is that the programmer never had to say explicitly that Book implements the Readable interface! The compiler checks automatically. This gives the programmer the ability to generate a list of only the methods absolutely necessary for its parameters to implement to achieve the necessary ends \u0026ndash; and nothing unnecessary. Further, it decouples the person implementing a function from the person using the function \u0026ndash; those two parties do not have to coordinate requirements beforehand. Finally, this functionality means that a structure can implement as few or as many interfaces as its designer wants.\nDip Our Toe Into the Pool of Pointers We only had a few minutes to start pointers, but we did make some headway. There will be more on this in the next lecture!\nIt is important to remember that pointers are like any other type \u0026ndash; they have a range of valid values and a set of valid operations that you can perform on those values. What are the range of valid values for a pointer? All valid memory addresses. And what are the valid operations? Addition, subtraction, dereference and assignment.\n In the diagram, the gray area is the memory of the computer. The blue box is a pointer. It points to the gold area of memory. It is important to remember that pointers and their targets both exist in memory! In fact, in true Inception (Links to an external site.)style, a pointer can pointer to a pointer!\nAt the same time that pointers are types, they also have types. The type of a pointer includes the type of the target object. In other words, if the memory in the gold box held an object of type T, the the green box\u0026rsquo;s type would be \u0026ldquo;pointer to type T.\u0026rdquo; If the programmer dereferences the blue pointer, they will get access to the object in memory in the gold.\nIn an ideal scenario, it would always be the case that the type of the pointer and the type of the object at the target of the pointer are the same. However, that\u0026rsquo;s not always the case. Come to the next lecture to see what can go wrong when that simple fact fails to hold!\n10/4/2021 Original is here One day closer to Candy Corn!\nCorrections When we were discussing the nature of the type of pointers, we specified that the range of valid values for a pointer are all memory addresses. In some languages this may be true. However, some other languages specify that the range of valid values for a pointer are all memory addresses and a special null value that explicitly specifies a pointer does not point to a target.\nWe also discussed the operations that you can perform on a pointer-type variable. What we omitted was a discussion of an operation that will fetch the address of a variable in memory. For languages that use pointers to support indirect addressing (see below), such an operation is required. In C/C++, this operation is performed using the address of (\u0026amp;) operator.\nPointers We continued the discussion of pointers that we started on Friday! On Friday we discussed that pointers are just like any other type \u0026ndash; they have valid values and defined operations that the programmer can perform on those values.\nThe Pros of Pointers Though a very famous and influential computer scientist (Links to an external site.) once called his invention of null references a \u0026ldquo;billion dollar mistake\u0026rdquo; (he low balled it, I think!), the presence and power of pointers in a language is important for at least two reasons:\n Without pointers, the programmer could not utilize the power of indirection. Pointers give the programmer the power to address and manage heap-dynamic memory.  Indirection gives the programmer the power to link between different objects in memory \u0026ndash; something that makes writing certain data structures (like trees, graphs, linked lists, etc) easier. Management of heap-dynamic memory gives the programmer the ability to allocate, manipulate and deallocate memory at runtime. Without this power, the programmer would have to know before execution the amount of memory their program will require.\nThe Cons of Pointers Their use as a means of indirection and managing heap-dynamic memory are powerful, but misusing either can cause serious problems.\n   Possible Problems when Using Pointers for Indirection\nAs we said in the last lecture, as long as a pointer targets memory that contains the expected type of object, everything is a-okay. Problems arise, however, when the target of the pointer is an area in memory that does not contain an object of the expected type (including garbage) and/or the pointer targets an area of memory that is inaccessible to the program.\nThe former problem can arise when code in a program writes to areas of memory beyond their control (this behavior is usually an error, but is very common). It can also arise because of a use after free. As the name implies, a use-after-free error occurs when a program uses memory after it has been freed. There are two common scenarios that give rise to a use after free:\n Scenario 1:  One part of a program (part A) frees an area of memory that held a variable of type T that it no longer needs Another part of the program (part B) has a pointer to that very memory A third part of the program (part C) overwrites that \u0026ldquo;freed\u0026rdquo; area of memory with a variable of type S Part B accesses the memory assuming that it still holds a variable of Type T   Scenario 2:  One part of a program (part A) frees an area of memory that held a variable of type T that it no longer needs Part A never nullifies the pointer it used to point to that area of memory though the pointer is now invalid because the program has released the space A second part of the program (part C) overwrites that \u0026ldquo;freed\u0026rdquo; area of memory with a variable of type S Part A incorrectly accesses the memory using the invalid pointer assuming that it still holds a variable of Type T    Scenario 2 is depicted visually in the following scenario and intimates why use-after-free errors are considered security vulnerabilities:\n In the example shown visually above, the program\u0026rsquo;s use of the invalid pointer means that the user of the invalid pointer can now access an object that is at a higher privilege level (Restricted vs Regular) than the programmer intended. When the programmer calls a function through the invalid pointer they expect that a method on the Regular object will be called. Unfortunately, a method on the Restricted object will be called instead. Trouble!\nThe latter problem occurs when a pointer targets memory beyond the program\u0026rsquo;s control. This most often occurs when the program sets a variable\u0026rsquo;s address to 0 (NULL, null, nil) to indicate that it is invalid but later uses that pointer without checking its validity. For compiled languages this often results in the dreaded segmentation fault and for interpreted languages it often results in other anomalous behavior (like Java\u0026rsquo;s Null Pointer Exception (NPE)). Neither are good!\n     Possible Solutions\nWouldn\u0026rsquo;t it be nice if we had a way to make sure that the pointer being dereferenced is valid so we fall victim to some of the aforementioned problems? What would be the requirements of such a solution?\n Pointers to areas of memory that have been deallocated cannot be dereferenced. The type of the object at the target of a pointer always matches the programmer\u0026rsquo;s expectation.  Your author describes two potential ways of doing this. First, are tombstones. Tombstones are essentially an intermediary between a pointer and its target. When the programming language implements pointers and uses tombstones for protection, a new tombstone is allocated for each pointer the programmer generates. The programmer\u0026rsquo;s pointer targets the tombstone and the tombstone targets the pointer\u0026rsquo;s actual target. The tombstone also contains an extra bit of information: whether it is valid. When the programmer first instantiates a pointer to some target a the compiler/interpreter\n generates a tombstone whose target is a sets the valid bit of the tombstone to valid points the programmer\u0026rsquo;s pointer to the tombstone.  When the programmer dereferences their pointer, the compiler/runtime will check to make sure that the target tombstone\u0026rsquo;s valid flag is set to valid before doing the actual dereference of the ultimate target. When the programmer \u0026ldquo;destroys\u0026rdquo; the pointer (by releasing the memory at its target or by some other means), the compiler/runtime will set the target tombstone\u0026rsquo;s valid flag to invalid. As a result, if the programmer later attempts to dereference the pointer after it was destroyed, the compiler/runtime will see that the tombstone\u0026rsquo;s valid flag is invalid and generate an appropriate error.\nThis process is depicted visually in the following diagram.\nTombstones.png\nThis seems like a great solution! Unfortunately, there are downsides. In order for the tombstone to provide protection for the entirety of the program\u0026rsquo;s execution, once a tombstone has been allocated it cannot be reclaimed. It must remain in place forever because it is always possible that the programmer can incorrectly reuse an invalid pointer. As soon as the tombstone is deallocated, the protection that it provides is gone. The other problem is that the use of tombstones adds an additional layer of indirection to dereference a pointer and every indirection causes memory accesses. Though memory access times are small, they are not zero \u0026ndash; the cost of these additional memory accesses add up.\nWhat about a solution that does not require an additional level of indirection? There is a so-called lock-and-key technique. This protection method requires that the pointer hold an additional piece of information beyond the address of the target: the key. The memory at the target of the pointer is also required to hold a key. When the system allocates memory it sets the keys of the pointer and the target to be the same value. When the programmer dereferences a pointer, the two keys are compared and the operation is only allowed to continue if the keys are the same. The process is depicted visually below.\n With this technique, there is no additional memory access \u0026ndash; that\u0026rsquo;s good! However, there are still downsides. First, there is a speed cost. For every dereference there must be a check of the equality of the keys. Depending on the length of the key that can take a significant amount of time. Second, there is a space cost. Every pointer and block of allocated memory now must have enough space to store the key. For systems where memory allocations are done in big chunks, the relative size overhead of storing, say, and 8byte key is not significant. However, if the system allocates many small areas of memory, the relative size overhead is tremendous. Moreover, the more heavily the system relies on pointers the more space will be used to store keys rather than meaningful data.\nWell, let\u0026rsquo;s just make the keys smaller? Great idea. There\u0026rsquo;s only one problem: The smaller the keys the fewer unique key values. Fewer unique key values mean that it is more likely an invalid pointer randomly points to a chunk of memory with a matching key. In this scenario, the protection afforded by the scheme is vitiated. (I just wanted to type that word \u0026ndash; I\u0026rsquo;m not even sure I am using it correctly!)\n  10/6/2021 Original is here.\nI love Reese\u0026rsquo;s Pieces.\nCorrections None to speak of!!\nPointers for Dynamic Memory Management We finished up our discussion of pointers in today\u0026rsquo;s class. In the previous class, we talked about how pointers have two important roles in programming languages:\n indirection \u0026ndash; referring to other objects dynamic memory management \u0026ndash; \u0026ldquo;handles\u0026rdquo; for areas of memory that are dynamically allocated and deallocated by the system.  On Monday we focused on the role of pointers in indirection and how to solve some of the problems that can arise from using pointers in that capacity. In today\u0026rsquo;s class, we focused on the role of pointers in dynamic memory management.\nAs tools for dynamic memory management, the programmer can use pointers to target blocks (N.B.: I am using blocks as a generic term for memory and am not using it in the sense of a block [a.k.a. page] as defined in the context of operating systems) of dynamic memory that are allocated and deallocated by the operating system for use by an application. The programmer can use these pointers to manipulate what is stored in those blocks and, ultimately, release them back to the operating system when they are no longer needed.\nMemory in the system is a finite resource. If a program repeatedly asks for memory from the system without releasing previous allocations back to the system, there will come a time when the memory is exhausted. In order to be able to release existing allocations back to the operating system for reuse by other applications, the programmer must not lose track of those existing allocations. When there is a memory allocation from the operating system to the application that can no longer be reached by a pointer in the application, that memory allocation is leaked. Because the application no longer has a pointer to it, there is no way for the application to release it back to the system. Leaked memory belongs to the leaking application until it terminates.\nFor some programs this is fine. Some applications run for a short, defined period of time. However, there are other programs (especially servers) that are written specifically to operate for extended periods of time. If such applications leak memory, they run the risk of exhausting the system\u0026rsquo;s memory resources and failing (Links to an external site.).\nPreventing Memory Leaks System behavior will be constrained when those systems are written in languages that do not support using pointers for dynamic memory management. However, what we learned (above) is that it is not always easy to use pointers for dynamic memory management correctly. What are some of the tools that programming languages provide to help the programmer manage pointers in their role as managers of dynamic memory.\n   Reference Counting\nIn a reference-counted memory management system, each allocated block of memory given to the application by the system contains a reference count. That reference count, well, counts the number of references to the object. In other words, for every pointer to an operating-system allocated block of memory, the reference count on that block increases. Every time that a pointer\u0026rsquo;s target is changed, the programming language updates the reference counts of the old target (decrement) and the new target (increment), if there is a new target (the pointer could be changed to null, in which case there is no new target). When a block\u0026rsquo;s reference count reaches zero, the language knows that the block is no longer needed, and automatically returns it to the system! Pretty cool.\n The scenario depicted visually shows the reference counting process. At time (a), the programmer allocates a block of memory dynamically from the operating system and puts an application object in that block. Assume that the application object is a node in a linked list. The first node is the head of the list. Because the programmer has a pointer that targets that allocation, the block\u0026rsquo;s reference count at time (a) is 1. At time (b), the programmer allocates a second block of memory dynamically from the system and puts a second application object in that block \u0026ndash; another node in the linked list (the tail of the list). Because the head of the list is referencing the tail of the list, the reference count of the tail is 1. At time (c) the programmer deletes their pointer (or reassigns it to a different target) to the head of the linked list. The programming language decrements the reference count of the block of memory holding the head node and deallocates it because the reference count has dropped to 0. Transitively, the pointer from the head application object to the tail application object is deleted and the programming language decrements the reference count of its target, the block of memory holding the tail application object (time (d)). The reference count of the block of memory holding the tail application object is now 0 and so the programming language automatically deallocates the associated storage (time (e)). Voila \u0026ndash; an automatic way to handle dynamic memory management.\nThere\u0026rsquo;s only one problem. What if the programmer wants to implement a circularly linked list?\n Because the tail node points to the head node, and the head node points to the tail node, even after the programmer\u0026rsquo;s pointer to the head node is deleted or retargeted, the reference counts of the two nodes will never drop to 0. In other words, even with reference-counted automatic memory management, there could still be a memory leak! Although there are algorithms to break these cycles, it\u0026rsquo;s important to remember that reference counting is not a panacea. Python is a language that manages memory using reference counting.\n  Garbage Collection Garbage collection (GC) is another method of automatically managing dynamically allocated memory. In a GC\u0026rsquo;d system, when a programmer allocates memory to store an object and no space is available, the programming language will stop the execution of the program (a so-called GC pause) to calculate which previously allocated memory blocks are no longer in use and can be returned to the system. Having freed up space as a result of cleaning up unused garbage, the allocation requested by the programmer can be satisfied and the execution of the program can continue.\nThe most efficient way to engineer a GC\u0026rsquo;d system is if the programming language allocates memory to the programmer in fixed-size cells. In this scenario, every allocation request from the programmer is satisfied by a block of memory from one of several banks of fixed-size blocks that are stacked back-to-back. For example, a programming language may manage three different banks \u0026ndash; one that holds reserves of X-sized blocks, one that holds reserves of Y-sized blocks and one that holds reserves of Z-sized blocks. When the programmer asks for memory to hold an object that is of size a, the programming language will deliver a block that is just big enough to that object. Because the size of the requested allocation may not be exactly the same size as one of the available fixed-size blocks, space may be wasted.\nThe fixed sizing of blocks in a GC\u0026rsquo;d system makes it easy/fast to walk through every block of memory. We will see shortly that the GC algorithm requires such an operation every time that it stops the program to do a cleanup. Without a consistent size, traversing the memory blocks would require that each block hold a tag indicating its size \u0026ndash; a waste of space and the cause of an additional memory read \u0026ndash; so that the algorithm could dynamically calculate the starting address of the next block.\nWhen the programmer requests an allocation that cannot be satisfied, the programming language stops the execution of the program and does a garbage collection. The classic GC algorithm is called mark and sweep and has three steps:\nEvery block of memory is marked as free using a free bit attached to the block. Of course, this is only true of some of the blocks, but the GC is optimistic! All pointers active at the time the program is paused are traced to their targets. The free bits of those blocks are reset. The blocks that are marked free and released.\nThe process is shown visually below:\n At times (a), (b) and (c), the programmer is allocating and manipulating references to dynamically allocated memory. At time (c), the allocation request for variable z cannot be satisfied because there are no available blocks. A GC pause starts at time (d) and the mark-and-sweep algorithm commences by setting the free bit of every block. At time (e) the pointers are traced and the appropriate free bits are cleared. At time (f) the memory is released from the unused block and its free bit, too, is reset. At time (g) the allocation for variable z can be satisfied, the GC pause completes and the programming language restarts execution of the program.\nThis process seems great, just like reference counting seemed great. However, there is a significant problem: The programmer cannot predict when GC pauses will occur and the programmer cannot predict how long those pauses will take. A GC pause is completely initiated by the programming language and (usually) completely beyond the control of the programmer. Such random pauses of program execution could be extremely harmful to a system that is controlling a system that needs to keep up with interactions from the outside world. For instance, it would be totally unacceptable for an autopilot system to take an extremely long GC pause as it calculates the heading needed to land a plane. There are myriad other systems where pauses are inappropriate.\nThe mark-and-sweep algorithm described above is extremely naive and GC algorithms are the subject of intense research. Languages like go and Java manage memory with a GC and their algorithms are incredibly sophisticated. If you want to know more, please let me know!\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/set-relations/",
	"title": "Set relations",
	"tags": [],
	"description": "",
	"content": "reflexive reflexive if, for every element \\(a \\in A\\) we have \\(aRa \\Rightarrow (a, a) \\in R\\)\n \\( A = \\{(a, a): a \\in A\\}\\)  Symmetric symmetric iff \\((x,y) \\in R \\wedge (y,x) \\in R\\)\nTransitive Iff R relates \\(a\\) to \\(b\\) and \\(b\\) to \\( c\\) then \\(a \\) relates to \\(c\\)\n \\(a \u0026lt; b \u0026lt; c \\rightarrow a \u0026lt; c\\) \\(a = b = c \\rightarrow a = c\\)  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/set-theory/",
	"title": "Standard Proof techniques",
	"tags": [],
	"description": "",
	"content": "Disproof by Counterexample Shows that a conjecture is not true by pointing out an example where the conjecture does not hold.\n No nickels 1 quarter + 5 pennies 3 dimes Greedy method is not appropriate with limited change  Proof by Contradiction Proof that the opposite cannot be true.\nSquare root of 2 is irrational  \\(\\sqrt 2 = a/b\\) \\(a/b\\) is simplified a or b or both must be odd (otherwise could be simplified) \\(2 = a^2/b^2\\) \\(a^2 = 2 * b^2\\) \\(a^2\\) must be even (2 times any number is even) \\(a\\) is even as well (odd times odd is odd) \\(a = 2 * k\\) where k is a / 2 \\(2 = (2 * k)^2/b^2 \\rightarrow b^2 = 2k^2\\) \\(b\\) is also odd by this method \\(a\\) and \\(b\\) cannot be odd \\(\\sqrt 2\\) cannot be rational  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/phys2001/test/",
	"title": "Test",
	"tags": [],
	"description": "",
	"content": "ah\n"
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/discrete-structures/trees/",
	"title": "Trees",
	"tags": [],
	"description": "",
	"content": " set of nodes first node is root every other node has a \u0026ldquo;parent\u0026rdquo; node  Two Trees  Every node that is not a leaf has 2 child nodes  Binary Trees  Every node has a maximum of 2 children  "
},
{
	"uri": "https://illustratedman-code.github.io/GuideToCS/intro-to-comp-systems/formulas-and-references/",
	"title": "Useful formulas and references",
	"tags": [],
	"description": "",
	"content": "Hard Drives Hard Drive Website Hard Drive Document from nitin\nFormulas  Disk access time = Seek Time + Rotational delay + Transfer time + Controller overhead + Queuing delay Average Disk Access Time = Average seek time + Average rotational delay + Transfer time + Controller overhead + Queuing delay Average Seek Time = 1/3 * Time taken for one full stroke  (Time taken to move from track 1 to track 1 + Time taken to move from track 1 to last track)/2 {0 + (k-1)t}/2 (k-1)t/2   Average Rotational Latency = 1/2 * Time taken for one full rotation Capacity of disk pack = Total number of surfaces * Number of tracks per surface * Number of sectors per track * storage capacity of one sector Formatting Overhead = Number of sectors * Overhead per sector Formatted Disk Space = Total disk space or capacity - formatting overhead Recording density or storage density = Capacity of track / circumference of the track Track Capacity = Recording density of the track * Circumference of the track Data Transfer Rate = Number of heads * Bytes that can be read in one full rotation * Number of rotations in one second = Number of heads * Capacity of one track * Number of rotations in one second Tracks per surface = (Outer radius - Inner radius) / Inter Track gap  Circuit reference  Logic Gates  Full Adder   Computer Arithmetic Computer arithemetic document from nitin\nMemory + Cache  Main Memory and Organization : credit to Robbie Schad Cache Notes Direct Mapping Examples Fully Associative Mapping Examples Set Associate Mapping Examples  Formulas Direct mapped cache  physical address size (bits) = TAG + Line Number + Block/Line Offset \\(\\text{MM(size in bytes)} = 2^{\\text{number Of Bits In Physical Address}} * 2^3\\) \\(\\text{BlockOffset(size in bytes)} = 2^{\\text{Bits In Block Offset}} * 2^3 \\) \\(\\text{number of lines} = 2^{\\text{bits in line number}} = \\frac{\\text{Cache size}}{\\text{Line Size}} \\) \\(\\text{tag directory size} = \\text{number of lines in cache * Number of bits in tag} = \\text{Number of Tags} * \\text{Tag size} \\)  Set Associative Mapped cache  physical address size (bits) = TAG + Set Number + Block/line Offset \\(2^\\text{Set Number (bits)} = \\frac{\\text{Lines in cache}}{\\text{Number of sets}}\\)  Fully associative cache  physical address size (bits) = TAG + Block offset  Pipelining pipelining document from nitin\nFormulas   \\(\\text{Speed Up (S)} = \\frac{\\text{Non-pipelined execution time}}{\\text{Pipelined execution time}}\\)\n  \\(\\text{Efficiency} = \\frac{\\text{Speed Up}}{\\text{Number of stages in Pipelined Architecture}}\\)\n  \\(\\text{Efficiency} = \\frac{\\text{Number of boxes utilized in phase time diagram}}{\\text{Number of boxes in phase time diagram}}\\)\n  \\(\\text{Throughput} = \\frac{\\text{Number of instructions executed}}{\\text{Total time taken}}\\)\n  Non-pipelined execution time = Total number of instructions * Time taken to execute one instruction = n * k clock cycles\n  Pipelined execution time\n= Time taken to execute first instruction + Time take to execute remaining instructions\n= 1 * k clock cycles + (n-1) * 1 clock cycle\n= (k + n-1) clock cycles\n  Cycle time = Maximum delay due to any stage + Delay due to its register\n  delay due to its register = latch delay\n  pipeline time for x tasks = Time taken for 1st task + Time taken for remaining tasks\n= number of phases * cycle time + (total tasks -1) * cycle time\n  MIPS  Mips Theory Mips Reference from Berkeley Mips Reference from Cburch MIPS DATAPATH MIPS PIPELINED DATAPATH  "
}]